<!DOCTYPE html>
<!--[if lt IE 10]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

  
    itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-offline-url="/"
data-url="/library/view/deep-learning-for/9781492045519/ch04.html"

data-archive="9781492045519"
data-publishers="O&#x27;Reilly Media, Inc."


  data-htmlfile-name="ch04.html"
  data-epub-title="Deep Learning for Coders with fastai and PyTorch"
  


data-federated-auth-enabled="true"

data-env="production"
data-debug="0" ><![endif]-->
<!--[if gt IE 9]><!--><html class="no-js" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

  
    itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-offline-url="/"
data-url="/library/view/deep-learning-for/9781492045519/ch04.html"

data-archive="9781492045519"
data-publishers="O&#x27;Reilly Media, Inc."


  data-htmlfile-name="ch04.html"
  data-epub-title="Deep Learning for Coders with fastai and PyTorch"
  


data-federated-auth-enabled="true"

data-env="production"
data-debug="0" ><!--<![endif]-->
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>4. Under the Hood: Training a Digit Classifier - Deep Learning for Coders with fastai and PyTorch [Book]</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="/library/view/static/CACHE/css/output.a33a000c6aec.css" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro' rel='stylesheet' type='text/css' />
    <link rel="stylesheet" type="text/css" href="https://cdn.oreillystatic.com/assets/css/2018_font_face.css" />
    

    <meta property="og:title" content="Deep Learning for Coders with fastai and PyTorch" />
    <meta itemprop="isPartOf" content="/library/view/deep-learning-for/9781492045519/" />
    <meta itemprop="name" content="4. Under the Hood: Training a Digit Classifier" />
    
    <meta property="og:url" itemprop="url" content="https://www.oreilly.com/library/view/deep-learning-for/9781492045519/ch04.html" />
    <meta property="og:site_name" content="O’Reilly Online Learning" />
    <meta property="og:image" itemprop="thumbnailUrl" content="https://www.oreilly.com/library/cover/9781492045519/" />
    
    <meta property="og:image:secure_url" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781492045519/360h/">
    <meta property="og:description" itemprop="description" name="description" content=" Chapter 4. Under the Hood: Training a Digit Classifier Having seen what it looks like to train a variety of models in Chapter 2, let’s now look under the …  - Selection from Deep Learning for Coders with fastai and PyTorch [Book]">
    
    <meta itemprop="inLanguage" content="en" />
    
    <meta itemprop="publisher" content="O&#x27;Reilly Media, Inc." />
    
    <meta property="og:type" content="article" />
    <meta property="og:book:isbn" itemprop="isbn" content="9781492045526" />
    
    <meta property="og:book:author" itemprop="author" content="Jeremy Howard" />
    <meta property="og:book:author" itemprop="author" content="Sylvain Gugger" />
    
    
    
    
    <meta property="og:image:width" content="400">
    <meta property="og:image:height" content="400">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@OReillyMedia">

<!-- Start Visual Website Optimizer Asynchronous Code -->
<script type='text/javascript'>
var _vwo_code=(function(){
var account_id=291788,
settings_tolerance=2000,
library_tolerance=2500,
use_existing_jquery=false,
/* DO NOT EDIT BELOW THIS LINE */
f=false,d=document;return{use_existing_jquery:function(){return use_existing_jquery;},library_tolerance:function(){return library_tolerance;},finish:function(){if(!f){f=true;var a=d.getElementById('_vis_opt_path_hides');if(a)a.parentNode.removeChild(a);}},finished:function(){return f;},load:function(a){var b=d.createElement('script');b.src=a;b.type='text/javascript';b.innerText;b.onerror=function(){_vwo_code.finish();};d.getElementsByTagName('head')[0].appendChild(b);},init:function(){settings_timer=setTimeout('_vwo_code.finish()',settings_tolerance);var a=d.createElement('style'),b='body{opacity:0 !important;filter:alpha(opacity=0) !important;background:none !important;}',h=d.getElementsByTagName('head')[0];a.setAttribute('id','_vis_opt_path_hides');a.setAttribute('type','text/css');if(a.styleSheet)a.styleSheet.cssText=b;else a.appendChild(d.createTextNode(b));h.appendChild(a);this.load('//dev.visualwebsiteoptimizer.com/j.php?a='+account_id+'&u='+encodeURIComponent(d.URL)+'&r='+Math.random());return settings_timer;}};}());_vwo_settings_timer=_vwo_code.init();
</script>
<!-- End Visual Website Optimizer Asynchronous Code -->

    <link class="t-canonical-link" rel="canonical" href="https://www.oreilly.com/library/view/deep-learning-for/9781492045519/ch04.html" />

    <link rel="shortcut icon" href="//www.oreilly.com/favicon.ico" type="image/vnd.microsoft.icon" />
    <link rel="apple-touch-icon" href="/library/view/static/images/apple-touch-icon.png" />
    

  
    <style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px auto !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px auto !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000;padding-top:.25em !important;margin-top:0 !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dl{margin-bottom:1.5em !important}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important;line-height:1.25rem;font-style:italic}#sbo-rt-content dd{margin:10px 0 .25em 1.5em !important;line-height:1.65em !important}#sbo-rt-content dd p{padding:0 !important;margin:0 0 10px !important}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content table li{margin:10px 0 0 .25em !important}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:top;font-size:80%}#sbo-rt-content th{vertical-align:bottom}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller;word-break:break-all}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:2em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content pre.break-code,#sbo-rt-content code.break-code,#sbo-rt-content .break-code pre,#sbo-rt-content .break-code code{word-break:break-all}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10,#sbo-rt-content figure.width-10 img{width:10% !important}#sbo-rt-content .width-20,#sbo-rt-content figure.width-20 img{width:20% !important}#sbo-rt-content .width-30,#sbo-rt-content figure.width-30 img{width:30% !important}#sbo-rt-content .width-40,#sbo-rt-content figure.width-40 img{width:40% !important}#sbo-rt-content .width-50,#sbo-rt-content figure.width-50 img{width:50% !important}#sbo-rt-content .width-60,#sbo-rt-content figure.width-60 img{width:60% !important}#sbo-rt-content .width-70,#sbo-rt-content figure.width-70 img{width:70% !important}#sbo-rt-content .width-80,#sbo-rt-content figure.width-80 img{width:80% !important}#sbo-rt-content .width-90,#sbo-rt-content figure.width-90 img{width:90% !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100% !important}#sbo-rt-content .sc{text-transform:none !important}#sbo-rt-content .right{float:none !important}#sbo-rt-content a.totri-footnote{padding:0 !important}#sbo-rt-content figure.width-10,#sbo-rt-content figure.width-20,#sbo-rt-content figure.width-30,#sbo-rt-content figure.width-40,#sbo-rt-content figure.width-50,#sbo-rt-content figure.width-60,#sbo-rt-content figure.width-70,#sbo-rt-content figure.width-80,#sbo-rt-content figure.width-90{width:auto !important}#sbo-rt-content div.chap10table table{display:none !important}#sbo-rt-content div.chap10table img{margin-top:1rem !important;width:100%}
    </style>
  




</head>
<body class="js-preview-content  ">


<div class="skipToMain" id="skipToMain"><a href="#maincontent"><span class="skipToMain-text">Skip to main content</span></a></div>
<header role="banner" class="global">
  <div class="global-nav">
    <div class="content">
      <nav role="navigation" aria-label="site sections">
        <a href="https://www.oreilly.com" class="logo" title="home page" aria-current="page"><img src="https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg" onerror="this.src='https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red_@2x.png'; this.onerror=null;" alt="O'Reilly home"></a>

        <button id="mobileNavButton" class="mobileNavButton mobileNavButton--collapse mobileNavButton--3dx" type="button" aria-expanded="false" aria-controls="menuList">
          <span class="mobileNavButton-box">
            <span class="mobileNavButton-inner"></span>
          </span>
        </button>


        <ul id="menuList" class="menuList mobileHidden ">
          <li class="menuList-itemsP1">
            <ul>
              
  <li class="menuList-item menuList-signIn"><a class="t-sign-in" href="https://learning.oreilly.com/accounts/login/?next=/library/view/deep-learning-for/9781492045519/ch04.html">Sign In</a></li>


              <li class="menuList-item menuList-tryNow"><a class="menuList-cta" href="https://learning.oreilly.com/p/register/">Try Now</a></li>
            </ul>
          </li>
          <li class="menuList-itemsP2">
            <ul>
              <li class="menuList-item"><a href="https://www.oreilly.com/online-learning/teams.html">Teams</a></li>
              <li class="menuList-item"><a href="https://www.oreilly.com/online-learning/individuals.html">Individuals</a></li>
              <li class="menuList-item menuList-itemWithSub"><a href="https://www.oreilly.com/online-learning/features.html">Features</a>
                <ul class="menuList-subList">
                  <li class="menuList-subItem"><a href="https://www.oreilly.com/online-learning/feature-certification.html">Certifications</a></li>
                  <li class="menuList-subItem"><a href="https://www.oreilly.com/online-learning/intro-interactive-learning.html">Interactive learning</a></li>
                  <li class="menuList-subItem"><a href="https://www.oreilly.com/online-learning/live-online-sessions.html">Live online sessions</a></li>
                </ul>
              </li>
              <li class="menuList-item"><a href="https://www.oreilly.com/whats-new.html">What’s New</a></li>
              <li class="menuList-item menuList-forMarketers"><a href="https://www.oreilly.com/content-marketing-solutions.html">O’Reilly for marketers</a></li>
            </ul>
          </li>
        </ul>
      </nav>
    </div>
  </div>


  <div class="global-search">
    <div id="globalSearchContent" class="content">
      <form id="js-search-form" class="t-navigation-form" action="https://learning.oreilly.com/search/">
        
        <script type="application/ld+json">
        {
          "@context": "http://schema.org",
          "@type": "WebSite",
          "url": "https://learning.oreilly.com",
          "potentialAction": {
            "@type": "SearchAction",
            "target": "https://learning.oreilly.com/search/?q={search_term_string}",
            "query-input": "required name=search_term_string"
          }
        }
        </script>
        <input data-search-text-focus= "See everything available through O’Reilly online learning and start a free trial. Explore now." data-search-text-idle = "See everything available through O’Reilly online learning and start a free trial. Explore now." id="search" type="search" name="query" placeholder="See everything available through O’Reilly online learning and start a free trial. Explore now." autocomplete="off" required />
        <input type="submit" value="Search" class="search-submit" />
      </form>
    </div>
  </div>
</header>

    
    
        <div class="sbo-menu-top">
           
            <section class="sbo-toc-container">
                <a href="https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/" class="sbo-toc-thumb">
                    <span class="sbo-title ss-list">
                        
                            <h1 class="t-title">Deep Learning for Coders with fastai and PyTorch by Jeremy Howard, Sylvain Gugger</h1>
                        
                    </span>
                </a>
            </section>
             
        </div>
    


</header>


<section id="trial-overlay">
  <div class="trial-overlay-content">
    <p>Get <em>Deep Learning for Coders with fastai and PyTorch</em> now with O’Reilly <span class="nowrap">online learning.</span></p>

    <p>O’Reilly members experience live online training, plus books, videos, and digital content from <span class="nowrap">200+ publishers.</span></p>

    <div class="controls">
      <a href="https://learning.oreilly.com/p/register/" class="button-primary" data-ga-label="Bottom CTA">Start your free trial</a>
    </div>
    <a class="modal-dismiss" aria-label="modal dismiss"></a>
  </div>
</section>

<main role="main" id="maincontent">
  <div role="document" class="document">
  	
<section id="sbo-reader">
    
    

<div class="sbo-reader-content sbo-sample-reader ">
    
    <div id="sbo-rt-content" class="sbo-rt-content">
    <div id="test-content-id"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Under the Hood: Training a Digit Classifier"><div class="chapter" id="chapter_mnist_basics">
<h1><span class="label">Chapter 4. </span>Under the Hood: Training a Digit Classifier</h1>


<p>Having seen what it looks like to train a variety of models in
<a data-type="xref" href="ch02.html#chapter_production">Chapter 2</a>, let’s now look under the hood and see exactly what is going
on. We’ll start by using computer vision to introduce fundamental tools
and concepts for deep learning.</p>

<p>To be exact, we’ll discuss the roles of arrays and tensors
and of broadcasting, a powerful technique for using them expressively.
We’ll explain stochastic gradient descent (SGD), the
mechanism for learning by updating weights automatically.
We’ll discuss the choice of a loss function for our basic
classification task, and the role of mini-batches. We’ll
also describe the math that a basic neural network is doing.
Finally, we’ll put all these pieces together.</p>

<p>In future chapters, we’ll do deep dives into other applications as well,
and see how these concepts and tools generalize. But this chapter is
about laying foundation stones. To be frank, that also makes this one of
the hardest chapters, because of how these concepts all depend on each
other. Like an arch, all the stones need to be in place for the
structure to stay up. Also like an arch, once that happens,
it’s a powerful structure that can support other things. But
it requires some patience to assemble.</p>

<p>Let’s begin. The first step is to consider how images are
represented in a computer.</p>






<section data-type="sect1" data-pdf-bookmark="Pixels: The Foundations of Computer Vision"><div class="sect1" id="idm46287182709240">
<h1>Pixels: The Foundations of Computer Vision</h1>

<p>To understand what happens in a computer vision model, we first<a data-type="indexterm" data-primary="computer vision models" data-secondary="pixels as foundation" id="ch04_pix"></a><a data-type="indexterm" data-primary="pixels" data-secondary="image basics" id="ch04_pix2"></a><a data-type="indexterm" data-primary="computer vision models" data-secondary="image basics" id="ch04_pix3"></a><a data-type="indexterm" data-primary="image classifier models" data-secondary="image basics" id="ch04_pix4"></a>
have to understand how computers handle images. We’ll use
one of the most famous datasets in computer vision,
<a href="https://oreil.ly/g3RDg">MNIST</a>, for our
experiments. MNIST contains images of handwritten digits, collected by the<a data-type="indexterm" data-primary="datasets" data-secondary="handwritten digits" id="idm46287182701112"></a><a data-type="indexterm" data-primary="handwritten digits dataset" id="idm46287182700136"></a><a data-type="indexterm" data-primary="MNIST handwritten digits dataset" id="idm46287182699496"></a><a data-type="indexterm" data-primary="National Institute of Standards and Technology" id="idm46287182698808"></a><a data-type="indexterm" data-primary="Lecun, Yann" id="idm46287182698040"></a><a data-type="indexterm" data-primary="datasets" data-secondary="MNIST handwritten digits dataset" id="idm46287182697368"></a><a data-type="indexterm" data-primary="number-related datasets" data-secondary="handwritten digits dataset" id="idm46287182696408"></a>
National Institute of Standards and Technology and collated into a
machine learning dataset by Yann Lecun and his colleagues. Lecun used
MNIST in 1998 in <a href="https://oreil.ly/LCNEx">LeNet-5</a>, the first computer system to demonstrate practically useful
recognition of handwritten digit sequences. This was one of the most
important breakthroughs in the history of AI.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46287182694296">
<h5>Tenacity and Deep Learning</h5>
<p>The story of deep learning is one of tenacity and grit by a handful of
dedicated researchers. After early hopes (and hype!), neural networks<a data-type="indexterm" data-primary="history" data-secondary="deep learning" id="idm46287182692600"></a><a data-type="indexterm" data-primary="deep learning" data-secondary="history" id="idm46287182691624"></a><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="history of development" id="idm46287182690680"></a><a data-type="indexterm" data-primary="Lecun, Yann" id="idm46287182689736"></a><a data-type="indexterm" data-primary="Bengio, Yoshua" id="idm46287182689064"></a><a data-type="indexterm" data-primary="Hinton, Geoffrey" id="idm46287182688392"></a><a data-type="indexterm" data-primary="Turing Award" id="idm46287182687720"></a>
went out of favor in the 1990s and 2000s, and
just a handful of researchers kept trying to make them work well. Three
of them, Yann Lecun, Yoshua Bengio, and Geoffrey Hinton, were awarded the
highest honor in computer science, the Turing Award (generally
considered the “Nobel Prize of computer science”), in 2018 after triumphing
despite the deep skepticism and disinterest of the wider machine
learning and statistics community.</p>

<p>Hinton has told of how academic papers showing dramatically
better results than anything previously published would be rejected by
top journals and conferences, just because they used a neural network.
Lecun’s work on convolutional neural networks, which we<a data-type="indexterm" data-primary="convolutional neural network (CNN)" data-secondary="Yann Lecun’s work" id="idm46287182685816"></a><a data-type="indexterm" data-primary="handwritten text read by models" data-seealso="numerical digit classifier" id="idm46287182684824"></a><a data-type="indexterm" data-primary="MNIST handwritten digits dataset" data-secondary="read by models" data-seealso="numerical digit classifier" id="idm46287182683848"></a>
will study in the next section, showed that these models could read
handwritten text—something that had never been achieved before. However,
his breakthrough was ignored by most researchers, even as it was used
commercially to read 10% of the checks in the US!</p>

<p>In addition to these three Turing Award winners, many other
researchers have battled to get us to where we are today. For
instance, Jurgen Schmidhuber (who many believe should have shared in the<a data-type="indexterm" data-primary="Schmidhuber, Jurgen" id="idm46287182681384"></a><a data-type="indexterm" data-primary="Hochreiter, Sepp" id="idm46287182680680"></a>
Turing Award) pioneered many important ideas, including working with his
student Sepp Hochreiter on the long short-term memory (LSTM) architecture (widely used for<a data-type="indexterm" data-primary="architecture of model" data-secondary="long short-term memory" id="idm46287182679704"></a><a data-type="indexterm" data-primary="long short-term memory (LSTM)" id="idm46287182678760"></a>
speech recognition and other text modeling tasks, and used in the IMDb
example in <a data-type="xref" href="ch01.html#chapter_intro">Chapter 1</a>). Perhaps most important of all,
Paul Werbos in 1974 invented backpropagation for neural networks, the<a data-type="indexterm" data-primary="Werbos, Paul" id="idm46287182677080"></a><a data-type="indexterm" data-primary="backpropagation" data-secondary="training neural networks" id="idm46287182676408"></a><a data-type="indexterm" data-primary="training" data-secondary="backpropagation for neural networks" id="idm46287182675448"></a><a data-type="indexterm" data-primary="neural networks" data-secondary="training via backpropagation" id="idm46287182674488"></a>
technique shown in this chapter and used universally for training neural
networks
(<a href="https://oreil.ly/wWIWp">Werbos 1994</a>). His development was almost entirely ignored for decades, but
today it is considered the most important foundation of modern AI.</p>

<p>There is a lesson here for all of us! On your deep learning journey, you
will face many obstacles, both technical and (even more difficult) posed by
people around you who don’t believe you’ll be
successful. There’s one <em>guaranteed</em> way to fail, and
that’s to stop trying. We’ve seen that the only
consistent trait among every fast.ai student who’s gone
on to be a world-class practitioner is that they are all very tenacious.</p>
</div></aside>

<p>For this initial tutorial, we are just going to try to create a model
that can classify any image as a 3 or a 7. So let’s
download a sample of MNIST that contains images of just these digits:<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="dataset download" id="idm46287182670712"></a><a data-type="indexterm" data-primary="MNIST handwritten digits dataset" data-secondary="downloading" id="idm46287182669880"></a><a data-type="indexterm" data-primary="handwritten digits dataset" data-secondary="downloading" id="idm46287182668920"></a><a data-type="indexterm" data-primary="training" data-secondary="numerical digit classifier" data-see="numerical digit classifier" id="idm46287182667960"></a><a data-type="indexterm" data-primary="number-related datasets" data-secondary="handwritten digits dataset" data-tertiary="downloading" id="idm46287182666712"></a><a data-type="indexterm" data-primary="image classifier models" data-secondary="numerical digit" data-see="numerical digit classifier" id="idm46287182665480"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">path</code> <code class="o">=</code> <code class="n">untar_data</code><code class="p">(</code><code class="n">URLs</code><code class="o">.</code><code class="n">MNIST_SAMPLE</code><code class="p">)</code></pre>

<p class="pagebreak-before">We can see what’s in this directory by using <code>ls</code>, a<a data-type="indexterm" data-primary="ls method in Path class" id="idm46287182633448"></a><a data-type="indexterm" data-primary="path to dataset" data-secondary="ls method" id="idm46287182632808"></a>
method added by fastai. This method returns an object of a special
fastai class called <code>L</code>, which has all the same functionality of
Python’s built-in <code>list</code>, plus a lot more. One of its handy
features is that, when printed, it displays the count of items before
listing the items themselves (if there are more than 10 items,
it shows just the first few):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">path</code><code class="o">.</code><code class="n">ls</code><code class="p">()</code></pre>

<pre data-type="programlisting">(#9) [Path('cleaned.csv'),Path('item_list.txt'),Path('trained_model.pkl'),Path('
 &gt; models'),Path('valid'),Path('labels.csv'),Path('export.pkl'),Path('history.cs
 &gt; v'),Path('train')]</pre>

<p>The MNIST dataset follows a common layout for machine learning
datasets: separate folders for the training set and the validation (and/or test) set. Let’s see what’s
inside the training set:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">(</code><code class="n">path</code><code class="o">/</code><code class="s1">'train'</code><code class="p">)</code><code class="o">.</code><code class="n">ls</code><code class="p">()</code></pre>

<pre data-type="programlisting">(#2) [Path('train/7'),Path('train/3')]</pre>

<p>There’s a folder of 3s, and a folder of 7s. In
machine learning parlance, we say that “3” and “7” are the <em>labels</em>
(or targets) in this dataset. Let’s take a look in one of
these folders (using <code>sorted</code> to ensure we all get the same order of
files):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">threes</code> <code class="o">=</code> <code class="p">(</code><code class="n">path</code><code class="o">/</code><code class="s1">'train'</code><code class="o">/</code><code class="s1">'3'</code><code class="p">)</code><code class="o">.</code><code class="n">ls</code><code class="p">()</code><code class="o">.</code><code class="n">sorted</code><code class="p">()</code>
<code class="n">sevens</code> <code class="o">=</code> <code class="p">(</code><code class="n">path</code><code class="o">/</code><code class="s1">'train'</code><code class="o">/</code><code class="s1">'7'</code><code class="p">)</code><code class="o">.</code><code class="n">ls</code><code class="p">()</code><code class="o">.</code><code class="n">sorted</code><code class="p">()</code>
<code class="n">threes</code></pre>

<pre data-type="programlisting">(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.pn
 &gt; g'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.p
 &gt; ng'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.p
 &gt; ng'),Path('train/3/10091.png')...]</pre>

<p>As we might expect, it’s full of image files. Let’s take a<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="viewing dataset images" id="idm46287182544744"></a><a data-type="indexterm" data-primary="Image class" id="idm46287182543896"></a><a data-type="indexterm" data-primary="Python Imaging Library (PIL)" id="idm46287182543224"></a><a data-type="indexterm" data-primary="computer vision models" data-secondary="Python Imaging Library" id="idm46287182542584"></a><a data-type="indexterm" data-primary="image classifier models" data-secondary="Python Imaging Library" id="idm46287182541640"></a><a data-type="indexterm" data-primary="PIL images" id="idm46287182540696"></a>
look at one now. Here’s an image of a handwritten number 3, taken from
the famous MNIST dataset of handwritten numbers:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">im3_path</code> <code class="o">=</code> <code class="n">threes</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
<code class="n">im3</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">im3_path</code><code class="p">)</code>
<code class="n">im3</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in01.png" alt="" width="28" height="28">
<h6></h6>
</div></figure>

<p>Here we are using the <code>Image</code> class from the <em>Python Imaging Library</em>
(PIL), which is the most widely used Python package for opening,
manipulating, and viewing images. Jupyter knows about PIL images, so it
displays the image for us automatically.</p>

<p>In a computer, everything is represented as a number. To view the
numbers that make up this image, we have to convert it to a <em>NumPy
array</em> or a <em>PyTorch tensor</em>. For instance, here’s what a section of the image looks like converted to a NumPy array:<a data-type="indexterm" data-primary="arrays" data-secondary="image section" id="idm46287182488152"></a><a data-type="indexterm" data-primary="NumPy" data-secondary="arrays" data-tertiary="image section" id="idm46287182487176"></a><a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="image as array or tensor" id="idm46287182485960"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">array</code><code class="p">(</code><code class="n">im3</code><code class="p">)[</code><code class="mi">4</code><code class="p">:</code><code class="mi">10</code><code class="p">,</code><code class="mi">4</code><code class="p">:</code><code class="mi">10</code><code class="p">]</code></pre>

<pre data-type="programlisting">array([[  0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,  29],
       [  0,   0,   0,  48, 166, 224],
       [  0,  93, 244, 249, 253, 187],
       [  0, 107, 253, 253, 230,  48],
       [  0,   3,  20,  20,  15,   0]], dtype=uint8)</pre>

<p>The <code>4:10</code> indicates we requested the rows from index 4 (inclusive) to 10 (noninclusive), and the same for the columns. NumPy indexes from top to bottom and from left to right, so this section is located near the top-left corner of the image. Here’s the same thing as a PyTorch tensor:<a data-type="indexterm" data-primary="tensors" data-secondary="image section" id="idm46287182475624"></a><a data-type="indexterm" data-primary="PyTorch" data-secondary="tensors" data-tertiary="image section" id="idm46287182474680"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tensor</code><code class="p">(</code><code class="n">im3</code><code class="p">)[</code><code class="mi">4</code><code class="p">:</code><code class="mi">10</code><code class="p">,</code><code class="mi">4</code><code class="p">:</code><code class="mi">10</code><code class="p">]</code></pre>

<pre data-type="programlisting">tensor([[  0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,  29],
        [  0,   0,   0,  48, 166, 224],
        [  0,  93, 244, 249, 253, 187],
        [  0, 107, 253, 253, 230,  48],
        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)</pre>

<p>We can slice the array to pick just the part with the top of the digit in
it, and then use a Pandas DataFrame to color-code the values using a<a data-type="indexterm" data-primary="Pandas library" data-secondary="DataFrame" data-tertiary="color-code image values" id="idm46287182442808"></a><a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="color-code array or tensor" id="idm46287182441656"></a><a data-type="indexterm" data-primary="DataFrame" data-secondary="color-code image values" id="idm46287182440776"></a>
gradient, which shows us clearly how the image is created from the pixel
values:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">im3_t</code> <code class="o">=</code> <code class="n">tensor</code><code class="p">(</code><code class="n">im3</code><code class="p">)</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">im3_t</code><code class="p">[</code><code class="mi">4</code><code class="p">:</code><code class="mi">15</code><code class="p">,</code><code class="mi">4</code><code class="p">:</code><code class="mi">22</code><code class="p">])</code>
<code class="n">df</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">set_properties</code><code class="p">(</code><code class="o">**</code><code class="p">{</code><code class="s1">'font-size'</code><code class="p">:</code><code class="s1">'6pt'</code><code class="p">})</code><code class="o">.</code><code class="n">background_gradient</code><code class="p">(</code><code class="s1">'Greys'</code><code class="p">)</code></pre>

<figure class="width-50"><div id="output_pd_pixels" class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in02.png" alt="" width="906" height="633">
<h6></h6>
</div></figure>

<p>You can see that the background white pixels are stored as the number
0, black is the number 255, and shades of gray are between the two.
The entire image contains 28 pixels across and 28 pixels down, for a
total of 768 pixels. (This is much smaller than an image that you would
get from a phone camera, which has millions of pixels, but is a
convenient size for our initial learning and experiments. We will build
up to bigger, full-color images soon.)<a data-type="indexterm" data-startref="ch04_pix" id="idm46287182343048"></a><a data-type="indexterm" data-startref="ch04_pix2" id="idm46287182342344"></a><a data-type="indexterm" data-startref="ch04_pix3" id="idm46287182341672"></a><a data-type="indexterm" data-startref="ch04_pix4" id="idm46287182341000"></a></p>

<p>So, now you’ve seen what an image looks like to a computer,
let’s recall our goal: create a model that can recognize
3s and 7s. How might you go about getting a computer to do that?</p>
<div data-type="warning" epub:type="warning"><h1>Stop and Think!</h1>
<p>Before you read on, take a moment to think about how a computer might be able to recognize these two digits. What kinds of features might it be able to look at? How might it be able to identify these features? How could it combine them? Learning works best when you try to solve problems yourself, rather than just reading somebody else’s answers; so step away from this book for a few minutes, grab a piece of paper and pen, and jot some ideas down.</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="First Try: Pixel Similarity"><div class="sect1" id="idm46287182708296">
<h1>First Try: Pixel Similarity</h1>

<p>So, here is a first idea: how about we find the average pixel value for<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="pixel similarity" id="ch04_sim"></a><a data-type="indexterm" data-primary="pixels" data-secondary="pixel similarity" id="ch04_sim3"></a><a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="ideal digit creation" id="ch04_idl"></a>
every pixel of the 3s, then do the same for the 7s. This
will give us two group averages, defining what we might call the
“ideal” 3 and 7. Then, to classify an image as one digit or the other, we see which of
these two ideal digits the image is most similar to. This certainly
seems like it should be better than nothing, so it will make a good
baseline.</p>
<div data-type="note" epub:type="note"><h1>Jargon: Baseline</h1>
<p>A simple model that you are confident should perform reasonably<a data-type="indexterm" data-primary="models" data-secondary="begin simply" data-seealso="beginning" id="idm46287182330184"></a><a data-type="indexterm" data-primary="beginning" data-secondary="begin with simple baseline model" id="idm46287182328936"></a><a data-type="indexterm" data-primary="training" data-secondary="baseline" id="idm46287182328024"></a> well. It should be simple to implement and easy to test, so that you can then test each of your improved ideas and make sure they are always better than your baseline. Without starting with a sensible baseline, it is difficult to know whether your super-fancy models are any good. One good approach to creating a baseline is doing what we have done here: think of a simple, easy-to-implement model. Another good approach is to search around to find other people who have solved problems similar to yours, and download and run their code on your dataset. Ideally, try both of these!</p>
</div>

<p>Step 1 for our simple model is to get the average of pixel values for
each of our two groups. In the process of doing this, we will learn a
lot of neat Python numeric programming tricks!</p>

<p>Let’s create a tensor containing all of our 3s stacked<a data-type="indexterm" data-primary="tensors" data-secondary="all images in directory" id="idm46287182325224"></a>
together. We already know how to create a tensor containing a single
image. To create a tensor containing all the images in a directory, we
will first use a Python list comprehension to create a plain list of the
single image tensors.</p>

<p>We will use Jupyter to do some little checks of our work along the way—in this case, making sure that the number of returned items seems
reasonable:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">seven_tensors</code> <code class="o">=</code> <code class="p">[</code><code class="n">tensor</code><code class="p">(</code><code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">o</code><code class="p">))</code> <code class="k">for</code> <code class="n">o</code> <code class="ow">in</code> <code class="n">sevens</code><code class="p">]</code>
<code class="n">three_tensors</code> <code class="o">=</code> <code class="p">[</code><code class="n">tensor</code><code class="p">(</code><code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">o</code><code class="p">))</code> <code class="k">for</code> <code class="n">o</code> <code class="ow">in</code> <code class="n">threes</code><code class="p">]</code>
<code class="nb">len</code><code class="p">(</code><code class="n">three_tensors</code><code class="p">),</code><code class="nb">len</code><code class="p">(</code><code class="n">seven_tensors</code><code class="p">)</code></pre>

<pre data-type="programlisting">(6131, 6265)</pre>
<div data-type="note" epub:type="note"><h1>List Comprehensions</h1>
<p>List and dictionary comprehensions are a wonderful feature of Python. Many Python programmers use them every day, including the authors of this book—they are part of “idiomatic Python.” But programmers coming from other languages may have never seen them before. A lot of great tutorials are just a web search away, so we won’t spend a long time discussing them now. Here is a quick explanation and example to get you started. A list comprehension<a data-type="indexterm" data-primary="Python" data-secondary="list comprehensions" id="idm46287182249672"></a><a data-type="indexterm" data-primary="list comprehensions" id="idm46287182248696"></a> looks like this: <code>new_list = [f(o) for o in a_list if o&gt;0]</code>. This will return every element of <code>a_list</code> that is greater than 0, after passing it to the function <code>f</code>. There are three parts here: the collection you are iterating over (<code>a_list</code>), an optional filter (<code>if o&gt;0</code>), and something to do to each element (<code>f(o)</code>). It’s not only shorter to write, but also way faster than the alternative ways of creating the same list with a loop.</p>
</div>

<p>We’ll also check that one of the images looks OK. Since we
now have tensors (which Jupyter by default will print as values), rather
than PIL images (which Jupyter by default will display images), we
need to use fastai’s <code>show_image</code> function to display it:<a data-type="indexterm" data-primary="show_image function" id="idm46287182244184"></a><a data-type="indexterm" data-primary="tensors" data-secondary="displaying as images" id="idm46287182243480"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">show_image</code><code class="p">(</code><code class="n">three_tensors</code><code class="p">[</code><code class="mi">1</code><code class="p">]);</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in03.png" alt="" width="68" height="68">
<h6></h6>
</div></figure>

<p>For every pixel position, we want to compute the average over all the
images of the intensity of that pixel. To do this, we first combine all
the images in this list into a single three-dimensional tensor. The most
common way to describe such a tensor is to call it a <em>rank-3 tensor</em>. We
often need to stack up individual tensors in a collection into a single
tensor. Unsurprisingly, PyTorch comes with a function called <code>stack</code> that we can use for this purpose.</p>

<p class="pagebreak-before">Some operations in PyTorch, such as taking a mean, require us to <em>cast</em>
our integer types to float types. Since we’ll be needing
this later, we’ll also cast our stacked tensor to <code>float</code>
now. Casting in PyTorch is as simple as writing the name of the type you<a data-type="indexterm" data-primary="PyTorch" data-secondary="casting" id="idm46287182233528"></a><a data-type="indexterm" data-primary="casting in PyTorch" id="idm46287182232552"></a><a data-type="indexterm" data-primary="floating point numbers" data-secondary="casting in PyTorch" id="idm46287182231880"></a>
wish to cast to, and treating it as a method.</p>

<p>Generally, when images are floats, the pixel values are expected to be between
0 and 1, so we will also divide by 255 here:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">stacked_sevens</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">stack</code><code class="p">(</code><code class="n">seven_tensors</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">/</code><code class="mi">255</code>
<code class="n">stacked_threes</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">stack</code><code class="p">(</code><code class="n">three_tensors</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">/</code><code class="mi">255</code>
<code class="n">stacked_threes</code><code class="o">.</code><code class="n">shape</code></pre>

<pre data-type="programlisting">torch.Size([6131, 28, 28])</pre>

<p>Perhaps the most important attribute of a tensor is its <em>shape</em>. This<a data-type="indexterm" data-primary="tensors" data-secondary="shape" id="idm46287182164360"></a><a data-type="indexterm" data-primary="pixels" data-secondary="pixel count" data-tertiary="tensor shape" id="idm46287182163416"></a>
tells you the length of each axis. In this case, we can see that we have
6,131 images, each of size 28×28 pixels. There is nothing specifically
about this tensor that says that the first axis is the number of images,
the second is the height, and the third is the width—the semantics of
a tensor are entirely up to us, and how we construct it. As far as
PyTorch is concerned, it is just a bunch of numbers in memory.</p>

<p>The <em>length</em> of a tensor’s shape is its rank:<a data-type="indexterm" data-primary="tensors" data-secondary="shape" data-tertiary="length for rank" id="idm46287182160808"></a><a data-type="indexterm" data-primary="rank of tensor" data-secondary="definition" id="idm46287182159560"></a><a data-type="indexterm" data-primary="tensors" data-secondary="rank" id="idm46287182158616"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="nb">len</code><code class="p">(</code><code class="n">stacked_threes</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<pre data-type="programlisting">3</pre>

<p>It is really important for you to commit to memory and practice these
bits of tensor jargon: <em>rank</em> is the number of axes or dimensions in a
tensor; <em>shape</em> is the size of each axis of a tensor.</p>
<div data-type="tip"><h1>Alexis Says</h1>
<p>Watch out because the term “dimension” is sometimes used in two<a data-type="indexterm" data-primary="dimension multiple meanings" id="idm46287182139320"></a> ways. Consider that we live in “three-dimensional space,” where a physical position can be described by a vector <code>v</code>, of length 3. But according to PyTorch, the attribute <code>v.ndim</code> (which sure looks like the “number of dimensions” of <code>v</code>) equals one, not three! Why? Because <code>v</code> is a vector, which is a tensor of rank one, meaning that it has only one <em>axis</em> (even if that axis has a length of three). In other words, sometimes dimension is used for the size of an axis (“space is three-dimensional”), while other times it is used for the rank, or the number of axes (“a matrix has two dimensions”). When confused, I find it helpful to translate all statements into terms of rank, axis, and length, which are unambiguous terms.</p>
</div>

<p class="pagebreak-before">We can also get a tensor’s rank directly with <code>ndim</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">stacked_threes</code><code class="o">.</code><code class="n">ndim</code></pre>

<pre data-type="programlisting">3</pre>

<p>Finally, we can compute what the ideal 3 looks like. We calculate
the mean of all the image tensors by taking the mean along dimension
0 of our stacked, rank-3 tensor. This is the dimension that indexes
over all the images.</p>

<p>In other words, for every pixel position, this will compute the average
of that pixel over all images. The result will be one value for every
pixel position, or a single image. Here it is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mean3</code> <code class="o">=</code> <code class="n">stacked_threes</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">show_image</code><code class="p">(</code><code class="n">mean3</code><code class="p">);</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in04.png" alt="" width="68" height="68">
<h6></h6>
</div></figure>

<p>According to this dataset, this is the ideal number 3! (You may not
like it, but this is what peak number 3 performance looks like.) You can
see how it’s very dark where all the images agree it should
be dark, but it becomes wispy and blurry where the images disagree.</p>

<p>Let’s do the same thing for the 7s, but put all the steps together at once to save time:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mean7</code> <code class="o">=</code> <code class="n">stacked_sevens</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">show_image</code><code class="p">(</code><code class="n">mean7</code><code class="p">);</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in05.png" alt="" width="68" height="68">
<h6></h6>
</div></figure>

<p>Let’s now pick an arbitrary 3 and measure its
<em>distance</em> from our “ideal digits.”<a data-type="indexterm" data-startref="ch04_idl" id="idm46287182047048"></a></p>
<div data-type="warning" epub:type="warning"><h1>Stop and Think!</h1>
<p>How would you calculate how similar a particular image is to each of our ideal digits? Remember to step away from this book and jot down some ideas before you move on! Research shows that recall and understanding improve dramatically when you are engaged with the learning process by solving problems, experimenting, and trying new ideas yourself.</p>
</div>

<p class="pagebreak-before">Here’s a sample 3:<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="comparing with ideal digit" id="idm46287182043800"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">a_3</code> <code class="o">=</code> <code class="n">stacked_threes</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
<code class="n">show_image</code><code class="p">(</code><code class="n">a_3</code><code class="p">);</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in06.png" alt="" width="68" height="68">
<h6></h6>
</div></figure>

<p>How can we determine its distance from our ideal 3? We can’t just add up the differences between the pixels of
this image and the ideal digit. Some differences will be positive, while others will be negative, and
these differences will cancel out, resulting in a situation where an image
that is too dark in some places and too light in others might be shown
as having zero total differences from the ideal. That would be

<span class="keep-together">misleading</span>!</p>

<p>To avoid this, data scientists use two main ways to measure
distance in this context:</p>

<ul>
<li>
<p>Take the mean of the <em>absolute value</em> of differences (absolute value
is the function that replaces negative values with positive values).
This is called the <em>mean absolute difference</em> or <em>L1 norm</em>.<a data-type="indexterm" data-primary="mean absolute difference (L1 norm)" id="idm46287181978904"></a><a data-type="indexterm" data-primary="L1 norm (mean absolute difference)" id="idm46287181978200"></a></p>
</li>
<li>
<p>Take the mean of the <em>square</em> of differences (which makes everything
positive) and then take the <em>square root</em> (which undoes the squaring).
This is called the <em>root mean squared error</em> (RMSE) or <em>L2 norm</em>.<a data-type="indexterm" data-primary="root mean squared error (RMSE or L2 norm)" id="idm46287181974776"></a><a data-type="indexterm" data-primary="L2 norm (root mean squared error)" id="idm46287181974072"></a></p>
</li>
</ul>
<div data-type="tip"><h1>It’s OK to Have Forgotten Your Math</h1>
<p>In this book, we generally assume that you have completed high school math, and remember at least some of it—but everybody forgets some things! It all depends on what you happen to have had reason to practice in the meantime. Perhaps you have forgotten what a <em>square root</em> is, or exactly how they work. No problem! Anytime you come across a math concept that is not explained fully in this book, don’t just keep moving on; instead, stop and look it up. Make sure you understand the basic idea, how it works, and why we might be using it. One of the best places to refresh your<a data-type="indexterm" data-primary="math tutorials online" id="idm46287181971064"></a><a data-type="indexterm" data-primary="tutorials" data-secondary="math tutorials online" id="idm46287181970360"></a><a data-type="indexterm" data-primary="web resources" data-secondary="math tutorials" id="idm46287181969416"></a><a data-type="indexterm" data-primary="Khan Academy math tutorials online" id="idm46287181968472"></a> understanding is Khan Academy. For instance, Khan Academy has a great <a href="https://oreil.ly/T7mxH">introduction to square roots</a>.</p>
</div>

<p class="pagebreak-before">Let’s try both of these now:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dist_3_abs</code> <code class="o">=</code> <code class="p">(</code><code class="n">a_3</code> <code class="o">-</code> <code class="n">mean3</code><code class="p">)</code><code class="o">.</code><code class="n">abs</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="n">dist_3_sqr</code> <code class="o">=</code> <code class="p">((</code><code class="n">a_3</code> <code class="o">-</code> <code class="n">mean3</code><code class="p">)</code><code class="o">**</code><code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="o">.</code><code class="n">sqrt</code><code class="p">()</code>
<code class="n">dist_3_abs</code><code class="p">,</code><code class="n">dist_3_sqr</code></pre>

<pre data-type="programlisting">(tensor(0.1114), tensor(0.2021))</pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">dist_7_abs</code> <code class="o">=</code> <code class="p">(</code><code class="n">a_3</code> <code class="o">-</code> <code class="n">mean7</code><code class="p">)</code><code class="o">.</code><code class="n">abs</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="n">dist_7_sqr</code> <code class="o">=</code> <code class="p">((</code><code class="n">a_3</code> <code class="o">-</code> <code class="n">mean7</code><code class="p">)</code><code class="o">**</code><code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="o">.</code><code class="n">sqrt</code><code class="p">()</code>
<code class="n">dist_7_abs</code><code class="p">,</code><code class="n">dist_7_sqr</code></pre>

<pre data-type="programlisting">(tensor(0.1586), tensor(0.3021))</pre>

<p>In both cases, the distance between our 3 and the “ideal” 3 is
less than the distance to the ideal 7, so our simple model will give
the right prediction in this case.<a data-type="indexterm" data-startref="ch04_sim" id="idm46287181847448"></a><a data-type="indexterm" data-startref="ch04_sim3" id="idm46287181846840"></a></p>

<p>PyTorch already provides both of these as <em>loss functions</em>.<a data-type="indexterm" data-primary="PyTorch" data-secondary="loss functions for comparisons" id="idm46287181845176"></a><a data-type="indexterm" data-primary="loss" data-secondary="PyTorch functions for comparisons" id="idm46287181844200"></a><a data-type="indexterm" data-primary="PyTorch" data-secondary="fastai torch.nn.functional import" id="idm46287181843288"></a><a data-type="indexterm" data-primary="torch.nn.functional" id="idm46287181842376"></a><a data-type="indexterm" data-primary="F (torch.nn.functional)" id="idm46287181841704"></a>
You’ll find these inside <code>torch.nn.functional</code>, which the
PyTorch team recommends importing as <code>F</code> (and is available by default
under that name in fastai):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">F</code><code class="o">.</code><code class="n">l1_loss</code><code class="p">(</code><code class="n">a_3</code><code class="o">.</code><code class="n">float</code><code class="p">(),</code><code class="n">mean7</code><code class="p">),</code> <code class="n">F</code><code class="o">.</code><code class="n">mse_loss</code><code class="p">(</code><code class="n">a_3</code><code class="p">,</code><code class="n">mean7</code><code class="p">)</code><code class="o">.</code><code class="n">sqrt</code><code class="p">()</code></pre>

<pre data-type="programlisting">(tensor(0.1586), tensor(0.3021))</pre>

<p>Here, <code>MSE</code> stands for <em>mean squared error</em>,<a data-type="indexterm" data-primary="mean squared error (MSE)" id="idm46287181831944"></a>
and <code>l1</code> refers to the standard mathematical jargon for <em>mean absolute
value</em> (in math it’s called the <em>L1 norm</em>).<a data-type="indexterm" data-primary="L1 norm (mean absolute difference)" id="idm46287181829864"></a><a data-type="indexterm" data-primary="mean absolute difference (L1 norm)" id="idm46287181829160"></a></p>
<div data-type="tip"><h1>Sylvain Says</h1>
<p>Intuitively, the difference between L1 norm and mean squared error (MSE) is that the latter will penalize bigger mistakes more heavily than the former (and be more lenient with small mistakes).</p>
</div>
<div data-type="tip"><h1>Jeremy Says</h1>
<p>When I first came across this L1 thingie, I looked it up to see what on earth it meant. I found on Google that it is a <em>vector norm</em> using <em>absolute value</em>, so I looked up “vector norm” and started reading: <em>Given a vector space V over a field F of the real or complex numbers, a norm on V is a nonnegative-valued any function p: V → \[0,+∞) with the following properties: For all a ∈ F and all u, v ∈ V, p(u + v) ≤ p(u) + p(v)…</em>Then I stopped reading. “Ugh, I’ll never understand math!” I thought, for the thousandth time. Since then, I’ve learned that every time these complex mathy bits of jargon come up in practice, it turns out I can replace them with a tiny bit of code! Like, the <em>L1 loss</em> is just equal to <code>(a-b).abs().mean()</code>, where <code>a</code> and <code>b</code> are tensors. I guess mathy folks just think differently than I do…I’ll make sure in this book that every time some mathy jargon comes up, I’ll give you the little bit of code it’s equal to as well, and explain in common-sense terms what’s going on.</p>
</div>

<p>We just completed various mathematical operations on
PyTorch tensors. If you’ve done numeric programming
in PyTorch before, you may recognize these as being similar to NumPy
arrays. Let’s have a look at those two important
data structures.</p>








<section data-type="sect2" data-pdf-bookmark="NumPy Arrays and PyTorch Tensors"><div class="sect2" id="idm46287181786424">
<h2>NumPy Arrays and PyTorch Tensors</h2>

<p><a href="https://numpy.org">NumPy</a> is the most widely used library for scientific<a data-type="indexterm" data-primary="NumPy" data-secondary="arrays" data-tertiary="about" id="idm46287181783912"></a><a data-type="indexterm" data-primary="PyTorch" data-secondary="tensors" data-tertiary="about" id="idm46287181782664"></a><a data-type="indexterm" data-primary="arrays" data-secondary="about" id="idm46287181781448"></a><a data-type="indexterm" data-primary="tensors" data-secondary="about" id="idm46287181780504"></a>
and numeric programming in Python. It provides similar
functionality and a similar API to that provided by PyTorch;
however, it does not support using the GPU or calculating gradients,
which are both critical for deep learning. Therefore, in this book, we
will generally use PyTorch tensors instead of NumPy arrays, where
possible.</p>

<p>(Note that fastai adds some features to NumPy and PyTorch to make them a
bit more similar to each other. If any code in this book
doesn’t work on your computer, it’s possible
that you forgot to include a line like this at the start of your notebook:
<code>from</code> 
<span class="keep-together"><code>fastai.vision.all import *</code></span>.)</p>

<p>But what are arrays and tensors, and why should you care?</p>

<p>Python is slow compared to many languages. Anything fast in Python,
NumPy, or PyTorch is likely to be a wrapper for a compiled object written
(and optimized) in another language—specifically, C. In fact, <em>NumPy
arrays and PyTorch tensors can finish computations many thousands of
times faster than using pure Python</em>.</p>

<p>A NumPy array is a multidimensional table of data, with all items of the
same type. Since that can be any type at all, they can even be arrays
of arrays, with the innermost arrays potentially being different sizes—this is called a <em>jagged array</em>. By “multidimensional table,” we<a data-type="indexterm" data-primary="arrays" data-secondary="arrays within arrays" id="idm46287181774472"></a><a data-type="indexterm" data-primary="jagged arrays" id="idm46287181773496"></a><a data-type="indexterm" data-primary="NumPy" data-secondary="arrays" data-tertiary="arrays within arrays" id="idm46287181772824"></a>
mean, for instance, a list (dimension of one), a table or matrix
(dimension of two), a table of tables or cube (dimension of
three), and so forth. If the items are all of simple type such as
integer or float, NumPy will store them as a compact C data
structure in memory. This is where NumPy shines. NumPy has a wide
variety of operators and methods that can run computations on these
compact structures at the same speed as optimized C, because they are
written in optimized C.</p>

<p>A PyTorch tensor is nearly the same thing as a NumPy array, but with an
additional restriction that unlocks additional capabilities.
It’s the same in that it, too, is a multidimensional table
of data, with all items of the same type. However, the restriction is
that a tensor cannot use just any old type—it has to use a single
basic numeric type for all components. As a result, a tensor is not as
flexible as a genuine array of arrays. For example, a PyTorch tensor cannot
be jagged. It is always a regularly shaped multidimensional rectangular
structure.</p>

<p>The vast majority of methods and operators supported by NumPy on these
structures are also supported by PyTorch, but PyTorch tensors have
additional capabilities. One major capability is that these structures
can live on the GPU, in which case their computation will be optimized<a data-type="indexterm" data-primary="GPU deep learning servers" data-secondary="PyTorch tensors optimized for" id="idm46287181748248"></a>
for the GPU and can run much faster (given lots of values to work on).
In addition, PyTorch can automatically calculate derivatives of these
operations, including combinations of operations. As you’ll
see, it would be impossible to do deep learning in practice without this
capability.</p>
<div data-type="tip"><h1>Sylvain Says</h1>
<p>If you don’t know what C is, don’t worry: you won’t need it at all. In a nutshell,<a data-type="indexterm" data-primary="C programming language" id="idm46287181745384"></a><a data-type="indexterm" data-primary="Python" data-secondary="loop inefficiency" id="idm46287181744680"></a> it’s a low-level  (low-level means more similar to the language that computers use internally) 
<span class="keep-together">language</span> that is very fast compared to Python. To take advantage of its speed while programming in Python, try to avoid as much as possible writing loops, and replace them by commands that work directly on arrays or tensors.</p>
</div>

<p>Perhaps the most important new coding skill for a Python programmer to<a data-type="indexterm" data-primary="Python" data-secondary="array APIs" id="idm46287181742056"></a><a data-type="indexterm" data-primary="Python" data-secondary="tensor APIs" id="idm46287181741080"></a><a data-type="indexterm" data-primary="arrays" data-secondary="APIs" id="idm46287181740136"></a><a data-type="indexterm" data-primary="tensors" data-secondary="APIs" id="idm46287181739192"></a>
learn is how to effectively use the array/tensor APIs. We will be
showing lots more tricks later in this book, but here’s a
summary of the key things you need to know for now.</p>

<p>To create an array or tensor, pass a list (or list of lists, or list of
lists of lists, etc.) to <code>array</code> or <code>tensor</code>:<a data-type="indexterm" data-primary="tensors" data-secondary="creating a tensor" id="idm46287181736568"></a><a data-type="indexterm" data-primary="arrays" data-secondary="creating an array" id="idm46287181735560"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">data</code> <code class="o">=</code> <code class="p">[[</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">3</code><code class="p">],[</code><code class="mi">4</code><code class="p">,</code><code class="mi">5</code><code class="p">,</code><code class="mi">6</code><code class="p">]]</code>
<code class="n">arr</code> <code class="o">=</code> <code class="n">array</code> <code class="p">(</code><code class="n">data</code><code class="p">)</code>
<code class="n">tns</code> <code class="o">=</code> <code class="n">tensor</code><code class="p">(</code><code class="n">data</code><code class="p">)</code></pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">arr</code>  <code class="c1"># numpy</code></pre>

<pre data-type="programlisting">array([[1, 2, 3],
       [4, 5, 6]])</pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">tns</code>  <code class="c1"># pytorch</code></pre>

<pre data-type="programlisting">tensor([[1, 2, 3],
        [4, 5, 6]])</pre>

<p>All the operations that follow are shown on tensors, but the syntax and results
for NumPy arrays are identical.</p>

<p>You can select a row (note that, like lists in Python, tensors are 0-indexed, so 1 refers to the second row/column):<a data-type="indexterm" data-primary="arrays" data-secondary="row selected" id="idm46287181685048"></a><a data-type="indexterm" data-primary="tensors" data-secondary="row selected" id="idm46287181711080"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tns</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code></pre>

<pre data-type="programlisting">tensor([4, 5, 6])</pre>

<p>Or a column, by using <code>:</code> to indicate <em>all of the first axis</em> (we
sometimes refer to the dimensions of tensors/arrays as <em>axes</em>):<a data-type="indexterm" data-primary="arrays" data-secondary="column selected" id="idm46287181686600"></a><a data-type="indexterm" data-primary="tensors" data-secondary="column selected" id="idm46287181635720"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tns</code><code class="p">[:,</code><code class="mi">1</code><code class="p">]</code></pre>

<pre data-type="programlisting">tensor([2, 5])</pre>

<p>You can combine these with Python slice syntax (<code>[<em>start</em>:<em>end</em>]</code>, with <em><code>end</code></em> being excluded) to select part of a row or column:<a data-type="indexterm" data-primary="arrays" data-secondary="slicing row or column" id="idm46287181600792"></a><a data-type="indexterm" data-primary="tensors" data-secondary="slicing row or column" id="idm46287181599848"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tns</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">:</code><code class="mi">3</code><code class="p">]</code></pre>

<pre data-type="programlisting">tensor([5, 6])</pre>

<p>And you can use the standard operators, such as <code>+</code>, <code>-</code>, <code>*</code>, and <code>/</code>:<a data-type="indexterm" data-primary="arrays" data-secondary="operators" id="idm46287181582648"></a><a data-type="indexterm" data-primary="tensors" data-secondary="operators" id="idm46287181581752"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tns</code><code class="o">+</code><code class="mi">1</code></pre>

<pre data-type="programlisting">tensor([[2, 3, 4],
        [5, 6, 7]])</pre>

<p>Tensors have a type:<a data-type="indexterm" data-primary="tensors" data-secondary="type" id="idm46287181579896"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tns</code><code class="o">.</code><code class="n">type</code><code class="p">()</code></pre>

<pre data-type="programlisting">'torch.LongTensor'</pre>

<p>And will automatically change that type as needed; for example, from <code>int</code> to <code>float</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tns</code><code class="o">*</code><code class="mf">1.5</code></pre>

<pre data-type="programlisting">tensor([[1.5000, 3.0000, 4.5000],
        [6.0000, 7.5000, 9.0000]])</pre>

<p>So, is our baseline model any good? To quantify this, we must define a
metric.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Computing Metrics Using Broadcasting"><div class="sect1" id="idm46287181785352">
<h1>Computing Metrics Using Broadcasting</h1>

<p>Recall that a <em>metric</em> is a number that is calculated based on the<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="accuracy metric" id="ch04_acc"></a><a data-type="indexterm" data-primary="metrics" data-secondary="numerical digit classifier" id="ch04_acc3"></a><a data-type="indexterm" data-primary="metrics" data-secondary="definition" id="idm46287181493080"></a><a data-type="indexterm" data-primary="accuracy metric" data-secondary="classification models" id="idm46287181492136"></a><a data-type="indexterm" data-primary="image classifier models" data-secondary="accuracy as metric" id="idm46287181491192"></a>
predictions of our model and the correct labels in our dataset, in
order to tell us how good our model is. For instance, we could use
either of the functions we saw in the previous section, mean squared
error or mean absolute error, and take the average of them over the
whole dataset. However, neither of these are numbers that are very
understandable to most people; in practice, we normally use <em>accuracy</em>
as the metric for classification models.</p>

<p>As we’ve discussed, we want to calculate our metric<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="validation set" id="idm46287181489032"></a><a data-type="indexterm" data-primary="validation set" data-secondary="numeric digit classifier" id="idm46287181523000"></a><a data-type="indexterm" data-primary="validation set" data-secondary="building" data-tertiary="numeric digit classifier" id="idm46287181522088"></a>
over a <em>validation set</em>. This is so that we don’t
inadvertently overfit—that is, train a model to work well only on our
training data. This is not really a risk with the
pixel similarity model we’re using here as a first try,
since it has no trained components, but we’ll use a
validation set anyway to follow normal practices and to be ready for our
second try later.</p>

<p>To get a validation set, we need to remove some of the data from training
entirely, so it is not seen by the model at all. As it turns out, the
creators of the MNIST dataset have already done this for us. Do you<a data-type="indexterm" data-primary="MNIST handwritten digits dataset" data-secondary="validation set" id="idm46287181519304"></a>
remember how there was a whole separate directory called <em>valid</em>?
That’s what this directory is for!</p>

<p>So to start, let’s create tensors for our 3s and
7s from that directory. These are the tensors we will use to
calculate a metric measuring the quality of our first-try model, which
measures distance from an ideal image:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">valid_3_tens</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">stack</code><code class="p">([</code><code class="n">tensor</code><code class="p">(</code><code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">o</code><code class="p">))</code>
                            <code class="k">for</code> <code class="n">o</code> <code class="ow">in</code> <code class="p">(</code><code class="n">path</code><code class="o">/</code><code class="s1">'valid'</code><code class="o">/</code><code class="s1">'3'</code><code class="p">)</code><code class="o">.</code><code class="n">ls</code><code class="p">()])</code>
<code class="n">valid_3_tens</code> <code class="o">=</code> <code class="n">valid_3_tens</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">/</code><code class="mi">255</code>
<code class="n">valid_7_tens</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">stack</code><code class="p">([</code><code class="n">tensor</code><code class="p">(</code><code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">o</code><code class="p">))</code>
                            <code class="k">for</code> <code class="n">o</code> <code class="ow">in</code> <code class="p">(</code><code class="n">path</code><code class="o">/</code><code class="s1">'valid'</code><code class="o">/</code><code class="s1">'7'</code><code class="p">)</code><code class="o">.</code><code class="n">ls</code><code class="p">()])</code>
<code class="n">valid_7_tens</code> <code class="o">=</code> <code class="n">valid_7_tens</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">/</code><code class="mi">255</code>
<code class="n">valid_3_tens</code><code class="o">.</code><code class="n">shape</code><code class="p">,</code><code class="n">valid_7_tens</code><code class="o">.</code><code class="n">shape</code></pre>

<pre data-type="programlisting">(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))</pre>

<p>It’s good to get in the habit of checking shapes as you go.
Here we see two tensors, one representing the 3s validation set of
1,010 images of size 28×28, and one representing the 7s validation
set of 1,028 images of size 28×28.</p>

<p>We ultimately want to write a function, <code>is_3</code>, that will decide whether an
arbitrary image is a 3 or a 7. It will do this by deciding which of our
two “ideal digits” that arbitrary image is closer to. For that we
need to define a notion of <em>distance</em>—that is, a function that
calculates the distance between two images.</p>

<p>We can write a simple function that calculates the mean
absolute error using an expression very similar to the one we wrote in
the last section:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">mnist_distance</code><code class="p">(</code><code class="n">a</code><code class="p">,</code><code class="n">b</code><code class="p">):</code> <code class="k">return</code> <code class="p">(</code><code class="n">a</code><code class="o">-</code><code class="n">b</code><code class="p">)</code><code class="o">.</code><code class="n">abs</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">((</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code class="o">-</code><code class="mi">2</code><code class="p">))</code>
<code class="n">mnist_distance</code><code class="p">(</code><code class="n">a_3</code><code class="p">,</code> <code class="n">mean3</code><code class="p">)</code></pre>

<pre data-type="programlisting">tensor(0.1114)</pre>

<p>This is the same value we previously calculated for the distance between
these two images, the ideal 3 <code>mean_3</code> and the arbitrary sample
3 <code>a_3</code>, which are both single-image tensors with a shape of
<code>[28,28]</code>.</p>

<p>But to calculate a metric for overall accuracy, we will need to
calculate the distance to the ideal 3 for <em>every</em> image in the
validation set. How do we do that calculation? We could write a loop
over all of the single-image tensors that are stacked within our
validation set tensor, <code>valid_3_tens</code>, which has a shape of <code>[1010,28,28]</code>
representing 1,010 images. But there is a better way.</p>

<p>Something interesting happens when we take this exact same distance
function, designed for comparing two single images, but pass in as an
argument <code>valid_3_tens</code>, the tensor that represents the 3s
validation set:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">valid_3_dist</code> <code class="o">=</code> <code class="n">mnist_distance</code><code class="p">(</code><code class="n">valid_3_tens</code><code class="p">,</code> <code class="n">mean3</code><code class="p">)</code>
<code class="n">valid_3_dist</code><code class="p">,</code> <code class="n">valid_3_dist</code><code class="o">.</code><code class="n">shape</code></pre>

<pre data-type="programlisting">(tensor([0.1050, 0.1526, 0.1186,  ..., 0.1122, 0.1170, 0.1086]),
 torch.Size([1010]))</pre>

<p>Instead of complaining about shapes not matching, it returned the
distance for every single image as a vector (i.e., a rank-1 tensor) of
length 1,010 (the number of 3s in our validation set). How did that
happen?</p>

<p>Take another look at our function <code>mnist_distance</code>, and<a data-type="indexterm" data-primary="PyTorch" data-secondary="tensors" data-tertiary="broadcasting" id="idm46287181328632"></a><a data-type="indexterm" data-primary="tensors" data-secondary="broadcasting" id="idm46287181327384"></a><a data-type="indexterm" data-primary="broadcasting" id="idm46287181326440"></a>
you’ll see we have there the subtraction <code>(a-b)</code>. The magic trick is that PyTorch, when it tries to perform a simple
subtraction operation between two tensors of different ranks, will use
<em>broadcasting</em>: it will
automatically expand the tensor with the smaller rank to have the same
size as the one with the larger rank. Broadcasting is an important
capability that makes tensor code much easier to write.</p>

<p>After broadcasting so the two argument tensors have the same
rank, PyTorch applies its usual logic for two tensors of the same rank: it performs
the operation on each corresponding element of the
two tensors, and returns the tensor result. For instance:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">3</code><code class="p">])</code> <code class="o">+</code> <code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">])</code></pre>

<pre data-type="programlisting">tensor([2, 3, 4])</pre>

<p>So in this case, PyTorch treats <code>mean3</code>, a rank-2 tensor representing a
single image, as if it were 1,010 copies of the same image, and then
subtracts each of those copies from each 3 in our validation
set. What shape would you expect this tensor to have? Try to figure it
out yourself before you look at the answer here:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">(</code><code class="n">valid_3_tens</code><code class="o">-</code><code class="n">mean3</code><code class="p">)</code><code class="o">.</code><code class="n">shape</code></pre>

<pre data-type="programlisting">torch.Size([1010, 28, 28])</pre>

<p>We are calculating the difference between our ideal 3 and each of the
1,010 3s in the validation set, for each of 28×28 images,
resulting in the shape <code>[1010,28,28]</code>.</p>

<p>There are a couple of important points about how broadcasting is<a data-type="indexterm" data-primary="broadcasting" id="idm46287181194456"></a><a data-type="indexterm" data-primary="tensors" data-secondary="broadcasting" id="idm46287181193752"></a><a data-type="indexterm" data-primary="PyTorch" data-secondary="tensors" data-tertiary="broadcasting" id="idm46287181192808"></a>
implemented, which make it valuable not just for expressivity but also
for performance:</p>

<ul>
<li>
<p>PyTorch doesn’t <em>actually</em> copy <code>mean3</code> 1,010 times.
It <em>pretends</em> it were a tensor of that shape, but
doesn’t allocate any additional memory.</p>
</li>
<li>
<p>It does the whole calculation in C (or, if you’re using a
GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of
times faster than pure Python (up to millions of times faster on a GPU!).</p>
</li>
</ul>

<p>This is true of all broadcasting and elementwise operations and
functions done in PyTorch. <em>It’s the most important<a data-type="indexterm" data-primary="PyTorch" data-secondary="most important technique" id="idm46287181204936"></a>
technique for you to know to create efficient PyTorch code.</em></p>

<p>Next in <code>mnist_distance</code> we see <code>abs</code>. You might be able to guess now
what this does when applied to a tensor. It applies the method to each
individual element in the tensor, and returns a tensor of the results
(that is, it applies the method <em>elementwise</em>). So in this case,
we’ll get back 1,010 absolute values.</p>

<p>Finally, our function calls <code>mean((-1,-2))</code>. The tuple <code>(-1,-2)</code>
represents a range of axes. In Python, <code>-1</code> refers to the last element,
and <code>-2</code> refers to the second-to-last. So in this case, this tells PyTorch
that we want to take the mean ranging over the values indexed by the
last two axes of the tensor. The last two axes are the horizontal and
vertical dimensions of an image. After taking the mean over the last
two axes, we are left with just the first tensor axis, which indexes
over our images, which is why our final size was <code>(1010)</code>. In other
words, for every image, we averaged the intensity of all the pixels in
that image.</p>

<p>We’ll be learning lots more about broadcasting throughout
this book, especially in <a data-type="xref" href="ch17.html#chapter_foundations">Chapter 17</a>, and will be
practicing it regularly too.</p>

<p>We can use <code>mnist_distance</code> to figure out whether an image is a
3 by using the following logic: if the distance between the
digit in question and the ideal 3 is less than the distance to the ideal
7, then it’s a 3. This function will automatically do
broadcasting and be applied elementwise, just like all PyTorch functions
and operators:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">is_3</code><code class="p">(</code><code class="n">x</code><code class="p">):</code> <code class="k">return</code> <code class="n">mnist_distance</code><code class="p">(</code><code class="n">x</code><code class="p">,</code><code class="n">mean3</code><code class="p">)</code> <code class="o">&lt;</code> <code class="n">mnist_distance</code><code class="p">(</code><code class="n">x</code><code class="p">,</code><code class="n">mean7</code><code class="p">)</code></pre>

<p>Let’s test it on our example case:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">is_3</code><code class="p">(</code><code class="n">a_3</code><code class="p">),</code> <code class="n">is_3</code><code class="p">(</code><code class="n">a_3</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code></pre>

<pre data-type="programlisting">(tensor(True), tensor(1.))</pre>

<p>Note that when we convert the Boolean response to a float, we get
<code>1.0</code> for <code>True</code> and <code>0.0</code> for <code>False</code>.</p>

<p>Thanks to broadcasting, we can also test it on the full validation set
of 3s:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">is_3</code><code class="p">(</code><code class="n">valid_3_tens</code><code class="p">)</code></pre>

<pre data-type="programlisting">tensor([True, True, True,  ..., True, True, True])</pre>

<p>Now we can calculate the accuracy for each of the 3s and 7s, by
taking the average of that function for all 3s and its
inverse for all 7s:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">accuracy_3s</code> <code class="o">=</code>      <code class="n">is_3</code><code class="p">(</code><code class="n">valid_3_tens</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code> <code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="n">accuracy_7s</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">is_3</code><code class="p">(</code><code class="n">valid_7_tens</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">())</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>

<code class="n">accuracy_3s</code><code class="p">,</code><code class="n">accuracy_7s</code><code class="p">,(</code><code class="n">accuracy_3s</code><code class="o">+</code><code class="n">accuracy_7s</code><code class="p">)</code><code class="o">/</code><code class="mi">2</code></pre>

<pre data-type="programlisting">(tensor(0.9168), tensor(0.9854), tensor(0.9511))</pre>

<p>This looks like a pretty good start! We’re getting over 90%
accuracy on both 3s and 7s, and we’ve seen how to
define a metric conveniently using broadcasting.<a data-type="indexterm" data-startref="ch04_acc" id="idm46287181011288"></a><a data-type="indexterm" data-startref="ch04_acc3" id="idm46287181010648"></a> But let’s be honest: 3s and 7s are very different-looking digits. And we’re classifying only 2 out of the
10 possible digits so far. So we’re going to need to do
better!</p>

<p>To do better, perhaps it is time to try a system that does some real<a data-type="indexterm" data-primary="training" data-secondary="stochastic gradient descent" id="ch04-sto"></a><a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="about" id="ch04_sto2"></a><a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="stochastic gradient descent" id="ch04_sto3"></a><a data-type="indexterm" data-primary="weights" data-secondary="stochastic gradient descent" id="ch04_sto4"></a>
learning—one that can automatically modify itself to improve its
performance. In other words, it’s time to talk about the
training process and SGD.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Stochastic Gradient Descent"><div class="sect1" id="idm46287181468216">
<h1>Stochastic Gradient Descent</h1>

<p>Do you remember the way that Arthur Samuel described machine learning,
which we quoted in <a data-type="xref" href="ch01.html#chapter_intro">Chapter 1</a>?</p>
<blockquote>
<p>Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.</p></blockquote>

<p>As we discussed, this is the key to allowing us to have a model that
can get better and better—that can learn. But our pixel similarity approach
does not really do this. We do not have any kind of weight assignment,
or any way of improving based on testing the effectiveness of a weight
assignment. In other words, we can’t really improve our
pixel similarity approach by modifying a set of parameters. To
take advantage of the power of deep learning, we will first have to
represent our task in the way that Samuel described it.</p>

<p>Instead of trying to find the similarity between an image and an “ideal
image,” we could instead look at each individual pixel and come up with
a set of weights for each, such that the highest weights are
associated with those pixels most likely to be black for a particular
category. For instance, pixels toward the bottom right are not very
likely to be activated for a 7, so they should have a low weight for
a 7, but they are likely to be activated for an 8, so they
should have a high weight for an 8. This can be represented as a
function and set of weight values for each possible category—for
instance, the probability of being the number 8:</p>
<pre>def pr_eight(x,w) = (x*w).sum()</pre>

<p>Here we are assuming that <code>X</code> is the image, represented as a vector—in
other words, with all of the rows stacked up end to end into a single
long line. And we are assuming that the weights are a vector <code>W</code>. If we
have this function, we just need some way to update the weights to
make them a little bit better. With such an approach, we can repeat that
step a number of times, making the weights better and better, until they
are as good as we can make them.</p>

<p>We want to find the specific values for the vector <code>W</code> that cause the result of our
function to be high for those images that are 8s, and low
for those images that are not. Searching for the best vector <code>W</code> is a way
to search for the best function for recognizing 8s. (Because we are
not yet using a deep neural network, we are limited by what our function
can do—we are going to fix that constraint later in this
chapter.)</p>

<p class="pagebreak-before">To be more specific, here are the steps required to
turn this function into a machine learning classifier:</p>
<ol>
<li>
<p><em>Initialize</em> the weights.</p>
</li>
<li>
<p>For each image, use these weights to <em>predict</em> whether it appears to be a 3 or a 7.</p>
</li>
<li>
<p>Based on these predictions, calculate how good the model is (its <em>loss</em>).</p>
</li>
<li>
<p>Calculate the <em>gradient</em>, which measures for each weight how changing that weight would change the loss.</p>
</li>
<li>
<p><em>Step</em> (that is, change) all the weights based on that calculation.</p>
</li>
<li>
<p>Go back to step 2 and <em>repeat</em> the process.</p>
</li>
<li>
<p>Iterate until you decide to <em>stop</em> the training process (for instance, because the model is good enough or you don’t want to wait any longer).</p>
</li>

</ol>

<p>These seven steps, illustrated in <a data-type="xref" href="#gradient_descent">Figure 4-1</a>, are the
key to the training of all deep learning models. That deep learning
turns out to rely entirely on these steps is extremely surprising and
counterintuitive. It’s amazing that this process can solve
such complex problems. But, as you’ll see, it really does!</p>

<figure><div id="gradient_descent" class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_0401.png" alt="Graph showing the steps for Gradient Descent" width="1427" height="246">
<h6><span class="label">Figure 4-1. </span>The gradient descent process</h6>
</div></figure>

<p>There are many ways to do each of these seven steps, and we
will be learning about them throughout the rest of this book. These are
the details that make a big difference for deep learning practitioners,
but it turns out that the general approach to each one follows
some basic principles. Here are a few guidelines:</p>
<dl>
<dt>Initialize</dt>
<dd>
<p>  We initialize the parameters to random values. This
may sound surprising. There are certainly other choices we could make,
such as initializing them to the percentage of times that pixel is
activated for that category—but since we already know that we have a
routine to improve these weights, it turns out that just starting with
random weights works perfectly well.</p>
</dd>
<dt>Loss</dt>
<dd>
<p>  This is what Samuel referred to when he spoke of <em>testing the effectiveness of any
current weight assignment in terms of actual performance</em>. We need
a function that will return a number that is small if the performance
of the model is good (the standard approach is to treat a small loss as
good and a large loss as bad, although this is just a convention).</p>
</dd>
<dt>Step</dt>
<dd>
<p>  A simple way to figure out whether a weight should be
increased a bit or decreased a bit would be just to try it: increase
the weight by a small amount, and see if the loss goes up or down. Once
you find the correct direction, you could then change that amount by a
bit more, or a bit less, until you find an amount that works well.
However, this is slow! As we will see, the magic of calculus allows us
to directly figure out in which direction, and by roughly how much, to change
each weight, without having to try all these small changes. The way to
do this is by calculating <em>gradients</em>. This is just a performance
optimization; we would get exactly the same results by using the slower
manual process as well.</p>
</dd>
<dt>Stop</dt>
<dd>
<p>Once we’ve decided how many epochs to train the model for (a few suggestions for this were given in the earlier list), we apply that decision. For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time.</p>
</dd>
</dl>

<p>Before applying these steps to our image classification problem,
let’s illustrate what they look like in a simpler case.
First we will define a very simple function, the quadratic—let’s pretend that this is our loss function, and <code>x</code> is a
weight parameter of the function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">f</code><code class="p">(</code><code class="n">x</code><code class="p">):</code> <code class="k">return</code> <code class="n">x</code><code class="o">**</code><code class="mi">2</code></pre>

<p>Here is a graph of that function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plot_function</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="s1">'x'</code><code class="p">,</code> <code class="s1">'x**2'</code><code class="p">)</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in07.png" alt="" width="392" height="268">
<h6></h6>
</div></figure>

<p class="pagebreak-before">The sequence of steps we described earlier starts by picking a random
value for a parameter, and calculating the value of the loss:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plot_function</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="s1">'x'</code><code class="p">,</code> <code class="s1">'x**2'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="o">-</code><code class="mf">1.5</code><code class="p">,</code> <code class="n">f</code><code class="p">(</code><code class="o">-</code><code class="mf">1.5</code><code class="p">),</code> <code class="n">color</code><code class="o">=</code><code class="s1">'red'</code><code class="p">);</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in08.png" alt="" width="392" height="268">
<h6></h6>
</div></figure>

<p>Now we look to see what would happen if we increased or decreased our
parameter by a little bit—the <em>adjustment</em>. This is simply the slope
at a particular point:</p>

<figure class="width-50"><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in09.png" alt="A graph showing the squared function with the slope at one point" width="999" height="753">
<h6></h6>
</div></figure>

<p class="pagebreak-before">We can change our weight by a little in the direction of the slope,
calculate our loss and adjustment again, and repeat this a few times.
Eventually, we will get to the lowest point on our curve:</p>

<figure class="width-50"><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in10.png" alt="An illustration of gradient descent" width="999" height="753">
<h6></h6>
</div></figure>

<p>This basic idea goes all the way back to Isaac Newton, who pointed out
that we can optimize arbitrary functions in this way. Regardless of how
complicated our functions become, this basic approach of gradient
descent will not significantly change. The only minor changes we will
see later in this book are some handy ways we can make it faster, by
finding better steps.<a data-type="indexterm" data-startref="ch04-sto" id="idm46287180850680"></a><a data-type="indexterm" data-startref="ch04_sto2" id="idm46287180849976"></a><a data-type="indexterm" data-startref="ch04_sto4" id="idm46287180849304"></a></p>








<section data-type="sect2" data-pdf-bookmark="Calculating Gradients"><div class="sect2" id="idm46287180848504">
<h2>Calculating Gradients</h2>

<p>The one magic step is the bit where we calculate the gradients. As we<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="stochastic gradient descent" data-tertiary="calculating gradients" id="ch04_cal4"></a><a data-type="indexterm" data-primary="training" data-secondary="stochastic gradient descent" data-tertiary="calculating gradients" id="ch04_cal5"></a><a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="calculating gradients" id="ch04_cal2"></a><a data-type="indexterm" data-primary="weights" data-secondary="stochastic gradient descent" data-tertiary="calculating gradients" id="ch04_cal6"></a>
mentioned, we use calculus as a performance optimization; it allows us
to more quickly calculate whether our loss will go up or down when we
adjust our parameters up or down. In other words, the gradients will
tell us how much we have to change each weight to make our model better.</p>

<p>You may remember from your high school calculus class that the<a data-type="indexterm" data-primary="derivative of a function" id="idm46287180839752"></a><a data-type="indexterm" data-primary="parameters" data-secondary="derivative of a function" id="idm46287180839080"></a><a data-type="indexterm" data-primary="web resources" data-secondary="math tutorials" data-tertiary="derivatives" id="idm46287180838120"></a><a data-type="indexterm" data-primary="Khan Academy math tutorials online" data-secondary="derivatives" id="idm46287180836904"></a><a data-type="indexterm" data-primary="math tutorials online" data-secondary="derivatives" id="idm46287180835944"></a><a data-type="indexterm" data-primary="tutorials" data-secondary="math tutorials online" data-tertiary="derivatives" id="idm46287180835000"></a>
<em>derivative</em> of a function tells you how much a change in its parameters will change its result. If not, don’t worry; lots of
us forget calculus once high school is behind us! But you will need some intuitive understanding of what a derivative is before you
continue, so if this is all very fuzzy in your head, head over to Khan
Academy and complete the <a href="https://oreil.ly/nyd0R">lessons on basic derivatives</a>. You
won’t have to know how to calculate them yourself; you
just have to know what a derivative is.</p>

<p>The key point about a derivative is this: for any function, such as the
quadratic function we saw in the previous section, we can calculate its derivative. The
derivative is another function. It calculates the change, rather than
the value. For instance, the derivative of the quadratic function at the
value 3 tells us how rapidly the function changes at the value
3. More specifically, you may recall that
gradient is defined as <em>rise/run</em>; that is, the change in the value of<a data-type="indexterm" data-primary="gradients" data-secondary="definition as rise/run" id="idm46287180830952"></a>
the function, divided by the change in the value of the parameter. When
we know how our function will change, we know what we need to do to
make it smaller. This is the key to machine learning: having a way to<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="key to ML via derivatives" id="idm46287180803176"></a><a data-type="indexterm" data-primary="gradients" data-secondary="calculating" id="ch04_cal"></a>
change the parameters of a function to make it smaller. Calculus
provides us with a computational shortcut, the derivative, which lets us
directly calculate the gradients of our functions.</p>

<p>One important thing to be aware of is that our function has lots of weights
that we need to adjust, so when we calculate the derivative, we
won’t get back one number, but lots of them—a gradient for
every weight. But there is nothing mathematically tricky here; you can
calculate the derivative with respect to one weight and treat all the
other ones as constant, and then repeat that for each other weight. This is how
all of the gradients are calculated, for every weight.</p>

<p>We mentioned just now that you won’t have to calculate any
gradients yourself. How can that be? Amazingly enough, PyTorch is able
to automatically compute the derivative of nearly any function!
What’s more, it does it very fast. Most of the time, it will
be at least as fast as any derivative function that you can create by
hand. Let’s see an example.</p>

<p>First, let’s pick a tensor value at which we want gradients:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">xt</code> <code class="o">=</code> <code class="n">tensor</code><code class="p">(</code><code class="mf">3.</code><code class="p">)</code><code class="o">.</code><code class="n">requires_grad_</code><code class="p">()</code></pre>

<p>Notice the special method <code>requires_grad_</code>? That’s the
magical incantation we use to tell PyTorch that we want to calculate
gradients with respect to that variable at that value. It is essentially
tagging the variable, so PyTorch will remember to keep track of how to
compute gradients of the other direct calculations on it that you will
ask for.</p>
<div data-type="tip"><h1>Alexis Says</h1>
<p>This API might throw you off if you’re coming from math or physics. In those contexts, the “gradient” of a function is just another function (i.e., its derivative), so you might expect gradient-related APIs to give you a new function. But in deep learning, “gradient” usually means the <em>value</em> of a function’s derivative at a particular argument value. The PyTorch API also puts the focus on the argument, not the function you’re actually computing the gradients of. It may feel backward at first, but it’s just a different perspective.</p>
</div>

<p>Now we calculate our function with that value. Notice how PyTorch prints
not just the value calculated, but also a note that it has a gradient
function it’ll be using to calculate our gradients when
needed:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">yt</code> <code class="o">=</code> <code class="n">f</code><code class="p">(</code><code class="n">xt</code><code class="p">)</code>
<code class="n">yt</code></pre>

<pre data-type="programlisting">tensor(9., grad_fn=&lt;PowBackward0&gt;)</pre>

<p>Finally, we tell PyTorch to calculate the gradients for us:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">yt</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code></pre>

<p>The “backward” here refers to <em>backpropagation</em>, which is the name<a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="backward" id="idm46287180773992"></a><a data-type="indexterm" data-primary="SGD" data-see="stochastic gradient descent" id="idm46287180773144"></a><a data-type="indexterm" data-primary="derivative of a function" data-secondary="backpropagation" id="idm46287180778728"></a><a data-type="indexterm" data-primary="layers" data-secondary="backpropagation for derivative" id="idm46287180777768"></a>
given to the process of calculating the derivative of each layer.
We’ll see how this is done exactly in <a data-type="xref" href="ch17.html#chapter_foundations">Chapter 17</a>, when we
calculate the gradients of a deep neural net from scratch. This is
called the <em>backward pass</em> of the network, as opposed to the <em>forward
pass</em>, which is where the activations are calculated. Life would<a data-type="indexterm" data-primary="backward pass" id="idm46287180744584"></a><a data-type="indexterm" data-primary="forward pass" id="idm46287180743992"></a><a data-type="indexterm" data-primary="activations" data-secondary="forward pass" id="idm46287180743320"></a><a data-type="indexterm" data-primary="layers" data-secondary="forward pass for activations" id="idm46287180742376"></a>
probably be easier if <code>backward</code> was just called <code>calculate_grad</code>, but
deep learning folks really do like to add jargon everywhere they can!</p>

<p>We can now view the gradients by checking the <code>grad</code> attribute of our
tensor:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">xt</code><code class="o">.</code><code class="n">grad</code></pre>

<pre data-type="programlisting">tensor(6.)</pre>

<p>If you remember your high school calculus rules, the derivative of
<code>x**2</code> is <code>2*x</code>, and we have <code>x=3</code>, so the gradients should be <code>2*3=6</code>,
which is what PyTorch calculated for us!</p>

<p>Now we’ll repeat the preceding steps, but with a vector argument
for our function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">xt</code> <code class="o">=</code> <code class="n">tensor</code><code class="p">([</code><code class="mf">3.</code><code class="p">,</code><code class="mf">4.</code><code class="p">,</code><code class="mf">10.</code><code class="p">])</code><code class="o">.</code><code class="n">requires_grad_</code><code class="p">()</code>
<code class="n">xt</code></pre>

<pre data-type="programlisting">tensor([ 3.,  4., 10.], requires_grad=True)</pre>

<p>And we’ll add <code>sum</code> to our function so it can take a vector (i.e., a
rank-1 tensor) and return a scalar (i.e., a rank-0 tensor):</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">f</code><code class="p">(</code><code class="n">x</code><code class="p">):</code> <code class="k">return</code> <code class="p">(</code><code class="n">x</code><code class="o">**</code><code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>

<code class="n">yt</code> <code class="o">=</code> <code class="n">f</code><code class="p">(</code><code class="n">xt</code><code class="p">)</code>
<code class="n">yt</code></pre>

<pre data-type="programlisting">tensor(125., grad_fn=&lt;SumBackward0&gt;)</pre>

<p>Our gradients are <code>2*xt</code>, as we’d expect!</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">yt</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
<code class="n">xt</code><code class="o">.</code><code class="n">grad</code></pre>

<pre data-type="programlisting">tensor([ 6.,  8., 20.])</pre>

<p>The gradients tell us only the slope of our function; they don’t tell us exactly how far to adjust the
parameters. But they do give us some idea of how far: if the slope is very
large, that may suggest that we have more adjustments to do,
whereas if the slope is very small, that may suggest that we are close
to the optimal value.<a data-type="indexterm" data-startref="ch04_cal" id="idm46287180598968"></a><a data-type="indexterm" data-startref="ch04_cal2" id="idm46287180598328"></a><a data-type="indexterm" data-startref="ch04_cal4" id="idm46287180597656"></a><a data-type="indexterm" data-startref="ch04_cal5" id="idm46287180596984"></a><a data-type="indexterm" data-startref="ch04_cal6" id="idm46287180596312"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Stepping with a Learning Rate"><div class="sect2" id="idm46287180595384">
<h2>Stepping with a Learning Rate</h2>

<p>Deciding how to change our parameters based on the values of the
gradients is an important part of the deep learning process. Nearly all<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="stochastic gradient descent" data-tertiary="stepping with learning rate" id="ch04_LR2"></a><a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="stepping with learning rate" id="ch04_LR3"></a><a data-type="indexterm" data-primary="training" data-secondary="stochastic gradient descent" data-tertiary="stepping with learning rate" id="ch04_LR4"></a><a data-type="indexterm" data-primary="weights" data-secondary="stochastic gradient descent" data-tertiary="stepping with learning rate" id="ch04_LR5"></a><a data-type="indexterm" data-primary="learning rate (LR)" data-secondary="about" id="ch04_LR"></a><a data-type="indexterm" data-primary="training" data-secondary="learning rate" id="ch04_LR6"></a>
approaches start with the basic idea of multiplying the gradient by some
small number, called the <em>learning rate</em> (LR). The learning rate is
often a number between 0.001 and 0.1, although it could be anything.
Often people select a learning rate just by trying a few, and finding
which results in the best model after training (we’ll show
you a better approach later in this book, called the <em>learning rate
finder</em>). Once you’ve picked a learning rate, you can adjust
your parameters using this simple function:</p>
<pre>w -= w.grad * lr</pre>

<p>This is known as <em>stepping</em> your parameters, using an <em>optimization step</em>.</p>

<p>If you pick a learning rate that’s too low, it can mean
having to do a lot of steps. <a data-type="xref" href="#descent_small">Figure 4-2</a> illustrates that.</p>

<figure class="width-50 no-frame"><div id="descent_small" class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_0402.png" alt="An illustration of gradient descent with a LR too low" width="999" height="753">
<h6><span class="label">Figure 4-2. </span>Gradient descent with low LR</h6>
</div></figure>

<p class="pagebreak-before">But picking a learning rate that’s too high is even
worse—it can result in the loss getting <em>worse</em>, as we see in
<a data-type="xref" href="#descent_div">Figure 4-3</a>!</p>

<figure class="width-50 no-frame"><div id="descent_div" class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_0403.png" alt="An illustration of gradient descent with a LR too high" width="999" height="753">
<h6><span class="label">Figure 4-3. </span>Gradient descent with high LR</h6>
</div></figure>

<p>If the learning rate is too high, it may also “bounce” around, rather
than diverging; <a data-type="xref" href="#descent_bouncy">Figure 4-4</a> shows how this results in taking many steps to train successfully.<a data-type="indexterm" data-startref="ch04_LR" id="idm46287180538808"></a><a data-type="indexterm" data-startref="ch04_LR2" id="idm46287180538136"></a><a data-type="indexterm" data-startref="ch04_LR3" id="idm46287180537464"></a><a data-type="indexterm" data-startref="ch04_LR4" id="idm46287180536792"></a><a data-type="indexterm" data-startref="ch04_LR5" id="idm46287180536120"></a><a data-type="indexterm" data-startref="ch04_LR6" id="idm46287180535448"></a></p>

<figure class="width-50 no-frame"><div id="descent_bouncy" class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_0404.png" alt="An illustation of gradient descent with a bouncy LR" width="998" height="753">
<h6><span class="label">Figure 4-4. </span>Gradient descent with bouncy LR</h6>
</div></figure>

<p>Now let’s apply all of this in an end-to-end example.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="An End-to-End SGD Example"><div class="sect2" id="idm46287180567368">
<h2>An End-to-End SGD Example</h2>

<p>We’ve seen how to use gradients to minimize our loss. Now<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="stochastic gradient descent" data-tertiary="example end-to-end" id="ch04-ee"></a><a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="example end-to-end" id="ch04-ee2"></a><a data-type="indexterm" data-primary="training" data-secondary="stochastic gradient descent" data-tertiary="example end-to-end" id="ch04_ee3"></a><a data-type="indexterm" data-primary="weights" data-secondary="stochastic gradient descent" data-tertiary="example end-to-end" id="ch04_ee4"></a>
it’s time to look at an SGD example and see how finding a
minimum can be used to train a model to fit data 
<span class="keep-together">better</span>.</p>

<p>Let’s start with a simple, synthetic example model. Imagine you were
measuring the speed of a roller coaster as it went over the top of a
hump. It would start fast, and then get slower as it went up the hill; it would be slowest at the top, and it would then speed up again
as it went downhill. You want to build a model of how the speed changes
over time. If you were measuring the speed manually every
second for 20 seconds, it might look something like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">time</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">20</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">();</code> <code class="n">time</code></pre>

<pre data-type="programlisting">tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,
 &gt; 14., 15., 16., 17., 18., 19.])</pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">speed</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">20</code><code class="p">)</code><code class="o">*</code><code class="mi">3</code> <code class="o">+</code> <code class="mf">0.75</code><code class="o">*</code><code class="p">(</code><code class="n">time</code><code class="o">-</code><code class="mf">9.5</code><code class="p">)</code><code class="o">**</code><code class="mi">2</code> <code class="o">+</code> <code class="mi">1</code>
<code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">time</code><code class="p">,</code><code class="n">speed</code><code class="p">);</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in11.png" alt="" width="371" height="251">
<h6></h6>
</div></figure>

<p>We’ve added a bit of random noise, since measuring things
manually isn’t precise. This means it’s not that
easy to answer the question: what was the roller coaster’s
speed? Using SGD, we can try to find a function that matches our
observations. We can’t consider every possible function, so
let’s use a guess that it will be quadratic; i.e., a function
of the form <code>a*(time**2)+(b*time)+c</code>.</p>

<p>We want to distinguish clearly between the function’s input
(the time when we are measuring the coaster’s speed) and its
parameters (the values that define <em>which</em> quadratic we’re
trying). So let’s collect the parameters in one argument and thus
separate the input, <code>t</code>, and the parameters, <code>params</code>, in the
function’s signature:<a data-type="indexterm" data-primary="signature of function" data-secondary="creating" id="idm46287180457384"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">f</code><code class="p">(</code><code class="n">t</code><code class="p">,</code> <code class="n">params</code><code class="p">):</code>
    <code class="n">a</code><code class="p">,</code><code class="n">b</code><code class="p">,</code><code class="n">c</code> <code class="o">=</code> <code class="n">params</code>
    <code class="k">return</code> <code class="n">a</code><code class="o">*</code><code class="p">(</code><code class="n">t</code><code class="o">**</code><code class="mi">2</code><code class="p">)</code> <code class="o">+</code> <code class="p">(</code><code class="n">b</code><code class="o">*</code><code class="n">t</code><code class="p">)</code> <code class="o">+</code> <code class="n">c</code></pre>

<p>In other words, we’ve restricted the problem of finding the
best imaginable function that fits the data to finding the best
<em>quadratic</em> function. This greatly simplifies the problem, since every
quadratic function is fully defined by the three parameters <code>a</code>, <code>b</code>,
and <code>c</code>. Thus, to find the best quadratic function, we need to find only
the best values for <code>a</code>, <code>b</code>, and <code>c</code>.</p>

<p>If we can solve this problem for the three parameters of a quadratic
function, we’ll be able to apply the same approach for
other, more complex functions with more parameters—such as a neural net.
Let’s find the parameters for <code>f</code> first, and then
we’ll come back and do the same thing for the MNIST dataset
with a neural net.</p>

<p>We need to define first what we mean by “best.” We define this
precisely by choosing a <em>loss function</em>, which will return a value based
on a prediction and a target, where lower values of the function
correspond to “better” predictions. For continuous data,
it’s common to use <em>mean squared error</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">mse</code><code class="p">(</code><code class="n">preds</code><code class="p">,</code> <code class="n">targets</code><code class="p">):</code> <code class="k">return</code> <code class="p">((</code><code class="n">preds</code><code class="o">-</code><code class="n">targets</code><code class="p">)</code><code class="o">**</code><code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code></pre>

<p>Now, let’s work through our seven-step process.</p>










<section data-type="sect3" data-pdf-bookmark="Step 1: Initialize the parameters"><div class="sect3" id="idm46287180379304">
<h3>Step 1: Initialize the parameters</h3>

<p>First, we initialize the parameters to random values and tell PyTorch
that we want to track their gradients using <code>requires_grad_</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">params</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code><code class="o">.</code><code class="n">requires_grad_</code><code class="p">()</code></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Step 2: Calculate the predictions"><div class="sect3" id="idm46287180318696">
<h3>Step 2: Calculate the predictions</h3>

<p>Next, we calculate the predictions:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">preds</code> <code class="o">=</code> <code class="n">f</code><code class="p">(</code><code class="n">time</code><code class="p">,</code> <code class="n">params</code><code class="p">)</code></pre>

<p>Let’s create a little function to see how close our
predictions are to our targets, and take a look:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">show_preds</code><code class="p">(</code><code class="n">preds</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="bp">None</code><code class="p">):</code>
    <code class="k">if</code> <code class="n">ax</code> <code class="ow">is</code> <code class="bp">None</code><code class="p">:</code> <code class="n">ax</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">()[</code><code class="mi">1</code><code class="p">]</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">time</code><code class="p">,</code> <code class="n">speed</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">time</code><code class="p">,</code> <code class="n">to_np</code><code class="p">(</code><code class="n">preds</code><code class="p">),</code> <code class="n">color</code><code class="o">=</code><code class="s1">'red'</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">300</code><code class="p">,</code><code class="mi">100</code><code class="p">)</code></pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">show_preds</code><code class="p">(</code><code class="n">preds</code><code class="p">)</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in12.png" alt="" width="399" height="256">
<h6></h6>
</div></figure>

<p>This doesn’t look very close—our random parameters suggest
that the roller coaster will end up going backward, since we have
negative speeds!</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Step 3: Calculate the loss"><div class="sect3" id="idm46287180185752">
<h3>Step 3: Calculate the loss</h3>

<p>We calculate the loss as follows:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">loss</code> <code class="o">=</code> <code class="n">mse</code><code class="p">(</code><code class="n">preds</code><code class="p">,</code> <code class="n">speed</code><code class="p">)</code>
<code class="n">loss</code></pre>

<pre data-type="programlisting">tensor(25823.8086, grad_fn=&lt;MeanBackward0&gt;)</pre>

<p>Our goal is now to improve this. To do that, we’ll need to
know the gradients.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Step 4: Calculate the gradients"><div class="sect3" id="idm46287180142392">
<h3>Step 4: Calculate the gradients</h3>

<p>The next step is to calculate the gradients, or an approximation of how the parameters need to change:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
<code class="n">params</code><code class="o">.</code><code class="n">grad</code></pre>

<pre data-type="programlisting">tensor([-53195.8594,  -3419.7146,   -253.8908])</pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">params</code><code class="o">.</code><code class="n">grad</code> <code class="o">*</code> <code class="mf">1e-5</code></pre>

<pre data-type="programlisting">tensor([-0.5320, -0.0342, -0.0025])</pre>

<p>We can use these gradients to improve our parameters. We’ll
need to pick a learning rate (we’ll discuss how to do that
in practice in the next chapter; for now, we’ll just use
1e-5 or 0.00001):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">params</code></pre>

<pre data-type="programlisting">tensor([-0.7658, -0.7506,  1.3525], requires_grad=True)</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Step 5: Step the weights"><div class="sect3" id="idm46287180087368">
<h3>Step 5: Step the weights</h3>

<p>Now we need to update the parameters based
on the gradients we just calculated:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">lr</code> <code class="o">=</code> <code class="mf">1e-5</code>
<code class="n">params</code><code class="o">.</code><code class="n">data</code> <code class="o">-=</code> <code class="n">lr</code> <code class="o">*</code> <code class="n">params</code><code class="o">.</code><code class="n">grad</code><code class="o">.</code><code class="n">data</code>
<code class="n">params</code><code class="o">.</code><code class="n">grad</code> <code class="o">=</code> <code class="bp">None</code></pre>
<div data-type="tip"><h1>Alexis Says</h1>
<p>Understanding this bit depends on remembering recent history. To calculate the gradients, we call <code>backward</code> on the <code>loss</code>. But this <code>loss</code> was itself calculated by <code>mse</code>, which in turn took <code>preds</code> as an input, which was calculated using <code>f</code> taking as an input <code>params</code>, which was the object on which we originally called <code>required_grads_</code>—which is the original call that now allows us to call <code>backward</code> on <code>loss</code>. This chain of function calls represents the mathematical composition of functions, which enables PyTorch to use calculus’s chain rule under the hood to calculate these gradients.</p>
</div>

<p>Let’s see if the loss has improved:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">preds</code> <code class="o">=</code> <code class="n">f</code><code class="p">(</code><code class="n">time</code><code class="p">,</code><code class="n">params</code><code class="p">)</code>
<code class="n">mse</code><code class="p">(</code><code class="n">preds</code><code class="p">,</code> <code class="n">speed</code><code class="p">)</code></pre>

<pre data-type="programlisting">tensor(5435.5366, grad_fn=&lt;MeanBackward0&gt;)</pre>

<p>And take a look at the plot:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">show_preds</code><code class="p">(</code><code class="n">preds</code><code class="p">)</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in13.png" alt="" width="399" height="256">
<h6></h6>
</div></figure>

<p>We need to repeat this a few times, so we’ll create a
function to apply one step:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">apply_step</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">prn</code><code class="o">=</code><code class="bp">True</code><code class="p">):</code>
    <code class="n">preds</code> <code class="o">=</code> <code class="n">f</code><code class="p">(</code><code class="n">time</code><code class="p">,</code> <code class="n">params</code><code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">mse</code><code class="p">(</code><code class="n">preds</code><code class="p">,</code> <code class="n">speed</code><code class="p">)</code>
    <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
    <code class="n">params</code><code class="o">.</code><code class="n">data</code> <code class="o">-=</code> <code class="n">lr</code> <code class="o">*</code> <code class="n">params</code><code class="o">.</code><code class="n">grad</code><code class="o">.</code><code class="n">data</code>
    <code class="n">params</code><code class="o">.</code><code class="n">grad</code> <code class="o">=</code> <code class="bp">None</code>
    <code class="k">if</code> <code class="n">prn</code><code class="p">:</code> <code class="k">print</code><code class="p">(</code><code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">())</code>
    <code class="k">return</code> <code class="n">preds</code></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Step 6: Repeat the process"><div class="sect3" id="idm46287180082408">
<h3>Step 6: Repeat the process</h3>

<p>Now we iterate. By looping and performing many
improvements, we hope to reach a good result:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">):</code> <code class="n">apply_step</code><code class="p">(</code><code class="n">params</code><code class="p">)</code></pre>

<pre data-type="programlisting">5435.53662109375
1577.4495849609375
847.3780517578125
709.22265625
683.0757446289062
678.12451171875
677.1839599609375
677.0025024414062
676.96435546875
676.9537353515625</pre>

<p>The loss is going down, just as we hoped! But looking only at these loss
numbers disguises the fact that each iteration represents an entirely
different quadratic function being tried, on the way to finding the best
possible quadratic function. We can see this process visually if,
instead of printing out the loss function, we plot the function at every
step. Then we can see how the shape is approaching the best possible
quadratic function for our data:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">_</code><code class="p">,</code><code class="n">axs</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">4</code><code class="p">,</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code><code class="mi">3</code><code class="p">))</code>
<code class="k">for</code> <code class="n">ax</code> <code class="ow">in</code> <code class="n">axs</code><code class="p">:</code> <code class="n">show_preds</code><code class="p">(</code><code class="n">apply_step</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="bp">False</code><code class="p">),</code> <code class="n">ax</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in14.png" alt="" width="837" height="186">
<h6></h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Step 7: Stop"><div class="sect3" id="idm46287179762008">
<h3>Step 7: Stop</h3>

<p>We just decided to stop after 10 epochs
arbitrarily. In practice, we would watch the training and validation losses
and our metrics to decide when to stop, as we’ve 
<span class="keep-together">discussed</span>.<a data-type="indexterm" data-startref="ch04-ee" id="idm46287179759800"></a><a data-type="indexterm" data-startref="ch04-ee2" id="idm46287179759064"></a><a data-type="indexterm" data-startref="ch04_ee3" id="idm46287179758392"></a><a data-type="indexterm" data-startref="ch04_ee4" id="idm46287179757720"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Summarizing Gradient Descent"><div class="sect2" id="idm46287180531160">
<h2>Summarizing Gradient Descent</h2>

<p>Now that you’ve seen what happens in each step, let’s take another<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="stochastic gradient descent" data-tertiary="summarizing" id="idm46287179755768"></a><a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="summarizing" id="idm46287179754584"></a><a data-type="indexterm" data-primary="training" data-secondary="stochastic gradient descent" data-tertiary="summarizing" id="idm46287179753672"></a><a data-type="indexterm" data-primary="weights" data-secondary="stochastic gradient descent" data-tertiary="summarizing" id="idm46287179752488"></a><a data-type="indexterm" data-primary="gradient descent" id="idm46287179751304"></a><a data-type="indexterm" data-primary="optimization" data-secondary="gradient descent" id="idm46287179750632"></a> look at our graphical representation of the gradient descent process (<a data-type="xref" href="#gradient_descent_process">Figure 4-5</a>) and do a quick recap.</p>

<figure><div id="gradient_descent_process" class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_0405.png" alt="Graph showing the steps for Gradient Descent" width="1427" height="246">
<h6><span class="label">Figure 4-5. </span>The gradient descent process</h6>
</div></figure>

<p>At the beginning, the weights of our model can be random<a data-type="indexterm" data-primary="weights" data-secondary="random in training from scratch" id="idm46287179746152"></a><a data-type="indexterm" data-primary="weights" data-secondary="transfer learning" data-tertiary="pretrained models" id="idm46287179745160"></a><a data-type="indexterm" data-primary="transfer learning" data-secondary="weights" id="idm46287179743944"></a><a data-type="indexterm" data-primary="pretrained models" data-secondary="transfer learning" id="idm46287179743000"></a>
(training <em>from scratch</em>) or come from a pretrained model (<em>transfer
learning</em>). In the first case, the output we will get from our inputs
won’t have anything to do with what we want, and even in the
second case, it’s likely the pretrained model
won’t be very good at the specific task we are targeting. So
the model will need to <em>learn</em> better weights.</p>

<p class="pagebreak-before">We begin by comparing the outputs the model gives us with our
targets (we have labeled data, so we know what result the model should
give) using a <em>loss function</em>, which returns a number that we want to make as low as possible by improving our weights. To do this, we take
a few data items (such as images) from the training set and feed them to our model. We compare the corresponding targets using our
loss function, and the score we get tells us how wrong our predictions were.
We then change the weights a little bit to make it slightly better.</p>

<p>To find how to change the weights to make the loss a bit better, we use
calculus to calculate the <em>gradients</em>. (Actually, we let PyTorch do it
for us!) Let’s consider an analogy. Imagine you are lost in the mountains with
your car parked at the lowest point. To find your way back to it, you might wander
in a random direction, but that probably wouldn’t help much.
Since you know your vehicle is at the lowest point, you would be better
off going downhill. By always taking a step in the direction of the steepest
downward slope, you should eventually arrive at your destination. We use
the magnitude of the gradient (i.e., the steepness of the slope) to tell
us how big a step to take; specifically, we multiply the gradient by a
number we choose called the <em>learning rate</em> to decide on the step size. We then <em>iterate</em> until we have reached the lowest point, which will be our parking lot; then we can <em>stop</em>.<a data-type="indexterm" data-startref="ch04_sto3" id="idm46287179735976"></a></p>

<p>All of what we just saw can be transposed directly to the MNIST dataset, except for the loss function. Let’s now see how we can define a good training objective.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="The MNIST Loss Function"><div class="sect1" id="idm46287181003928">
<h1>The MNIST Loss Function</h1>

<p>We already have our <code>x</code>s—that is, our independent variables, the images themselves.<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="MNIST loss function" id="ch04_mnist"></a><a data-type="indexterm" data-primary="loss" data-secondary="numerical digit image classifier" id="ch04_mnist3"></a><a data-type="indexterm" data-primary="loss" data-secondary="MNIST loss function" id="ch04_mnist4"></a>
We’ll concatenate them all into a single tensor, and also
change them from a list of matrices (a rank-3 tensor) to a list of
vectors (a rank-2 tensor). We can do this using <code>view</code>, which is a
PyTorch method that changes the shape of a tensor without changing its
contents. <code>-1</code> is a special parameter to <code>view</code> that means “make this
axis as big as necessary to fit all the data”:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">train_x</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">([</code><code class="n">stacked_threes</code><code class="p">,</code> <code class="n">stacked_sevens</code><code class="p">])</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">)</code></pre>

<p>We need a label for each image. We’ll use <code>1</code> for 3s and <code>0</code>
for 7s:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">train_y</code> <code class="o">=</code> <code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">]</code><code class="o">*</code><code class="nb">len</code><code class="p">(</code><code class="n">threes</code><code class="p">)</code> <code class="o">+</code> <code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">*</code><code class="nb">len</code><code class="p">(</code><code class="n">sevens</code><code class="p">))</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="n">train_x</code><code class="o">.</code><code class="n">shape</code><code class="p">,</code><code class="n">train_y</code><code class="o">.</code><code class="n">shape</code></pre>

<pre data-type="programlisting">(torch.Size([12396, 784]), torch.Size([12396, 1]))</pre>

<p class="pagebreak-before">A <code>Dataset</code> in PyTorch is required to return a tuple of <code>(x,y)</code> when
indexed. Python provides a <code>zip</code> function that, when combined with
<code>list</code>, provides a simple way to get this functionality:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dset</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">train_x</code><code class="p">,</code><code class="n">train_y</code><code class="p">))</code>
<code class="n">x</code><code class="p">,</code><code class="n">y</code> <code class="o">=</code> <code class="n">dset</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">,</code><code class="n">y</code></pre>

<pre data-type="programlisting">(torch.Size([784]), tensor([1]))</pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">valid_x</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">([</code><code class="n">valid_3_tens</code><code class="p">,</code> <code class="n">valid_7_tens</code><code class="p">])</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">)</code>
<code class="n">valid_y</code> <code class="o">=</code> <code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">]</code><code class="o">*</code><code class="nb">len</code><code class="p">(</code><code class="n">valid_3_tens</code><code class="p">)</code> <code class="o">+</code> <code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">*</code><code class="nb">len</code><code class="p">(</code><code class="n">valid_7_tens</code><code class="p">))</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="n">valid_dset</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">valid_x</code><code class="p">,</code><code class="n">valid_y</code><code class="p">))</code></pre>

<p>Now we need an (initially random) weight for every pixel (this is the
<em>initialize</em> step in our seven-step process):</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">init_params</code><code class="p">(</code><code class="n">size</code><code class="p">,</code> <code class="n">std</code><code class="o">=</code><code class="mf">1.0</code><code class="p">):</code> <code class="k">return</code> <code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="n">size</code><code class="p">)</code><code class="o">*</code><code class="n">std</code><code class="p">)</code><code class="o">.</code><code class="n">requires_grad_</code><code class="p">()</code></pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">weights</code> <code class="o">=</code> <code class="n">init_params</code><code class="p">((</code><code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">,</code><code class="mi">1</code><code class="p">))</code></pre>

<p>The function <code>weights*pixels</code> won’t be flexible enough—it is
always equal to 0 when the pixels are equal to 0
(i.e., its <em>intercept</em> is 0). You might remember from
high school math that the formula for a line is <code>y=w*x+b</code>; we still need
the <code>b</code>. We’ll initialize it to a random number too:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">bias</code> <code class="o">=</code> <code class="n">init_params</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre>

<p>In neural networks, the <code>w</code> in the equation <code>y=w*x+b</code> is called the
<em>weights</em>, and the <code>b</code> is called the <em>bias</em>. Together, the weights and
bias make up the <em>parameters</em>.</p>
<div data-type="note" epub:type="note"><h1>Jargon: Parameters</h1>
<p>The <em>weights</em> and <em>biases</em> of a model. The weights are the <code>w</code> in the equation <code>w*x+b</code>, and the biases are the <code>b</code> in that equation.</p>
</div>

<p>We can now calculate a prediction for one image:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">(</code><code class="n">train_x</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">*</code><code class="n">weights</code><code class="o">.</code><code class="n">T</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code> <code class="o">+</code> <code class="n">bias</code></pre>

<pre data-type="programlisting">tensor([20.2336], grad_fn=&lt;AddBackward0&gt;)</pre>

<p>While we could use a Python <code>for</code> loop to calculate the prediction for<a data-type="indexterm" data-primary="Python" data-secondary="loop inefficiency" id="idm46287179351720"></a>
each image, that would be very slow. Because Python loops
don’t run on the GPU, and because Python is a slow language
for loops in general, we need to represent as much of the computation in
a model as possible using higher-level functions.</p>

<p>In this case, there’s an extremely convenient mathematical<a data-type="indexterm" data-primary="matrix multiplication" id="idm46287179350040"></a><a data-type="indexterm" data-primary="tensors" data-secondary="matrix multiplication" id="idm46287179349336"></a>
operation that calculates <code>w*x</code> for every row of a
matrix—it’s called <em>matrix multiplication</em>.
<a data-type="xref" href="#matmul">Figure 4-6</a> shows what matrix multiplication looks like.</p>

<figure class="width-50"><div id="matmul" class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_0406.png" alt="Matrix multiplication" width="1055" height="1029">
<h6><span class="label">Figure 4-6. </span>Matrix multiplication</h6>
</div></figure>

<p>This image shows two matrices, <code>A</code> and <code>B</code>, being multiplied together.
Each item of the result, which we’ll call <code>AB</code>, contains
each item of its corresponding row of <code>A</code> multiplied by each item of its
corresponding column of <code>B</code>, added together. For instance, row 1, column
2 (the yellow dot with a red border) is calculated as
<math xmlns="http://www.w3.org/1998/Math/MathML" alttext="a Subscript 1 comma 1 Baseline asterisk b Subscript 1 comma 2 plus a Subscript 1 comma 2 Baseline asterisk b Subscript 2 comma 2">
  <mrow>
    <msub><mi>a</mi> <mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow> </msub>
    <mo>*</mo>
    <msub><mi>b</mi> <mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow> </msub>
    <mo>+</mo>
    <msub><mi>a</mi> <mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow> </msub>
    <mo>*</mo>
    <msub><mi>b</mi> <mrow><mn>2</mn><mo>,</mo><mn>2</mn></mrow> </msub>
  </mrow>
</math>. If you need a
refresher on matrix multiplication, we suggest you take a look at the
<a href="https://oreil.ly/w0XKS">“Intro to Matrix Multiplication”</a> on Khan Academy, since
this is the most important mathematical operation in deep learning.</p>

<p>In Python, matrix multiplication is represented with the <code>@</code> operator.
Let’s try it:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">linear1</code><code class="p">(</code><code class="n">xb</code><code class="p">):</code> <code class="k">return</code> <code class="n">xb</code><code class="nd">@weights</code> <code class="o">+</code> <code class="n">bias</code>
<code class="n">preds</code> <code class="o">=</code> <code class="n">linear1</code><code class="p">(</code><code class="n">train_x</code><code class="p">)</code>
<code class="n">preds</code></pre>

<pre data-type="programlisting">tensor([[20.2336],
        [17.0644],
        [15.2384],
        ...,
        [18.3804],
        [23.8567],
        [28.6816]], grad_fn=&lt;AddBackward0&gt;)</pre>

<p>The first element is the same as we calculated before, as<a data-type="indexterm" data-primary="neural networks" data-secondary="fundamental weights and bias equation" id="idm46287179262664"></a>
we’d expect. This equation, <code>batch @ weights + bias</code>, is one
of the two fundamental equations of any neural network (the other one is
the <em>activation function</em>, which we’ll see in a moment).</p>

<p>Let’s check our accuracy. To decide if an output represents
a 3 or a 7, we can just check whether it’s greater than
0, so our accuracy for each item can be calculated (using
broadcasting, so no loops!) as follows:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">corrects</code> <code class="o">=</code> <code class="p">(</code><code class="n">preds</code><code class="o">&gt;</code><code class="mf">0.0</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code> <code class="o">==</code> <code class="n">train_y</code>
<code class="n">corrects</code></pre>

<pre data-type="programlisting">tensor([[ True],
        [ True],
        [ True],
        ...,
        [False],
        [False],
        [False]])</pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">corrects</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="o">.</code><code class="n">item</code><code class="p">()</code></pre>

<pre data-type="programlisting">0.4912068545818329</pre>

<p>Now let’s see what the change in accuracy is for a small
change in one of the weights:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">weights</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*=</code> <code class="mf">1.0001</code></pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">preds</code> <code class="o">=</code> <code class="n">linear1</code><code class="p">(</code><code class="n">train_x</code><code class="p">)</code>
<code class="p">((</code><code class="n">preds</code><code class="o">&gt;</code><code class="mf">0.0</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code> <code class="o">==</code> <code class="n">train_y</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="o">.</code><code class="n">item</code><code class="p">()</code></pre>

<pre data-type="programlisting">0.4912068545818329</pre>

<p>As we’ve seen, we need gradients in order to improve our
model using SGD, and in order to calculate gradients we need a <em>loss
function</em> that represents how good our model is. That is because the
gradients are a measure of how that loss function changes with small
tweaks to the weights.</p>

<p>So, we need to choose a loss function. The obvious approach would be to
use accuracy, which is our metric, as our loss function as well. In this
case, we would calculate our prediction for each image, collect these
values to calculate an overall accuracy, and then calculate the
gradients of each weight with respect to that overall accuracy.</p>

<p>Unfortunately, we have a significant technical problem here. The
gradient of a function is its <em>slope</em>, or its steepness, which can be
defined as <em>rise over run</em>—that is, how much the value of the function<a data-type="indexterm" data-primary="gradients" data-secondary="definition as rise/run" id="idm46287179120232"></a>
goes up or down, divided by how much we changed the input. We can write
this mathematically as:</p>

<pre data-type="programlisting">(y_new – y_old) / (x_new – x_old)</pre>

<p>This gives a good approximation of the gradient when <code>x_new</code> is very similar to <code>x_old</code>, meaning that their difference is very small. But accuracy changes at all only when a
prediction changes from a 3 to a 7, or vice versa. The problem is
that a small change in weights from <code>x_old</code> to <code>x_new</code> isn’t
likely to cause any prediction to change, so <code>(y_new – y_old)</code> will almost always be 0.
In other words, the gradient is 0 almost everywhere.</p>

<p>A very small change in the value of a weight will often not change the accuracy at all. This means it is not useful to use
accuracy as a loss function—if we do,
most of the time our gradients will be 0, and the model will
not be able to learn from that 
<span class="keep-together">number</span>.</p>
<div data-type="tip"><h1>Sylvain Says</h1>
<p>In mathematical terms, accuracy is a function that is constant almost everywhere (except at the threshold, 0.5), so its derivative is nil almost everywhere (and infinity at the threshold). This then gives gradients that are 0 or infinite, which are useless for updating the model.</p>
</div>

<p>Instead, we need a loss function that, when our weights result in
slightly better predictions, gives us a slightly better loss. So what
does a “slightly better prediction” look like, exactly? Well, in this
case, it means that if the correct answer is a 3, the score is a
little higher, or if the correct answer is a 7, the score is a
little lower.</p>

<p>Let’s write such a function now. What form does it take?</p>

<p>The loss function receives not the images themselves, but the predictions
from the model. So let’s make one argument, <code>prds</code>, of values between 0 and 1, where each value is the prediction that an image is a 3. It is a vector (i.e., a rank-1 tensor) indexed over the images.</p>

<p>The purpose of the loss function is to measure the difference between
predicted values and the true values—that is, the targets (aka
labels). Let’s therefore make another
argument, <code>trgts</code>, with values of 0 or 1 that tells whether an image actually is a 3 or not. It is also a vector (i.e., another rank-1 tensor) indexed over the images.</p>

<p>For instance, suppose we had three images that we knew were a 3, a
7, and a 3. And suppose our model predicted with high confidence (<code>0.9</code>) that
the first was a 3, with slight confidence (<code>0.4</code>) that the second was a 7, and
with fair confidence (<code>0.2</code>), but incorrectly, that the last was a 7. This
would mean our loss function would receive these values as its inputs:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">trgts</code>  <code class="o">=</code> <code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">])</code>
<code class="n">prds</code>   <code class="o">=</code> <code class="n">tensor</code><code class="p">([</code><code class="mf">0.9</code><code class="p">,</code> <code class="mf">0.4</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">])</code></pre>

<p>Here’s a first try at a loss function that measures the
distance between <code>predictions</code> and <code>targets</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">mnist_loss</code><code class="p">(</code><code class="n">predictions</code><code class="p">,</code> <code class="n">targets</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">torch</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">targets</code><code class="o">==</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="o">-</code><code class="n">predictions</code><code class="p">,</code> <code class="n">predictions</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code></pre>

<p>We’re using a new function, <code>torch.where(a,b,c)</code>. This is
the same as running the list comprehension
<code>[b[i] if a[i] else c[i] for i in range(len(a))]</code>, except it works on
tensors, at C/CUDA speed. In plain English, this function will measure
how distant each prediction is from 1 if it should be 1, and how distant
it is from 0 if it should be 0, and then it will take the mean of all
those distances.</p>
<div data-type="note" epub:type="note"><h1>Read the Docs</h1>
<p>It’s important to learn about PyTorch functions like this, because looping over tensors in Python performs at Python speed, not C/CUDA speed! Try running <code>help(torch.where)</code> now to read the docs for this function,
or, better still, look it up on the PyTorch documentation site.</p>
</div>

<p>Let’s try it on our <code>prds</code> and <code>trgts</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">torch</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">trgts</code><code class="o">==</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="o">-</code><code class="n">prds</code><code class="p">,</code> <code class="n">prds</code><code class="p">)</code></pre>

<pre data-type="programlisting">tensor([0.1000, 0.4000, 0.8000])</pre>

<p>You can see that this function returns a lower number when predictions
are more accurate, when accurate predictions are more confident (higher
absolute values), and when inaccurate predictions are less confident. In
PyTorch, we always assume that a lower value of a loss function is
better. Since we need a scalar for the final loss, <code>mnist_loss</code> takes the mean of the previous tensor:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mnist_loss</code><code class="p">(</code><code class="n">prds</code><code class="p">,</code><code class="n">trgts</code><code class="p">)</code></pre>

<pre data-type="programlisting">tensor(0.4333)</pre>

<p>For instance, if we change our prediction for the one “false” target
from <code>0.2</code> to <code>0.8</code>, the loss will go down, indicating that this is a
better prediction:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mnist_loss</code><code class="p">(</code><code class="n">tensor</code><code class="p">([</code><code class="mf">0.9</code><code class="p">,</code> <code class="mf">0.4</code><code class="p">,</code> <code class="mf">0.8</code><code class="p">]),</code><code class="n">trgts</code><code class="p">)</code></pre>

<pre data-type="programlisting">tensor(0.2333)</pre>

<p>One problem with <code>mnist_loss</code> as currently defined is that it assumes that
predictions are always between 0 and 1. We need to ensure, then,
that this is actually the case! As it happens, there is a function that
does exactly that—let’s take a look.</p>








<section data-type="sect2" data-pdf-bookmark="Sigmoid"><div class="sect2" id="idm46287178917960">
<h2>Sigmoid</h2>

<p>The <code>sigmoid</code> function always outputs a number between 0 and 1. It’s defined as 
<span class="keep-together">follows</span>:<a data-type="indexterm" data-primary="sigmoid function" data-secondary="binary decision" id="idm46287178915096"></a><a data-type="indexterm" data-primary="loss" data-secondary="numerical digit image classifier" data-tertiary="sigmoid function" id="idm46287178914088"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">sigmoid</code><code class="p">(</code><code class="n">x</code><code class="p">):</code> <code class="k">return</code> <code class="mi">1</code><code class="o">/</code><code class="p">(</code><code class="mi">1</code><code class="o">+</code><code class="n">torch</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="o">-</code><code class="n">x</code><code class="p">))</code></pre>

<p>PyTorch defines an accelerated version for us, so we don’t really need our own. This is an important function in deep learning, since
we often want to ensure that values are between 0 and 1. This is what it
looks like:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plot_function</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">,</code> <code class="n">title</code><code class="o">=</code><code class="s1">'Sigmoid'</code><code class="p">,</code> <code class="nb">min</code><code class="o">=-</code><code class="mi">4</code><code class="p">,</code> <code class="nb">max</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in15.png" alt="" width="375" height="268">
<h6></h6>
</div></figure>

<p>As you can see, it takes any input value, positive or negative, and
smooshes it into an output value between 0 and 1. It’s also
a smooth curve that only goes up, which makes it easier for SGD to find
meaningful gradients.</p>

<p>Let’s update <code>mnist_loss</code> to first apply <code>sigmoid</code> to the
inputs:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">mnist_loss</code><code class="p">(</code><code class="n">predictions</code><code class="p">,</code> <code class="n">targets</code><code class="p">):</code>
    <code class="n">predictions</code> <code class="o">=</code> <code class="n">predictions</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">()</code>
    <code class="k">return</code> <code class="n">torch</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">targets</code><code class="o">==</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="o">-</code><code class="n">predictions</code><code class="p">,</code> <code class="n">predictions</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code></pre>

<p>Now we can be confident our loss function will work, even if the
predictions are not between 0 and 1. All that is required is that a
higher prediction corresponds to higher confidence.</p>

<p>Having defined a loss function, now is a good moment to recapitulate why
we did this. After all, we already had a metric, which was overall
accuracy. So why did we define a loss?</p>

<p>The key difference is that the metric is to drive human understanding
and the loss is to drive automated learning. To drive automated
learning, the loss must be a function that has a meaningful derivative.
It can’t have big flat sections and large jumps, but
instead must be reasonably smooth. This is why we designed a loss
function that would respond to small changes in confidence level. This
requirement means that sometimes it does not really reflect
exactly what we are trying to achieve, but is rather a compromise
between our real goal and a function that can be optimized using its
gradient. The loss function is calculated for each item in our dataset,
and then at the end of an epoch, the loss values are all averaged and the overall
mean is reported for the epoch.</p>

<p>Metrics, on the other hand, are the numbers that we care about.
These are the values that are printed at the end of each epoch that
tell us how our model is doing. It is important that we learn to
focus on these metrics, rather than the loss, when judging the
performance of a model.<a data-type="indexterm" data-startref="ch04_mnist" id="idm46287178753864"></a><a data-type="indexterm" data-startref="ch04_mnist3" id="idm46287178753192"></a><a data-type="indexterm" data-startref="ch04_mnist4" id="idm46287178752520"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="SGD and Mini-Batches"><div class="sect2" id="idm46287178915832">
<h2>SGD and Mini-Batches</h2>

<p>Now that we have a loss function suitable for driving SGD, we can<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="optimization step" id="ch04_opt"></a><a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="mini-batches" id="idm46287178748712"></a><a data-type="indexterm" data-primary="batch operations" data-secondary="SGD and mini-batches" id="idm46287178747800"></a><a data-type="indexterm" data-primary="batch operations" data-secondary="batch size" id="idm46287178746856"></a><a data-type="indexterm" data-primary="optimization" data-secondary="numerical digit classifier" id="ch04_opt3"></a><a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="optimization of numerical digit classifier" id="ch04_opt4"></a><a data-type="indexterm" data-primary="optimization" data-secondary="stochastic gradient descent" id="ch04_opt5"></a>
consider some of the details involved in the next phase of the learning
process, which is to change or update the weights based
on the gradients. This is called an <em>optimization step</em>.</p>

<p>To take an optimization step, we need to calculate the loss over
one or more data items. How many should we use? We could calculate it
for the whole dataset and take the average, or we could calculate it
for a single data item. But neither of these is ideal. Calculating it
for the whole dataset would take a long time. Calculating it for a
single item would not use much information, so it would result in an imprecise and unstable gradient. You’d be
going to the trouble of updating the weights, but taking into account
only how that would improve the model’s performance on that
single item.</p>

<p>So instead we compromise: we calculate the
average loss for a few data items at a time. This is called a
<em>mini-batch</em>. The number of data items in the mini-batch is called the
<em>batch size</em>. A larger batch size means that you will get a more
accurate and stable estimate of your dataset’s gradients from
the loss function, but it will take longer, and you will process fewer
mini-batches per epoch. Choosing a good batch size is one of the
decisions you need to make as a deep learning practitioner to train your
model quickly and accurately. We will talk about how to make this choice
throughout this book.</p>

<p>Another good reason for using mini-batches rather than calculating the
gradient on individual data items is that, in practice, we nearly always
do our training on an accelerator such as a GPU. These accelerators
perform well only if they have lots of work to do at a time, so it’s helpful
if we can give them lots of data items to work on. Using
mini-batches is one of the best ways to do this. However, if you give
them too much data to work on at once, they run out of memory—making
GPUs happy is also tricky!</p>

<p>As you saw in our discussion of data augmentation in <a data-type="xref" href="ch02.html#chapter_production">Chapter 2</a>, we
get better generalization if we can vary things during training. One
simple and effective thing we can vary is what data
items we put in each mini-batch. Rather than simply enumerating our
dataset in order for every epoch, instead what we normally do is
randomly shuffle it on every epoch, before we create mini-batches.
PyTorch and fastai provide a class that will do the shuffling and mini-batch collation for you, called <code>DataLoader</code>.</p>

<p>A <code>DataLoader</code> can take any Python collection and turn it into an
iterator over many batches, like so:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">coll</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">15</code><code class="p">)</code>
<code class="n">dl</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">coll</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="nb">list</code><code class="p">(</code><code class="n">dl</code><code class="p">)</code></pre>

<pre data-type="programlisting">[tensor([ 3, 12,  8, 10,  2]),
 tensor([ 9,  4,  7, 14,  5]),
 tensor([ 1, 13,  0,  6, 11])]</pre>

<p>For training a model, we don’t just want any Python
collection, but a collection containing independent and dependent
variables (the inputs and targets of the model). A collection
that contains tuples of independent and dependent variables is known in
PyTorch as a <code>Dataset</code>. Here’s an example of an extremely
simple <code>Dataset</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ds</code> <code class="o">=</code> <code class="n">L</code><code class="p">(</code><code class="nb">enumerate</code><code class="p">(</code><code class="n">string</code><code class="o">.</code><code class="n">ascii_lowercase</code><code class="p">))</code>
<code class="n">ds</code></pre>

<pre data-type="programlisting">(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7,
 &gt; 'h'),(8, 'i'),(9, 'j')...]</pre>

<p>When we pass a <code>Dataset</code> to a <code>DataLoader</code> we will get back many batches
that are themselves tuples of tensors representing batches of
independent and dependent 
<span class="keep-together">variables</span>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dl</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">ds</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">6</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="nb">list</code><code class="p">(</code><code class="n">dl</code><code class="p">)</code></pre>

<pre data-type="programlisting">[(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),
 (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),
 (tensor([ 7, 25,  6,  5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),
 (tensor([ 1,  3,  0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),
 (tensor([2, 4]), ('c', 'e'))]</pre>

<p>We are now ready to write our first training loop for a model using SGD!</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Putting It All Together"><div class="sect1" id="idm46287179733256">
<h1>Putting It All Together</h1>

<p>It’s time to implement the process we saw in
<a data-type="xref" href="#gradient_descent">Figure 4-1</a>. In code, our process will be implemented
something like this for each epoch:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">x</code><code class="p">,</code><code class="n">y</code> <code class="ow">in</code> <code class="n">dl</code><code class="p">:</code>
    <code class="n">pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">loss_func</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
    <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
    <code class="n">parameters</code> <code class="o">-=</code> <code class="n">parameters</code><code class="o">.</code><code class="n">grad</code> <code class="o">*</code> <code class="n">lr</code></pre>

<p>First, let’s reinitialize our parameters:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">weights</code> <code class="o">=</code> <code class="n">init_params</code><code class="p">((</code><code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">,</code><code class="mi">1</code><code class="p">))</code>
<code class="n">bias</code> <code class="o">=</code> <code class="n">init_params</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre>

<p>A <code>DataLoader</code> can be created from a <code>Dataset</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dl</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">dset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">256</code><code class="p">)</code>
<code class="n">xb</code><code class="p">,</code><code class="n">yb</code> <code class="o">=</code> <code class="n">first</code><code class="p">(</code><code class="n">dl</code><code class="p">)</code>
<code class="n">xb</code><code class="o">.</code><code class="n">shape</code><code class="p">,</code><code class="n">yb</code><code class="o">.</code><code class="n">shape</code></pre>

<pre data-type="programlisting">(torch.Size([256, 784]), torch.Size([256, 1]))</pre>

<p>We’ll do the same for the validation set:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">valid_dl</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">valid_dset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">256</code><code class="p">)</code></pre>

<p>Let’s create a mini-batch of size 4 for testing:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">batch</code> <code class="o">=</code> <code class="n">train_x</code><code class="p">[:</code><code class="mi">4</code><code class="p">]</code>
<code class="n">batch</code><code class="o">.</code><code class="n">shape</code></pre>

<pre data-type="programlisting">torch.Size([4, 784])</pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">preds</code> <code class="o">=</code> <code class="n">linear1</code><code class="p">(</code><code class="n">batch</code><code class="p">)</code>
<code class="n">preds</code></pre>

<pre data-type="programlisting">tensor([[-11.1002],
        [  5.9263],
        [  9.9627],
        [ -8.1484]], grad_fn=&lt;AddBackward0&gt;)</pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">loss</code> <code class="o">=</code> <code class="n">mnist_loss</code><code class="p">(</code><code class="n">preds</code><code class="p">,</code> <code class="n">train_y</code><code class="p">[:</code><code class="mi">4</code><code class="p">])</code>
<code class="n">loss</code></pre>

<pre data-type="programlisting">tensor(0.5006, grad_fn=&lt;MeanBackward0&gt;)</pre>

<p>Now we can calculate the gradients:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
<code class="n">weights</code><code class="o">.</code><code class="n">grad</code><code class="o">.</code><code class="n">shape</code><code class="p">,</code><code class="n">weights</code><code class="o">.</code><code class="n">grad</code><code class="o">.</code><code class="n">mean</code><code class="p">(),</code><code class="n">bias</code><code class="o">.</code><code class="n">grad</code></pre>

<pre data-type="programlisting">(torch.Size([784, 1]), tensor(-0.0001), tensor([-0.0008]))</pre>

<p>Let’s put that all in a function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">calc_grad</code><code class="p">(</code><code class="n">xb</code><code class="p">,</code> <code class="n">yb</code><code class="p">,</code> <code class="n">model</code><code class="p">):</code>
    <code class="n">preds</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">xb</code><code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">mnist_loss</code><code class="p">(</code><code class="n">preds</code><code class="p">,</code> <code class="n">yb</code><code class="p">)</code>
    <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code></pre>

<p>And test it:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">calc_grad</code><code class="p">(</code><code class="n">batch</code><code class="p">,</code> <code class="n">train_y</code><code class="p">[:</code><code class="mi">4</code><code class="p">],</code> <code class="n">linear1</code><code class="p">)</code>
<code class="n">weights</code><code class="o">.</code><code class="n">grad</code><code class="o">.</code><code class="n">mean</code><code class="p">(),</code><code class="n">bias</code><code class="o">.</code><code class="n">grad</code></pre>

<pre data-type="programlisting">(tensor(-0.0002), tensor([-0.0015]))</pre>

<p>But look what happens if we call it twice:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">calc_grad</code><code class="p">(</code><code class="n">batch</code><code class="p">,</code> <code class="n">train_y</code><code class="p">[:</code><code class="mi">4</code><code class="p">],</code> <code class="n">linear1</code><code class="p">)</code>
<code class="n">weights</code><code class="o">.</code><code class="n">grad</code><code class="o">.</code><code class="n">mean</code><code class="p">(),</code><code class="n">bias</code><code class="o">.</code><code class="n">grad</code></pre>

<pre data-type="programlisting">(tensor(-0.0003), tensor([-0.0023]))</pre>

<p>The gradients have changed! The reason for this is that <code>loss.backward</code> <em>adds</em> the gradients of <code>loss</code> to any gradients that are
currently stored. So, we have to set the current gradients to 0 first:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">weights</code><code class="o">.</code><code class="n">grad</code><code class="o">.</code><code class="n">zero_</code><code class="p">()</code>
<code class="n">bias</code><code class="o">.</code><code class="n">grad</code><code class="o">.</code><code class="n">zero_</code><code class="p">();</code></pre>
<div data-type="note" epub:type="note"><h1>In-Place Operations</h1>
<p>Methods in PyTorch whose names end in an underscore modify<a data-type="indexterm" data-primary="PyTorch" data-secondary="names ending in underscore" id="idm46287178141000"></a> their objects <em>in place</em>. For instance, <code>bias.zero_</code> sets all elements of the tensor <code>bias</code> to 0.</p>
</div>

<p>Our only remaining step is to update the weights and biases based on
the gradient and learning rate. When we do so, we have to tell PyTorch
not to take the gradient of this step too—otherwise, things will get confusing when we try to compute the derivative at the next batch!
If we assign to the <code>data</code> attribute of a tensor, PyTorch will not
take the gradient of that step. Here’s our basic training
loop for an epoch:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">train_epoch</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">lr</code><code class="p">,</code> <code class="n">params</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">xb</code><code class="p">,</code><code class="n">yb</code> <code class="ow">in</code> <code class="n">dl</code><code class="p">:</code>
        <code class="n">calc_grad</code><code class="p">(</code><code class="n">xb</code><code class="p">,</code> <code class="n">yb</code><code class="p">,</code> <code class="n">model</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="n">params</code><code class="p">:</code>
            <code class="n">p</code><code class="o">.</code><code class="n">data</code> <code class="o">-=</code> <code class="n">p</code><code class="o">.</code><code class="n">grad</code><code class="o">*</code><code class="n">lr</code>
            <code class="n">p</code><code class="o">.</code><code class="n">grad</code><code class="o">.</code><code class="n">zero_</code><code class="p">()</code></pre>

<p>We also want to check how we’re doing, by looking at the
accuracy of the validation set. To decide if an output represents a 3 or
a 7, we can just check whether it’s greater than 0. So
our accuracy for each item can be calculated (using broadcasting, so no
loops!) as follows:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">(</code><code class="n">preds</code><code class="o">&gt;</code><code class="mf">0.0</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code> <code class="o">==</code> <code class="n">train_y</code><code class="p">[:</code><code class="mi">4</code><code class="p">]</code></pre>

<pre data-type="programlisting">tensor([[False],
        [ True],
        [ True],
        [False]])</pre>

<p>That gives us this function to calculate our validation accuracy:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">batch_accuracy</code><code class="p">(</code><code class="n">xb</code><code class="p">,</code> <code class="n">yb</code><code class="p">):</code>
    <code class="n">preds</code> <code class="o">=</code> <code class="n">xb</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">()</code>
    <code class="n">correct</code> <code class="o">=</code> <code class="p">(</code><code class="n">preds</code><code class="o">&gt;</code><code class="mf">0.5</code><code class="p">)</code> <code class="o">==</code> <code class="n">yb</code>
    <code class="k">return</code> <code class="n">correct</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code></pre>

<p>We can check it works:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">batch_accuracy</code><code class="p">(</code><code class="n">linear1</code><code class="p">(</code><code class="n">batch</code><code class="p">),</code> <code class="n">train_y</code><code class="p">[:</code><code class="mi">4</code><code class="p">])</code></pre>

<pre data-type="programlisting">tensor(0.5000)</pre>

<p>And then put the batches together:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">validate_epoch</code><code class="p">(</code><code class="n">model</code><code class="p">):</code>
    <code class="n">accs</code> <code class="o">=</code> <code class="p">[</code><code class="n">batch_accuracy</code><code class="p">(</code><code class="n">model</code><code class="p">(</code><code class="n">xb</code><code class="p">),</code> <code class="n">yb</code><code class="p">)</code> <code class="k">for</code> <code class="n">xb</code><code class="p">,</code><code class="n">yb</code> <code class="ow">in</code> <code class="n">valid_dl</code><code class="p">]</code>
    <code class="k">return</code> <code class="nb">round</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">stack</code><code class="p">(</code><code class="n">accs</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code> <code class="mi">4</code><code class="p">)</code></pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">validate_epoch</code><code class="p">(</code><code class="n">linear1</code><code class="p">)</code></pre>

<pre data-type="programlisting">0.5219</pre>

<p>That’s our starting point. Let’s train for one
epoch and see if the accuracy improves:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">lr</code> <code class="o">=</code> <code class="mf">1.</code>
<code class="n">params</code> <code class="o">=</code> <code class="n">weights</code><code class="p">,</code><code class="n">bias</code>
<code class="n">train_epoch</code><code class="p">(</code><code class="n">linear1</code><code class="p">,</code> <code class="n">lr</code><code class="p">,</code> <code class="n">params</code><code class="p">)</code>
<code class="n">validate_epoch</code><code class="p">(</code><code class="n">linear1</code><code class="p">)</code></pre>

<pre data-type="programlisting">0.6883</pre>

<p>Then do a few more:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">20</code><code class="p">):</code>
    <code class="n">train_epoch</code><code class="p">(</code><code class="n">linear1</code><code class="p">,</code> <code class="n">lr</code><code class="p">,</code> <code class="n">params</code><code class="p">)</code>
    <code class="k">print</code><code class="p">(</code><code class="n">validate_epoch</code><code class="p">(</code><code class="n">linear1</code><code class="p">),</code> <code class="n">end</code><code class="o">=</code><code class="s1">' '</code><code class="p">)</code></pre>

<pre data-type="programlisting">0.8314 0.9017 0.9227 0.9349 0.9438 0.9501 0.9535 0.9564 0.9594 0.9618 0.9613
 &gt; 0.9638 0.9643 0.9652 0.9662 0.9677 0.9687 0.9691 0.9691 0.9696</pre>

<p>Looking good! We’re already about at the same accuracy as
our “pixel similarity” approach, and we’ve created a
general-purpose foundation we can build on. Our next step will be to
create an object that will handle the SGD step for us. In PyTorch,
it’s called an <em>optimizer</em>.</p>








<section data-type="sect2" data-pdf-bookmark="Creating an Optimizer"><div class="sect2" id="idm46287177745880">
<h2>Creating an Optimizer</h2>

<p>Because this is such a general foundation, PyTorch provides some useful
classes to make it easier to implement. The first thing we can do is replace our <code>linear</code> function with PyTorch’s
<code>nn.Linear</code> module. A <em>module</em> is an object of a class that inherits<a data-type="indexterm" data-primary="modules" id="idm46287177722008"></a><a data-type="indexterm" data-primary="activations" data-secondary="models returning" id="idm46287177721304"></a><a data-type="indexterm" data-primary="optimization" data-secondary="creating an optimizer" id="ch04_optcreat2"></a><a data-type="indexterm" data-primary="PyTorch" data-secondary="optimizer creation" id="ch04_optcreat"></a>
from the PyTorch <code>nn.Module</code> class. Objects of this class behave
identically to standard Python functions, in that you can call them using
parentheses, and they will return the activations of a model.</p>

<p><code>nn.Linear</code> does the same thing as our <code>init_params</code> and <code>linear</code>
together. It contains both the <em>weights</em> and <em>biases</em> in a single class.
Here’s how we replicate our model from the previous section:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">linear_model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code></pre>

<p>Every PyTorch module knows what parameters it has that can be trained;
they are available through the <code>parameters</code> method:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">w</code><code class="p">,</code><code class="n">b</code> <code class="o">=</code> <code class="n">linear_model</code><code class="o">.</code><code class="n">parameters</code><code class="p">()</code>
<code class="n">w</code><code class="o">.</code><code class="n">shape</code><code class="p">,</code><code class="n">b</code><code class="o">.</code><code class="n">shape</code></pre>

<pre data-type="programlisting">(torch.Size([1, 784]), torch.Size([1]))</pre>

<p>We can use this information to create an optimizer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">BasicOptim</code><code class="p">:</code>
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code class="n">params</code><code class="p">,</code><code class="n">lr</code><code class="p">):</code> <code class="bp">self</code><code class="o">.</code><code class="n">params</code><code class="p">,</code><code class="bp">self</code><code class="o">.</code><code class="n">lr</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">params</code><code class="p">),</code><code class="n">lr</code>

    <code class="k">def</code> <code class="nf">step</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="o">*</code><code class="n">args</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">params</code><code class="p">:</code> <code class="n">p</code><code class="o">.</code><code class="n">data</code> <code class="o">-=</code> <code class="n">p</code><code class="o">.</code><code class="n">grad</code><code class="o">.</code><code class="n">data</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">lr</code>

    <code class="k">def</code> <code class="nf">zero_grad</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="o">*</code><code class="n">args</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">params</code><code class="p">:</code> <code class="n">p</code><code class="o">.</code><code class="n">grad</code> <code class="o">=</code> <code class="bp">None</code></pre>

<p>We can create our optimizer by passing in the model’s
parameters:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">opt</code> <code class="o">=</code> <code class="n">BasicOptim</code><code class="p">(</code><code class="n">linear_model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="p">)</code></pre>

<p>Our training loop can now be simplified:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">train_epoch</code><code class="p">(</code><code class="n">model</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">xb</code><code class="p">,</code><code class="n">yb</code> <code class="ow">in</code> <code class="n">dl</code><code class="p">:</code>
        <code class="n">calc_grad</code><code class="p">(</code><code class="n">xb</code><code class="p">,</code> <code class="n">yb</code><code class="p">,</code> <code class="n">model</code><code class="p">)</code>
        <code class="n">opt</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
        <code class="n">opt</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code></pre>

<p>Our validation function doesn’t need to change at all:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">validate_epoch</code><code class="p">(</code><code class="n">linear_model</code><code class="p">)</code></pre>

<pre data-type="programlisting">0.4157</pre>

<p>Let’s put our little training loop in a function, to make
things simpler:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">train_model</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
        <code class="n">train_epoch</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>
        <code class="k">print</code><code class="p">(</code><code class="n">validate_epoch</code><code class="p">(</code><code class="n">model</code><code class="p">),</code> <code class="n">end</code><code class="o">=</code><code class="s1">' '</code><code class="p">)</code></pre>

<p>The results are the same as in the previous section:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">train_model</code><code class="p">(</code><code class="n">linear_model</code><code class="p">,</code> <code class="mi">20</code><code class="p">)</code></pre>

<pre data-type="programlisting">0.4932 0.8618 0.8203 0.9102 0.9331 0.9468 0.9555 0.9629 0.9658 0.9673 0.9687
 &gt; 0.9707 0.9726 0.9751 0.9761 0.9761 0.9775 0.978 0.9785 0.9785</pre>

<p>fastai provides the <code>SGD</code> class that, by default, does the same thing<a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="SGD class" id="idm46287177350328"></a><a data-type="indexterm" data-primary="SGD class" data-seealso="stochastic gradient descent" id="idm46287177349416"></a>
as our 
<span class="keep-together"><code>BasicOptim</code></span>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">linear_model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code>
<code class="n">opt</code> <code class="o">=</code> <code class="n">SGD</code><code class="p">(</code><code class="n">linear_model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="p">)</code>
<code class="n">train_model</code><code class="p">(</code><code class="n">linear_model</code><code class="p">,</code> <code class="mi">20</code><code class="p">)</code></pre>

<pre data-type="programlisting">0.4932 0.852 0.8335 0.9116 0.9326 0.9473 0.9555 0.9624 0.9648 0.9668 0.9692
 &gt; 0.9712 0.9731 0.9746 0.9761 0.9765 0.9775 0.978 0.9785 0.9785</pre>

<p>fastai also provides <code>Learner.fit</code>, which we can use instead of
<code>train_model</code>. To create a <code>Learner</code>, we first need to create a
<code>DataLoaders</code>, by passing in our training and validation <code>DataLoader</code>s:<a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="Learner creation" id="idm46287177263544"></a><a data-type="indexterm" data-primary="Learner" data-secondary="numerical digit classifier" id="idm46287177262568"></a></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dls</code> <code class="o">=</code> <code class="n">DataLoaders</code><code class="p">(</code><code class="n">dl</code><code class="p">,</code> <code class="n">valid_dl</code><code class="p">)</code></pre>

<p>To create a <code>Learner</code> without using an application (such as<a data-type="indexterm" data-primary="Learner" data-secondary="about" id="idm46287177255288"></a>
<code>cnn_learner</code>), we need to pass in all the elements that
we’ve created in this chapter: the <code>DataLoaders</code>, the model,
the optimization function (which will be passed the parameters), the
loss function, and optionally any metrics to print:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">learn</code> <code class="o">=</code> <code class="n">Learner</code><code class="p">(</code><code class="n">dls</code><code class="p">,</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">,</code><code class="mi">1</code><code class="p">),</code> <code class="n">opt_func</code><code class="o">=</code><code class="n">SGD</code><code class="p">,</code>
                <code class="n">loss_func</code><code class="o">=</code><code class="n">mnist_loss</code><code class="p">,</code> <code class="n">metrics</code><code class="o">=</code><code class="n">batch_accuracy</code><code class="p">)</code></pre>

<p>Now we can call <code>fit</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">learn</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">lr</code><code class="o">=</code><code class="n">lr</code><code class="p">)</code></pre>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>batch_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.636857</td>
      <td>0.503549</td>
      <td>0.495584</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.545725</td>
      <td>0.170281</td>
      <td>0.866045</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.199223</td>
      <td>0.184893</td>
      <td>0.831207</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.086580</td>
      <td>0.107836</td>
      <td>0.911187</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.045185</td>
      <td>0.078481</td>
      <td>0.932777</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.029108</td>
      <td>0.062792</td>
      <td>0.946516</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.022560</td>
      <td>0.053017</td>
      <td>0.955348</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.019687</td>
      <td>0.046500</td>
      <td>0.962218</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.018252</td>
      <td>0.041929</td>
      <td>0.965162</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.017402</td>
      <td>0.038573</td>
      <td>0.967615</td>
      <td>00:00</td>
    </tr>
  </tbody>
</table>

<p>As you can see, there’s nothing magic about the PyTorch and
fastai classes. They are just convenient prepackaged pieces that make
your life a bit easier! (They also provide a lot of extra functionality
we’ll be using in future chapters.)</p>

<p>With these classes, we can now replace our linear model with a neural
network.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Adding a Nonlinearity"><div class="sect1" id="idm46287177724408">
<h1>Adding a Nonlinearity</h1>

<p>So far, we have a general procedure for optimizing the parameters of a
function, and we have tried it out on a boring function: a simple
linear classifier. A linear classifier is constrained in terms of
what it can do. To make it a bit more complex (and able to handle more<a data-type="indexterm" data-primary="optimization" data-secondary="nonlinearity added" id="idm46287177095048"></a><a data-type="indexterm" data-primary="neural networks" data-secondary="explained" id="idm46287177094296"></a><a data-type="indexterm" data-primary="linear and nonlinear layers" id="idm46287177093352"></a><a data-type="indexterm" data-primary="nonlinear and linear layers" id="idm46287177092712"></a>
tasks), we need to add something nonlinear (i.e., different from ax+b) between two linear classifiers—this is what gives us a neural network.</p>

<p>Here is the entire definition of a basic neural network:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">simple_net</code><code class="p">(</code><code class="n">xb</code><code class="p">):</code>
    <code class="n">res</code> <code class="o">=</code> <code class="n">xb</code><code class="nd">@w1</code> <code class="o">+</code> <code class="n">b1</code>
    <code class="n">res</code> <code class="o">=</code> <code class="n">res</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">tensor</code><code class="p">(</code><code class="mf">0.0</code><code class="p">))</code>
    <code class="n">res</code> <code class="o">=</code> <code class="n">res</code><code class="nd">@w2</code> <code class="o">+</code> <code class="n">b2</code>
    <code class="k">return</code> <code class="n">res</code></pre>

<p>That’s it! All we have in <code>simple_net</code> is two linear
classifiers with a <code>max</code> function between them.</p>

<p>Here, <code>w1</code> and <code>w2</code> are weight tensors, and <code>b1</code> and <code>b2</code> are bias
tensors; that is, parameters that are initially randomly initialized,
just as we did in the previous section:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">w1</code> <code class="o">=</code> <code class="n">init_params</code><code class="p">((</code><code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">,</code><code class="mi">30</code><code class="p">))</code>
<code class="n">b1</code> <code class="o">=</code> <code class="n">init_params</code><code class="p">(</code><code class="mi">30</code><code class="p">)</code>
<code class="n">w2</code> <code class="o">=</code> <code class="n">init_params</code><code class="p">((</code><code class="mi">30</code><code class="p">,</code><code class="mi">1</code><code class="p">))</code>
<code class="n">b2</code> <code class="o">=</code> <code class="n">init_params</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre>

<p>The key point is that <code>w1</code> has 30 output activations (which
means that <code>w2</code> must have 30 input activations, so they match). That
means that the first layer can construct 30 different features, each
representing a different mix of pixels. You can change that <code>30</code> to
anything you like, to make the model more or less complex.</p>

<p>That little function <code>res.max(tensor(0.0))</code> is called a <em>rectified
linear unit</em>, also known as <em>ReLU</em>. We think we can all agree that<a data-type="indexterm" data-primary="rectified linear unit (ReLU)" id="idm46287176209560"></a>
<em>rectified linear unit</em> sounds pretty fancy and complicated…But
actually, there’s nothing more to it than
<code>res.max(tensor(0.0))</code>—in other words, replace every negative number
with a zero. This tiny function is also available in PyTorch as
<code>F.relu</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plot_function</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">relu</code><code class="p">)</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in16.png" alt="" width="382" height="251">
<h6></h6>
</div></figure>
<div data-type="tip"><h1>Jeremy Says</h1>
<p>There is an enormous amount of jargon in deep learning, including<a data-type="indexterm" data-primary="jargon" data-seealso="terminology" id="idm46287176200744"></a> terms like <em>rectified linear unit</em>. The vast majority of this jargon is no more complicated than can be implemented in a short line of code, as we saw in this example. The reality is that for academics to get their papers published, they need to make them sound as impressive and sophisticated as possible. One way that they do that is to introduce jargon. Unfortunately, this results in the field becoming far more intimidating and difficult to get into than it should be. You do have to learn the jargon, because otherwise papers and tutorials are not going to mean much to you. But that doesn’t mean you have to find the jargon intimidating. Just remember, when you come across a word or phrase that you haven’t seen before, it will almost certainly turn out to be referring to a very simple concept.</p>
</div>

<p>The basic idea is that by using more linear layers, we can have our<a data-type="indexterm" data-primary="layers" data-secondary="more linear layers, more computations" id="idm46287176197896"></a>
model do more computation, and therefore model more complex functions.
But there’s no point in just putting one linear layout directly
after another one, because when we multiply things together and then add
them up multiple times, that could be replaced by multiplying different
things together and adding them up just once! That is to say, a series
of any number of linear layers in a row can be replaced with a single
linear layer with a different set of parameters.</p>

<p>But if we put a nonlinear function between them, such as <code>max</code>, this<a data-type="indexterm" data-primary="layers" data-secondary="nonlinear function between linears" id="idm46287176192440"></a>
is no longer true. Now each linear layer is somewhat decoupled
from the other ones and can do its own useful work. The <code>max</code> function is
particularly interesting, because it operates as a simple <code>if</code>
statement.</p>
<div data-type="tip"><h1>Sylvain Says</h1>
<p>Mathematically, we say the composition of two linear functions is another linear function. So, we can stack as many linear classifiers as we want on top of each other, and without nonlinear functions between them, it will just be the same as one linear classifier.</p>
</div>

<p>Amazingly enough, it can be mathematically proven that this little
function can solve any computable problem to an arbitrarily high level
of accuracy, if you can find the right parameters for <code>w1</code> and <code>w2</code> and
if you make these matrices big enough. For any arbitrarily wiggly function, we can approximate it as
a bunch of lines joined together; to make it closer to the wiggly<a data-type="indexterm" data-primary="universal approximation theorem" id="idm46287176186744"></a>
function, we just have to use shorter lines. This is known as the <em>universal
approximation theorem</em>. The three lines of code that we have here are
known as <em>layers</em>. The first and third are known as <em>linear layers</em>, and
the second line of code is known variously as a <em>nonlinearity</em>, or
<em>activation function</em>.<a data-type="indexterm" data-primary="activation function" data-secondary="nonlinear layer" id="idm46287176151704"></a><a data-type="indexterm" data-primary="nonlinear and linear layers" id="idm46287176150696"></a><a data-type="indexterm" data-primary="linear and nonlinear layers" id="idm46287176149960"></a></p>

<p>Just as in the previous section, we can replace this code with something
a bit simpler by taking advantage of PyTorch:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">simple_net</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">,</code><code class="mi">30</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code>
<code class="p">)</code></pre>

<p><code>nn.Sequential</code> creates a module that will call each of the listed
layers or functions in turn.<a data-type="indexterm" data-primary="Sequential class" id="idm46287176108280"></a></p>

<p><code>nn.ReLU</code> is a PyTorch module that does exactly the same thing as the <code>F.relu</code> function. Most functions that can appear
in a model also have identical forms that are modules. Generally,
it’s just a case of replacing <code>F</code> with <code>nn</code> and changing
the capitalization. When using <code>nn.Sequential</code>, PyTorch requires us to
use the module version. Since modules are classes, we have to
instantiate them, which is why you see <code>nn.ReLU</code> in this 
<span class="keep-together">example</span>.</p>

<p>Because <code>nn.Sequential</code> is a module, we can get its parameters, which will return
a list of all the parameters of all the modules it contains. Let’s try it out! As this is a deeper model, we’ll use a
lower learning rate and a few more epochs:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">learn</code> <code class="o">=</code> <code class="n">Learner</code><code class="p">(</code><code class="n">dls</code><code class="p">,</code> <code class="n">simple_net</code><code class="p">,</code> <code class="n">opt_func</code><code class="o">=</code><code class="n">SGD</code><code class="p">,</code>
                <code class="n">loss_func</code><code class="o">=</code><code class="n">mnist_loss</code><code class="p">,</code> <code class="n">metrics</code><code class="o">=</code><code class="n">batch_accuracy</code><code class="p">)</code></pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">learn</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="mi">40</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">)</code></pre>

<p>We’re not showing the 40 lines of output here to save room;
the training process is recorded in <code>learn.recorder</code>, with the table of
output stored in the <code>values</code> attribute, so we can plot the accuracy
over training:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">L</code><code class="p">(</code><code class="n">learn</code><code class="o">.</code><code class="n">recorder</code><code class="o">.</code><code class="n">values</code><code class="p">)</code><code class="o">.</code><code class="n">itemgot</code><code class="p">(</code><code class="mi">2</code><code class="p">));</code></pre>

<figure><div class="figure">
<img src="/library/view/deep-learning-for/9781492045519/assets/dlcf_04in17.png" alt="" width="375" height="254">
<h6></h6>
</div></figure>

<p>And we can view the final accuracy:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">learn</code><code class="o">.</code><code class="n">recorder</code><code class="o">.</code><code class="n">values</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">][</code><code class="mi">2</code><code class="p">]</code></pre>

<pre data-type="programlisting">0.982826292514801</pre>

<p>At this point, we have something that is rather magical:</p>

<ul>
<li>
<p>A function that can solve any problem to any level of accuracy (the
neural network) given the correct set of parameters</p>
</li>
<li>
<p>A way to
find the best set of parameters for any function (stochastic gradient
descent)</p>
</li>
</ul>

<p>This is why deep learning can do such fantastic things.
Believing that this combination of simple techniques can really solve
any problem is one of the biggest steps that we find many students
have to take. It seems too good to be true—surely things should
be more difficult and complicated than this? Our recommendation: try it
out! We just tried it on the MNIST dataset, and you’ve seen the results. And since we are doing everything from scratch ourselves (except
for calculating the gradients), you know that there is no special magic
hiding behind the scenes.</p>








<section data-type="sect2" data-pdf-bookmark="Going Deeper"><div class="sect2" id="idm46287175974856">
<h2>Going Deeper</h2>

<p>There is no need to stop at just two linear layers. We can add as many<a data-type="indexterm" data-primary="layers" data-secondary="more linear layers, more computations" id="idm46287175973288"></a><a data-type="indexterm" data-primary="layers" data-secondary="optimization and" id="idm46287175972344"></a><a data-type="indexterm" data-primary="optimization" data-secondary="layers and" id="idm46287175971400"></a>
as we want, as long as we add a nonlinearity between each pair of linear<a data-type="indexterm" data-primary="layers" data-secondary="nonlinear function between linears" id="idm46287175970328"></a>
layers. As you will learn, however, the deeper the model gets, the harder
it is to optimize the parameters in practice. Later in this book, you will
learn about some simple but brilliantly effective techniques for
training deeper models.</p>

<p>We already know that a single nonlinearity with two linear layers is
enough to approximate any function. So why would we use deeper models?<a data-type="indexterm" data-primary="deeper models having more layers" id="idm46287175934024"></a><a data-type="indexterm" data-primary="layers" data-secondary="deeper models having more layers" id="idm46287175933352"></a><a data-type="indexterm" data-primary="parameters" data-secondary="deeper models and" id="idm46287175932440"></a>
The reason is performance. With a deeper model (one with more
layers), we do not need to use as many parameters; it turns out that we
can use smaller matrices, with more layers, and get better results than
we would get with larger matrices and few layers.</p>

<p>That means that we can train the model more quickly, and it will take<a data-type="indexterm" data-primary="training" data-secondary="deeper models" id="idm46287175930712"></a><a data-type="indexterm" data-primary="accuracy metric" data-secondary="deeper models" id="idm46287175929736"></a>
up less memory. In the 1990s, researchers were so focused on the
universal approximation theorem that few were experimenting with
more than one nonlinearity. This theoretical but not practical
foundation held back the field for years. Some researchers, however, did
experiment with deep models, and eventually were able to show that these
models could perform much better in practice. Eventually, theoretical
results were developed that showed why this happens. Today, it is
extremely unusual to find anybody using a neural network with just one
nonlinearity.</p>

<p>Here is what happens when we train an 18-layer model using the same approach
we saw in <a data-type="xref" href="ch01.html#chapter_intro">Chapter 1</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dls</code> <code class="o">=</code> <code class="n">ImageDataLoaders</code><code class="o">.</code><code class="n">from_folder</code><code class="p">(</code><code class="n">path</code><code class="p">)</code>
<code class="n">learn</code> <code class="o">=</code> <code class="n">cnn_learner</code><code class="p">(</code><code class="n">dls</code><code class="p">,</code> <code class="n">resnet18</code><code class="p">,</code> <code class="n">pretrained</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code>
                    <code class="n">loss_func</code><code class="o">=</code><code class="n">F</code><code class="o">.</code><code class="n">cross_entropy</code><code class="p">,</code> <code class="n">metrics</code><code class="o">=</code><code class="n">accuracy</code><code class="p">)</code>
<code class="n">learn</code><code class="o">.</code><code class="n">fit_one_cycle</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">)</code></pre>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.082089</td>
      <td>0.009578</td>
      <td>0.997056</td>
      <td>00:11</td>
    </tr>
  </tbody>
</table>

<p>Nearly 100% accuracy! That’s a big difference compared to<a data-type="indexterm" data-startref="ch04_opt" id="idm46287175864024"></a><a data-type="indexterm" data-startref="ch04_opt3" id="idm46287175863432"></a><a data-type="indexterm" data-startref="ch04_opt4" id="idm46287175862760"></a><a data-type="indexterm" data-startref="ch04_opt5" id="idm46287175862088"></a>
our simple neural net. But as you’ll learn in the remainder
of this book, there are just a few little tricks you need to use to get
such great results from scratch yourself. You already know the key
foundational pieces. (Of course, even when you know all the tricks,
you’ll nearly always want to work with the prebuilt classes
provided by PyTorch and fastai, because they save you from having to think
about all the little details yourself.)<a data-type="indexterm" data-startref="ch04_optcreat" id="idm46287175860824"></a><a data-type="indexterm" data-startref="ch04_optcreat2" id="idm46287175860152"></a></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Jargon Recap"><div class="sect1" id="idm46287177095576">
<h1>Jargon Recap</h1>

<p>Congratulations: you now know how to create and train a deep neural<a data-type="indexterm" data-primary="terminology for deep learning" id="idm46287175858120"></a><a data-type="indexterm" data-primary="deep learning" data-secondary="terminology" id="idm46287175857448"></a><a data-type="indexterm" data-primary="numerical digit classifier" data-secondary="terminology" id="idm46287175856504"></a>
network from scratch! We’ve gone through quite a few steps to get to this
point, but you might be surprised at how simple it really is.</p>

<p>Now that we are at this point, it is a good opportunity to define, and
review, some jargon and key concepts.</p>

<p>A neural network contains a lot of numbers, but they are only of two types: numbers that are calculated, and the parameters
that these numbers are calculated from. This gives us the two most important
pieces of jargon to learn:</p>
<dl>
<dt>Activations</dt>
<dd>
<p>Numbers that are calculated (both by linear and nonlinear layers)<a data-type="indexterm" data-primary="activations" data-secondary="definition" id="idm46287175852632"></a></p>
</dd>
<dt>Parameters</dt>
<dd>
<p>Numbers that are randomly initialized, and optimized (that is, the numbers that define the model)<a data-type="indexterm" data-primary="parameters" data-secondary="definition" id="idm46287175850264"></a></p>
</dd>
</dl>

<p>We will often talk in this book about activations and parameters.
Remember that they have specific meanings. They are numbers. They
are not abstract concepts, but they are actual specific numbers that are
in your model. Part of becoming a good deep learning practitioner is
getting used to the idea of looking at your activations and
parameters, and plotting them and testing whether they are behaving
correctly.</p>

<p>Our activations and parameters are all contained in <em>tensors</em>. These are<a data-type="indexterm" data-primary="tensors" data-secondary="definition" id="idm46287175847160"></a>
simply regularly shaped arrays—for example, a matrix. Matrices have
rows and columns; we call these the <em>axes</em> or <em>dimensions</em>. The number<a data-type="indexterm" data-primary="axis of tensor or matrix" id="idm46287175845064"></a><a data-type="indexterm" data-primary="rank of tensor" data-secondary="definition" id="idm46287175844360"></a><a data-type="indexterm" data-primary="rank of tensor" data-secondary="scalar versus vector versus matrix" id="idm46287175843416"></a>
of dimensions of a tensor is its <em>rank</em>. There are some special tensors:</p>

<ul>
<li>
<p>Rank-0: scalar</p>
</li>
<li>
<p>Rank-1: vector</p>
</li>
<li>
<p>Rank-2: matrix</p>
</li>
</ul>

<p>A neural network contains a number of layers. Each layer is either<a data-type="indexterm" data-primary="neural networks" data-secondary="layers" data-see="layers" id="idm46287175838344"></a><a data-type="indexterm" data-primary="linear and nonlinear layers" id="idm46287175837096"></a><a data-type="indexterm" data-primary="nonlinear and linear layers" id="idm46287175836456"></a>
<em>linear</em> or <em>nonlinear</em>. We generally alternate between these two kinds of
layers in a neural network. Sometimes people refer to both a linear
layer and its subsequent nonlinearity together as a single layer. Yes,
this is confusing. Sometimes a nonlinearity is referred to as an
<em>activation function</em>.<a data-type="indexterm" data-primary="activation function" data-secondary="nonlinear layer" id="idm46287175834136"></a></p>

<p class="pagebreak-before"><a data-type="xref" href="#dljargon1">Table 4-1</a> summarizes the key concepts related to SGD.</p>
<table id="dljargon1">
<caption><span class="label">Table 4-1. </span>Deep learning vocabulary</caption>
<thead>
<tr>
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>ReLU</p></td>
<td><p>Function that returns 0 for negative numbers and doesn’t change positive numbers.<a data-type="indexterm" data-primary="rectified linear unit (ReLU)" id="idm46287175798984"></a></p></td>
</tr>
<tr>
<td><p>Mini-batch</p></td>
<td><p>A small group of inputs and labels gathered together in two arrays. A gradient descent step is updated on this batch (rather than a whole epoch).<a data-type="indexterm" data-primary="batch operations" data-secondary="mini-batch" id="idm46287175796568"></a><a data-type="indexterm" data-primary="mini-batch" id="idm46287175795592"></a></p></td>
</tr>
<tr>
<td><p>Forward pass</p></td>
<td><p>Applying the model to some input and computing the predictions.<a data-type="indexterm" data-primary="forward pass" id="idm46287175793384"></a></p></td>
</tr>
<tr>
<td><p>Loss</p></td>
<td><p>A value that represents how well (or badly) our model is doing.<a data-type="indexterm" data-primary="loss" data-secondary="definition" id="idm46287175791064"></a></p></td>
</tr>
<tr>
<td><p>Gradient</p></td>
<td><p>The derivative of the loss with respect to some parameter of the model.<a data-type="indexterm" data-primary="gradients" data-secondary="definition" id="idm46287175788472"></a></p></td>
</tr>
<tr>
<td><p>Backward pass</p></td>
<td><p>Computing the gradients of the loss with respect to all model parameters.<a data-type="indexterm" data-primary="backward pass" id="idm46287175785880"></a></p></td>
</tr>
<tr>
<td><p>Gradient descent</p></td>
<td><p>Taking a step in the direction opposite to the gradients to make the model parameters a little bit better.<a data-type="indexterm" data-primary="gradient descent" id="idm46287175783512"></a><a data-type="indexterm" data-primary="optimization" data-secondary="gradient descent" id="idm46287175782808"></a></p></td>
</tr>
<tr>
<td><p>Learning rate</p></td>
<td><p>The size of the step we take when applying SGD to update the parameters of the model.<a data-type="indexterm" data-primary="learning rate (LR)" data-secondary="definition" id="idm46287175780328"></a><a data-type="indexterm" data-primary="training" data-secondary="learning rate" data-tertiary="definition" id="idm46287175779352"></a></p></td>
</tr>
</tbody>
</table>
<div data-type="note" epub:type="note"><h1><em>Choose Your Own Adventure</em> Reminder</h1>
<p>Did you choose to skip over Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch02.html#chapter_production">2</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.html#chapter_ethics">3</a>, in your excitement to peek
under the hood? Well, here’s your reminder to head back to
<a data-type="xref" href="ch02.html#chapter_production">Chapter 2</a> now, because you’ll be needing to know that stuff soon!</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Questionnaire"><div class="sect1" id="idm46287175859224">
<h1>Questionnaire</h1>
<ol>
<li>
<p>How is a grayscale image represented on a computer? How about a color image?</p>
</li>
<li>
<p>How are the files and folders in the <code>MNIST_SAMPLE</code> dataset structured? Why?</p>
</li>
<li>
<p>Explain how the “pixel similarity” approach to classifying digits works.</p>
</li>
<li>
<p>What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.</p>
</li>
<li>
<p>What is a rank-3 tensor?</p>
</li>
<li>
<p>What is the difference between tensor rank and shape? How do you get the rank from the shape?</p>
</li>
<li>
<p>What are RMSE and L1 norm?</p>
</li>
<li>
<p>How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?</p>
</li>
<li>
<p>Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.</p>
</li>
<li>
<p>What is broadcasting?</p>
</li>
<li>
<p>Are metrics generally calculated using the training set or the validation set? Why?</p>
</li>
<li>
<p>What is SGD?</p>
</li>
<li>
<p>Why does SGD use mini-batches?</p>
</li>
<li>
<p>What are the seven steps in SGD for machine learning?</p>
</li>
<li>
<p>How do we initialize the weights in a model?</p>
</li>
<li>
<p>What is loss?</p>
</li>
<li>
<p>Why can’t we always use a high learning rate?</p>
</li>
<li>
<p>What is a gradient?</p>
</li>
<li>
<p>Do you need to know how to calculate gradients yourself?</p>
</li>
<li>
<p>Why can’t we use accuracy as a loss function?</p>
</li>
<li>
<p>Draw the sigmoid function. What is special about its shape?</p>
</li>
<li>
<p>What is the difference between a loss function and a metric?</p>
</li>
<li>
<p>What is the function to calculate new weights using a learning rate?</p>
</li>
<li>
<p>What does the <code>DataLoader</code> class do?</p>
</li>
<li>
<p>Write pseudocode showing the basic steps taken in each epoch for SGD.</p>
</li>
<li>
<p>Create a function that, if passed two arguments <code>[1,2,3,4]</code> and <code>'abcd'</code>, returns <code>[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]</code>. What is special about that output data structure?</p>
</li>
<li>
<p>What does <code>view</code> do in PyTorch?</p>
</li>
<li>
<p>What are the bias parameters in a neural network? Why do we need them?</p>
</li>
<li>
<p>What does the <code>@</code> operator do in Python?</p>
</li>
<li>
<p>What does the <code>backward</code> method do?</p>
</li>
<li>
<p>Why do we have to zero the gradients?</p>
</li>
<li>
<p>What information do we have to pass to <code>Learner</code>?</p>
</li>
<li>
<p>Show Python or pseudocode for the basic steps of a training loop.</p>
</li>
<li>
<p>What is ReLU? Draw a plot of it for values from <code>-2</code> to <code>+2</code>.</p>
</li>
<li>
<p>What is an activation function?</p>
</li>
<li>
<p>What’s the difference between <code>F.relu</code> and <code>nn.ReLU</code>?</p>
</li>
<li>
<p>The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?</p>
</li>

</ol>








<section data-type="sect2" data-pdf-bookmark="Further Research"><div class="sect2" id="idm46287175770872">
<h2>Further Research</h2>
<ol>
<li>
<p>Create your own implementation of <code>Learner</code> from scratch, based on the training loop shown in this chapter.</p>
</li>
<li>
<p>Complete all the steps in this chapter using the full MNIST datasets (for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You’ll need to do some of your own research to figure out how to overcome obstacles you’ll meet on the way.</p>
</li>

</ol>
</div></section>





</div></section>







</div></section></div>
    </div>
    
    <section class="t-bottom-cta bottom-cta bottom-cta-book free-chapter">
  <p>Get <em>Deep Learning for Coders with fastai and PyTorch</em> now with O’Reilly <span class="nowrap">online learning.</span></p>
  <p>O’Reilly members experience live online training, plus books, videos, and digital content from <span class="nowrap">200+ publishers.</span></p>

  <div class="controls">
      <a href="https://learning.oreilly.com/p/register/" class="button-primary" data-ga-label="Bottom CTA">Start your free trial</a>
  </div>
</section>

</div>


</section>



  </div>
</main>


  <footer id="footer" class="footer">
  <div class="content">
    <div class="footer-main" aria-label="company info">
      <div class="footer-mainLeft">
        <div class="footer-mainLeftOne">
          <div class="footer-approach">
            <h2 class="footer-header"><a href="https://www.oreilly.com/about/">About O’Reilly</a></h2>
            <ul class="footer-links">
              <li><a href="https://www.oreilly.com/work-with-us.html">Teach/write/train</a></li>
              <li><a href="https://www.oreilly.com/careers/">Careers</a></li>
              <li><a href="https://www.oreilly.com/partner/signup.csp">Community partners</a></li>
              <li><a href="https://www.oreilly.com/affiliates/">Affiliate program</a></li>
              <li><a href="https://www.oreilly.com/diversity/">Diversity</a></li>
            </ul>
          </div>
        </div>
        <div class="footer-mainLeftTwo">
          <div class="footer-contact">
            <h2 class="footer-header"><a href="https://www.oreilly.com/online-learning/support/">Support</a></h2>

            <ul class="footer-links">
              <li><a href="https://www.oreilly.com/about/contact.html">Contact us</a></li>
              <li><a href="https://www.oreilly.com/emails/newsletters/">Newsletters</a></li>
              <li><a href="https://www.oreilly.com/privacy.html">Privacy policy</a></li>
            </ul>
          
            <a href="https://twitter.com/oreillymedia" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 400 400" style="enable-background:new 0 0 400 400;" xml:space="preserve"><style type="text/css">.st0{fill:#ee0000;}.st1{fill:#FFFFFF;}</style><g id="Red"><circle class="st0" cx="200" cy="200" r="200"/></g><g id="Logo__x2014__FIXED"><path class="st1" d="M163.4,305.5c88.7,0,137.2-73.5,137.2-137.2c0-2.1,0-4.2-0.1-6.2c9.4-6.8,17.6-15.3,24.1-25 c-8.6,3.8-17.9,6.4-27.7,7.6c10-6,17.6-15.4,21.2-26.7c-9.3,5.5-19.6,9.5-30.6,11.7c-8.8-9.4-21.3-15.2-35.2-15.2 c-26.6,0-48.2,21.6-48.2,48.2c0,3.8,0.4,7.5,1.3,11c-40.1-2-75.6-21.2-99.4-50.4c-4.1,7.1-6.5,15.4-6.5,24.2 c0,16.7,8.5,31.5,21.5,40.1c-7.9-0.2-15.3-2.4-21.8-6c0,0.2,0,0.4,0,0.6c0,23.4,16.6,42.8,38.7,47.3c-4,1.1-8.3,1.7-12.7,1.7 c-3.1,0-6.1-0.3-9.1-0.9c6.1,19.2,23.9,33.1,45,33.5c-16.5,12.9-37.3,20.6-59.9,20.6c-3.9,0-7.7-0.2-11.5-0.7 C110.8,297.5,136.2,305.5,163.4,305.5"/></g></svg></a>
            <a href="https://www.facebook.com/OReilly/" target="_blank"><svg data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32.25 32.25"><defs><style>.cls-1{fill:#E00;fill-rule:evenodd;}</style></defs><title>facebook-logo</title><path class="cls-1" d="M16.13,0A16.13,16.13,0,1,0,32.25,16.12,16.13,16.13,0,0,0,16.13,0Zm5.12,9.54H19.57a1.52,1.52,0,0,0-1.51,1.51v1.14h3.19v3.42H18.06V26.12H13.57V15.61H10.69V12.19h2.88v-2a4,4,0,0,1,4-4h3.65Z"/></svg></a>
            <a href="https://www.linkedin.com/company/oreilly-media" target="_blank"><svg data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32.25 32.25"><defs><style>.cls-1{fill:#E00;fill-rule:evenodd;}</style></defs><title>linkedin-logo</title><path class="cls-1" d="M17.43,13.53v0l0,0ZM16.12,0A16.13,16.13,0,1,0,32.25,16.12,16.12,16.12,0,0,0,16.12,0ZM11.77,22.92H8.12v-11h3.65ZM9.94,10.44h0a1.89,1.89,0,0,1-2-1.89A1.91,1.91,0,0,1,10,6.65a1.9,1.9,0,1,1,0,3.79Zm15,12.48H21.28V17.05c0-1.48-.53-2.49-1.85-2.49a2,2,0,0,0-1.88,1.34,2.63,2.63,0,0,0-.12.89v6.13H13.79s.05-10,0-11h3.64V13.5a3.63,3.63,0,0,1,3.29-1.82c2.4,0,4.21,1.57,4.21,4.95Z"/></svg></a>
            <a href="https://www.youtube.com/user/OreillyMedia" target="_blank"><svg data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32.25 32.25"><defs><style>.cls-1{fill:#E00;fill-rule:evenodd;}</style></defs><title>youtube-logo</title><path class="cls-1" d="M18.35,18.68a.89.89,0,0,0-.86.63V16.94h-1v7.59h1v-.59a.92.92,0,0,0,.89.74c.52,0,.86-.36,1-1.07a8.56,8.56,0,0,0,.14-1.9,10.47,10.47,0,0,0-.12-1.93C19.21,19.05,18.87,18.68,18.35,18.68Zm.09,4.14c-.05.52-.2.77-.45.77s-.43-.21-.49-.63a7.22,7.22,0,0,1,0-1c0-.72,0-1.14,0-1.26.07-.58.23-.88.51-.88s.4.27.46.79c0,.09,0,.47,0,1.13S18.46,22.71,18.44,22.82ZM8.61,18.19H9.86v6.34H11V18.19H12.2V16.94H8.61Zm7.71-6.12c.27,0,.43-.22.49-.66,0-.09,0-.45,0-1.07V9.78c0-.58,0-.93,0-1-.07-.43-.23-.65-.48-.65s-.41.19-.48.57a8.79,8.79,0,0,0,0,1v.53c0,.71,0,1.09,0,1.16C15.88,11.82,16.05,12.07,16.32,12.07Zm-1.74,10.4a2.92,2.92,0,0,1-.06.71c-.07.25-.2.37-.4.37s-.32-.11-.38-.34a2.63,2.63,0,0,1-.05-.66V18.83h-1v4.48c0,.91.33,1.37,1,1.37A1,1,0,0,0,14.6,24v.57h1v-5.7h-1Zm7.1-3.79a1.35,1.35,0,0,0-1.22.59,2.48,2.48,0,0,0-.3,1.07c0,.23,0,.61,0,1.16s0,1.13,0,1.37a2.93,2.93,0,0,0,.35,1.27,1.25,1.25,0,0,0,1.14.54,1.36,1.36,0,0,0,1.13-.45,2.29,2.29,0,0,0,.37-1.43s0-.07,0-.11v-.12h-1c0,.75-.17,1.12-.51,1.12s-.38-.18-.45-.56a4.68,4.68,0,0,1-.06-.9c0-.18,0-.31,0-.37h2v-.43a4.86,4.86,0,0,0-.3-2.11A1.28,1.28,0,0,0,21.68,18.68ZM22.15,21h-1c0-.05,0-.11,0-.16v-.08a1.64,1.64,0,0,1,.11-.8.39.39,0,0,1,.38-.23.42.42,0,0,1,.43.33,2.32,2.32,0,0,1,.07.7Zm-6-21A16.13,16.13,0,1,0,32.25,16.12,16.12,16.12,0,0,0,16.12,0Zm2.51,7.2h1V11a2.52,2.52,0,0,0,.06.66c.06.23.18.34.38.34s.34-.12.41-.37a3.76,3.76,0,0,0,.05-.71V7.2h1v5.74h-1v-.57a1,1,0,0,1-.93.72c-.66,0-1-.46-1-1.38ZM15,8a1.29,1.29,0,0,1,1.33-.92A1.3,1.3,0,0,1,17.65,8a6.77,6.77,0,0,1,.23,2.1,6.77,6.77,0,0,1-.23,2.1,1.3,1.3,0,0,1-1.32.92A1.29,1.29,0,0,1,15,12.17a6.77,6.77,0,0,1-.23-2.1A6.86,6.86,0,0,1,15,8ZM11.45,5.3l.89,2.87.88-2.87H14.4L12.88,9.7v3.24h-1.1V9.7L10.24,5.3ZM24.93,23.91a3,3,0,0,1-3,3h-12a3,3,0,0,1-3-3V18.05a3,3,0,0,1,3-3h12a3,3,0,0,1,3,3Z"/></svg></a>
          </div>
        </div>
      </div>

      <div class="footer-download" id="download-info">
        <h2 class="footer-header">Download the O’Reilly App</h2>

        <div class="footer-downloadLinks">
          <a href="https://itunes.apple.com/us/app/safari-to-go/id881697395"><img src="https://cdn.oreillystatic.com/oreilly/images/app-store-logo.png" alt="Apple app store" /></a>
          <a href="https://play.google.com/store/apps/details?id=com.safariflow.queue"><img src="https://cdn.oreillystatic.com/oreilly/images/google-play-logo.png"  alt="Google play store" /></a>
        </div>

        <p>Take O’Reilly online learning with you and learn anywhere, anytime on your phone <span class="nowrap">and tablet.</span></p>

        <ul>
          <li>Get unlimited access to books, videos, and <span class="nowrap">live training.</span></li>
          <li>Sync all your devices and never lose your place.</li>
          <li>Learn even when there’s no signal with <span class="nowrap">offline access.</span></li>
        </ul>
      </div>

      <div class="footer-donotsell" id="donotsell-info">
        <h2 class="footer-header">Do not sell my personal information</h2>

        <p style="line-height: 1.5em;">Exercise your consumer rights by contacting us at <a href="mailto:donotsell@oreilly.com?subject=Do Not Sell My Personal Information Request" style="text-decoration:underline;">donotsell@oreilly.com</a>.</p>
      </div>
    </div>

    <div class="footer-subfooter">

      <a href="https://www.oreilly.com" title="home page" aria-current="page">
        <img 
        class="footer-subfooterLogo" 
        id="footer-subfooterLogo"
        src="https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg" 
        onerror="this.src='https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red_@2x.png'; this.onerror=null;" 
        alt="O'Reilly home">
      </a>

      <p>&copy; 2020, O’Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.</p>
      
      

      <p><a href="https://www.oreilly.com/terms/">Terms of service</a> • <a href="https://www.oreilly.com/privacy.html">Privacy policy</a> • <a href="https://www.oreilly.com/about/editorial_independence.html">Editorial independence</a></p>
    </div>

  </div>
</footer>




<script>
  var g = {
    
    position_cache: {},
    title: "Deep Learning for Coders with fastai and PyTorch",
    author_list: "Jeremy Howard, Sylvain Gugger",
    format: "book",
    source: "application/epub+zip",
    is_system_book: true,
    is_public: true,
    loaded_from_server: true,
    allow_scripts: false,
    has_mathml: true
    
  };
</script>
<script>
  window.dataLayer = window.dataLayer || [];
  window.dataLayer.push({
    'product.title': "Deep Learning for Coders with fastai and PyTorch",
    'product.type': "book",
    'product.identifier': "9781492045519",
    'content.identifier': "9781492045519/ch04.html",
    'content.publisher': "O'Reilly Media, Inc.",
    'content.free': "no",
    
    'content.subdirectory': "none",
    'content.subTopic': "none",
    'content.parentTopic': "none",
    'content.formatType': "book-chapter",
    'content.author': "Jeremy Howard, Sylvain Gugger",
    'content.releaseDate': "2020-07-21",
    'content.title': "4. Under the Hood: Training a Digit Classifier",
  });
</script>






<script src="/library/view/static/CACHE/js/output.3fa4edd6d217.js"></script>


<noscript> 
  <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
          height="0" width="0"
          style="display:none;visibility:hidden">
  </iframe>
</noscript>


<script async defer src="/library/view/pageview.js"></script>

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>

<script>
  jQuery(document).ready(function($) {
    //Toggle isActive and mobileHidden classes for mobileNavButton
    $('nav #mobileNavButton').on('click', function() {
      var expanded = $(this).attr('aria-expanded') === 'true' || false;
      $(this).attr('aria-expanded', !expanded);
      $(this).toggleClass("isActive");
      $(this).next().toggleClass("mobileHidden");
    });

    //Toggle isFocused class for keyboard navigation of submenus
    $('nav #menuList .menuList-subItem a').each(function(navItem) {
      $(this).on('focus', function() {
        $(this).parent().parent().toggleClass('isFocused');
      });
      $(this).on('blur', function() {
        $(this).parent().parent().toggleClass('isFocused');
      });
    });

    //Toggle mobileHidden class accordian elements
    $('.mobileAccordian').each(function() {
      var $btn = $(this).find('button');
      var $target =  $(this).next();
      $btn.on('click', function() {
        var expanded = $btn.attr('aria-expanded') === 'true' || false;

        $btn.attr('aria-expanded', !expanded);
        $target.toggleClass("mobileHidden");
      });
    });


    //Hide title page's long description behind disclosure button
    var titleDescriptionExtra = $('.title-description p:first-of-type').nextAll(':nth-child(3)').nextAll();
    titleDescriptionExtra.wrapAll('<div id="titleDescriptionExtra" />');
    if (titleDescriptionExtra.length > 0) {
      $('#titleDescriptionExtra').toggle();
      $('#showMoreDescription').removeClass('hidden');
      $('#showMoreDescription button').on('click', function() {
        $(this).toggleClass('more');
        $('#titleDescriptionExtra').slideToggle();
      });
    }

    //Hide title page's long TOC behind disclosure button
    var titleTOCShort = $('.sbo-reader-title .detail-toc').find('li').slice(0,4);
    var titleTOCExtra = $('.sbo-reader-title .detail-toc').find('li').not(titleTOCShort);
    if (titleTOCExtra.length > 0) {
      titleTOCExtra.toggle();
      $('#showMoreTOC').removeClass('hidden');
      $('#showMoreTOC button').on('click', function() {
        $(this).toggleClass('more');
        titleTOCExtra.toggle();
      });
    }

    //Hide product information on page load
    $('#product-information').toggle();
    $('#title-tabs button').on('click', function() {
      var $thisTab = $(this);
      if (!$thisTab.hasClass('title-tab-active')) {
        var lastTabTarget = $('#title-tabs .title-tab-active').removeClass('title-tab-active').attr('data-target');
        $('#' + lastTabTarget).toggle();

        $thisTab.addClass('title-tab-active');
        var $tabTarget = $(this).attr('data-target');
        $('#' + $tabTarget).toggle();
      }
    });


    //Trigger GA events for related products
    $('#recommendations .recommendations-item').each(function(i) {
      $(this).on('click', function(e) {
        var cardTitle = $(this).find('.recommendations-title').text().toLowerCase();
        var productType = $(this).find('.recommendations-format').text().toLowerCase();
        var contentID = $(this).attr('data-id');
        var eventLbl = String(i);

        dataLayer.push({
          'product.title': undefined,
          'content.formatType': undefined,
          'product.identifier': undefined,
          'content.title': undefined,
          'content.author': undefined,
          'content.publisher': undefined,
          'content.releaseDate': undefined,
          'content.free': undefined,
          'content.subdirectory': undefined,
          'content.parentTopic': undefined,
          'content.subTopic': undefined,
          'event': 'eventTracker',
          'eventCat': 'recommended titles',
          'eventAct': 'card click',
          'eventLbl': eventLbl,
          'eventVal': 0, 
          'nonInteraction': 0,
          'cardTitle': cardTitle,
          'product.type': productType,
          'content.identifier': contentID,
          'eventCallback': function() {
            dataLayer.push({'cardTitle': undefined});
          },
          'eventTimeout' : 2000
        });
      });
    });

  });

</script>



<!-- MARC Intercept Support -->

<script src="https://service.seamlessaccess.org/thiss.js"></script>

<script src="/library/view/static/CACHE/js/output.5426f7a3b865.js"></script>


</body>
</html>
