# CHAPTER 5.1: IMAGE CLASSIFICATION

To make your model really work, and work reliably, there are a lot of details you have to get right, 
and a lot of details that you have to check.     
This process requires being able to look inside your neural network as it trains and as it makes predictions, 
find possible problems, and know how to fix them.

We'll need to do a deep dive into the mechanics of deep learning. 
- What is the architecture of a computer vision model, 
 - an NLP model, 
 - a tabular model, and others.
 - How do we create an architecture that matches the needs of our particular domain
 - How do we get the best possible results from the training process 
 - How do we make things faster
 - What do we have to change as our datasets change.
 
 We will start by repeating the same basic applications that we looked at in the first chapter, but we are going to do two things: 
 - Make them better.  
 - Apply them to a wider variety of types of data.

## FROM DOGS AND CATS TO PET BREEDES

The same dataset we used to identify cats from dogs will also allow us to work on a much more challenging problem:     
**figuring out what breed of pet is shown in each image.**

In real life, we'll normally start with a dataset that we don't know anything about. We then have to figure out:
= how it is put together, 
- how to extract the data we need from it, 
- and what that data looks like.

We already downloaded the Pets dataset, and we can get a path to this dataset using the same code as in Chapter 1: 
```python
from fastai.vision.all import *
path = untar_data(URLs.PETS)
```

Now, to be able to extract the breed of each pet, we'll need to understand how the data is laid out.     
Data is usually provided in one of these two ways: 
- Individual files representing items of data, possibly organized into folders or with filenames representing information about those items  
- A table of data (e.g., in CSV format) in which each row is an item and may include filenames providing connections between the data in the table 
and data in other formats, such as text documents and images.

To see what is in our dataset, we can use the *ls* method: 
```python
path.ls() 
```
(#3) [Path('annotations'),Path('images'),Path('models')]

The *annotations* directory contains information about where the pets are. 
Since we'll be doing classification, not localization, we will ignore the *annotations* directory.     
We can use the same *ls* method to se what's inside the *images* folder:
```python
(path/'images').ls()
```

(#7394) [Path('images/great_pyrenees_173.jpg'),Path('images/wheaten_terrier_46.jpg'),
Path('images/Ragdoll_262.jpg'),Path('images/german_shorthaired_3.jpg'),
Path('images/american_bulldog_196.jpg'),Path('images/boxer_188.jpg'),
Path('images/staffordshire_bull_terrier_173.jpg'),Path('images/basset_hound_71.jpg'),
Path('images/staffordshire_bull_terrier_37.jpg'),Path('images/yorkshire_terrier_18.jpg')...].    

The first thing that is shown is the number of items in the collection, prefixed with a *#*, and only the first few items are displayed.

We can see that each file name is structured as follows:
1. the pet breed,
2. then an underscore (_),
3. a number,
4. the file extension (.jpg)

We need to create a piece of code that extracts the breed from a single Path, and the use it for the whole dataset.     
To allow us to test our code, let’s pick out one of these filenames: 
```python
fname = (path/"images").ls()[0]
```
The authors suggest that the most powerful and flexible way to extract information from strings like this is to use a **regular expressions**, 
also known as a ***regex***. A regular expression is a special string, written in the regular expression language, 
which specifies a general rule for deciding whether another string passes a test (i.e., “matches” the regular expression), 
and also possibly for plucking a particular part or parts out of that other string.

> Since Jeremy talks so highly about *regex* and considers them as one of the most useful tools in our programming toolkit, and also many of his students found regex as the most exiting tool to learn, I dived into several tutorials in the web and summarised the main concpets in a notebook. I hadn't worked with regex before and found them interesting but challenging and not easy to digest without a lot of prectice. But you can form your opinion looking at the notebook here, if you're not used to them. [regex notebook](https://github.com/luisenoz/luisenoz.github.io/blob/master/images/RegEx.ipynb)

Jeremy uses *findall*, one of the *regex* methods, to try a regular expression against the filename of the *fname* object:
```python
re.findall(r'(.+)_\d+.jpg$', fname.name)
```
*This regular expression plucks out all the characters leading up to the last underscore character, 
as long as the subsequent characters are numerical digits and then the JPEG file extension.*

['great_pyrenees']

Now that we know it worked for one example, we ca use it to label the whole dataset.     
As expected, *fastai* comes with several classes to help with labelling. For example, if we want to label with regular expressions, we can use the *RegexLabeller* class. And in this case we'll use the Datablock API we already used in Chapter 2.
```python
pets = DataBlock(blocks = (ImageBlock, CategoryBlock),
                 get_items=get_image_files,
splitter=RandomSplitter(seed=42),
                 get_y=using_attr(RegexLabeller(r'(.+)_\d+.jpg$'), 'name'),
                 item_tfms=Resize(460),
                 batch_tfms=aug_transforms(size=224, min_scale=0.75))
dls = pets.dataloaders(path/"images")
```
One important piece of this DataBlock call that we haven’t seen before is in these two lines: 
```python
item_tfms=Resize(460),
batch_tfms=aug_transforms(size=224, min_scale=0.75)
```

Those lines implement a *fastai* data augmentation strategy called *presizing*, that is designaed to reduce data destruction while maintaining a good performance.

## Presizing

We need the images to have all the same dimensions.     
We also search to minimise the claculations needed for augmentation.     
There is also the challenge that, if performed after resizing down, various common data augmentation transforms might introduce 
spurious empty zones, degrade data, or both.
*Presizing* adopts two startegies to work around those challenges:
1. Resizes images to relatively *large* dimensions - significantly larger than the target training dimensiosn.
2. Compose all common augmentation operations *(including a resize to the final target size)* into one, 
and perform the combined operation on the GPU only once at the end of processing, 
rather than performing the operations individually and interpolating multiple times.

The first step, ***crop full width or height*** is in *item_tfms* so it's applied to each individual image before it's copied to the GPU. It ensures all images are of the same size.     
The resize, creates images large enough that they have spare margin to allow further augmentation transforms on their inner regions without creating empty zones. This transformation works by resizing to a square, using a large crop size. On the training set, the crop area is chosen randomly, and the size of the crop is selected to cover the entire width (for portrait images) or height (for landscape images) of the image, whichever is smaller. In the validation set the centre square of the image is always chosen.

In the second step, ***random crop and augment*** is in *batch_tfms* and it's applied to a batch all at once on the GPU, which means is faster and all of the potentially destructive operations are done together, with a single interpolation at the end. On the validation set, only the resize to the final size needed for the model is done here. On the training set, the random crop and any other augmentations are done first.

To implement this process in fastai, you use *Resize* as an item transform with a large size, and *RandomResizedCrop* as a batch transform with a smaller size. *RandomResizedCrop* will be added if you include the *min_scale* parameter in your *aug_transforms* function, as was done in the DataBlock. Alternatively, you can use *pad* or *squish* instead of *crop* (the default) for the initial *Resize*.

If you looked at an image that has been zoomed, interpolated, rotated, and then interpolated again *(which is the approach used by all other deep learning libraries)*, and an image that has been zoomed and rotated as one operation and then interpolated once *(the fastai approach)*, you would see that the first image is less well defined and could have reflection padding artifacts; as well as some parts disappeared entirely. Jeremy found that, in practice, using *presizing* significantly improves the accuracy of models and often results in speedups too.

### Checking and debugging a DataBlock

Writing a DataBlock we will get an error message if we have a syntax error somewhere in our code, but we have no guarantee that our template is going to work on our data source as we intend. So, **before training a model, we should always check our data**.

```python
dls.show_batch(nrows=1, ncols=3)
```
The *show_batch* method will show the images and we can then check that each photo corresponds to the right label. If we're not familiar with the dataset, we can always search a sample in Google and verify that our images and labels are right.

If we made a mistake while building our DataBlock, we likely won’t see it before this step.     
To debug this, the authors encourage us to use the *summary* method. It will attempt to create a batch from the source we give it, with a lot of details.     
Also, if it fails, we will see exactly at which point the error happens, and the library will try to give some help.

For instance, one common mistake is to forget to use a *Resize* transform, so we end up with pictures of different sizes and are not able to batch them.

```python
pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),
                 get_items=get_image_files, 
                 splitter=RandomSplitter(seed=42),
                 get_y=using_attr(RegexLabeller(r'(.+)_\d+.jpg$'), 'name'))
pets1.summary(path/"images")
```
The summary will provide the following result:

---
Setting-up type transforms pipelines.    
Collecting items from /storage/data/oxford-iiit-pet/images.    
Found 7390 items.    
2 datasets of sizes 5912,1478.    
Setting up Pipeline: PILBase.create.    
Setting up Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}.    

Building one sample.    
  Pipeline: PILBase.create.    
      starting from.    
            /storage/data/oxford-iiit-pet/images/shiba_inu_98.jpg.    
      applying PILBase.create gives.    
      PILImage mode=RGB size=500x374
      Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}.    
      starting from.    
           /storage/data/oxford-iiit-pet/images/shiba_inu_98.jpg.    
      applying partial gives. 
      shiba_inu.    
      applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives.    
      TensorCategory(33)

Final sample: (PILImage mode=RGB size=500x374, TensorCategory(33))

Setting up after_item: Pipeline: ToTensor.    
Setting up before_batch: Pipeline:      
Setting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}

Building one batch.    
Applying item_tfms to the first sample:     
  Pipeline: ToTensor.    
      starting from.    
       (PILImage mode=RGB size=500x374, TensorCategory(33))
    applying ToTensor gives.    
     (TensorImage of size 3x374x500, TensorCategory(33))

Adding the next 3 samples

No before_batch transform to apply

Collating items in a batch.     
**Error! It's not possible to collate your items in a batch. 
Collating items in a batch.     
Could not collate the 0-th members of your tuples because got the following shapes.     
torch.Size([3, 374, 500]),torch.Size([3, 375, 500]),torch.Size([3, 500, 424]),torch.Size([3, 351, 500])**

---

We can see exactly how we gathered the data and split it, how we went from a filename to a sample (the tuple (image, category)), 
then what item transforms were applied and how it failed to collate those samples in a batch (because of the different shapes).

Once we think the data looks right, the authors generally recommend the next step should be using it to train a simple model.     
For this initial test, we can use the same simple model that we used in Chapter 1:
```python
learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(2)
```
|epoch. |train_loss. |valid_loss  |error_rate  |time|
|-----|-----|-----|-----|-----|
|0	|1.483786	|0.346272	|0.100812	|00:30|

|epoch. |train_loss. |valid_loss  |error_rate  |time|
|-----|-----|-----|-----|-----|
|0	|0.495099	|0.311673	|0.089986	|00:35|
|1	|0.318182	|0.216932	|0.071719	|00:30|

We haven’t actually told fastai what loss function we want to use. But fastai will generally try to select an appropriate loss function 
based on the kind of data and model we are using.     
In this case, we have image data and a categorical outcome, so fastai will default to using ***cross-entropy loss***.


## Cross-Entropy Loss

Cross-entropy loss is a function similar to the one we've been using but with 2 advantages:
1. It works, even when the dependent variable has more than 2 categories *(several breeds instead of dog or cat)*
2. It results in faster and more reliable training.

But, before trying to understand how cross-entropy loss works, we first have to understand how the actual data 
and activations that are seen by the loss function look like.

### Viewing Activation and Labels

We can start by using the *one_batch* method to get a btach of real data from the *DataLoaders*
```python
x,y = dls.one_batch()
y
```
TensorCategory([31,  4,  1, 30,  6,  6, 11, 15, 24, 14, 13, 17,  2, 12,  5, 34,  6, 19, 15,  3,  7,  8, 33, 12,  8,     
1, 13, 14, 30,  8, 25, 32, 25,  5, 17, 31, 29,  9, 35, 24,  2, 30, 15, 19,  6, 35, 10,  5,     
        14,  6, 10, 29, 25, 22,  2, 33,  6,  7, 25, 29, 27, 15, 32, 13], device='cuda:0')

Our batch size is 64, so we have 64 rows in this tensor.     
Each row is a single integer between 0 and 36, representing our 37 possible pet breeds.

We can view the predictions *(the activations of the final layer of our neural network)* by using *Learner.get_preds*.     
This function takes either a dataset index (0 for train and 1 for valid) or an iterator of batches. Thus, we can pass it a simple list with our batch to get our predictions.     
It returns predictions and targets by default, but since we already have the targets, we can effectively ignore them by assigning to the special variable *_*:

```python
preds,_ = learn.get_preds(dl=[(x,y)])
preds[0]
```
TensorImage([2.9641e-08, 5.0075e-07, 8.4463e-07, 9.4931e-07, 4.5364e-06, 1.2933e-05, 1.2391e-06, 3.8727e-05, 1.4260e-05, 1.2109e-06, 4.3693e-08, 2.6370e-07, 3.9565e-06, 6.5704e-06, 1.8649e-06, 1.2336e-06,
        1.7263e-07, 1.7804e-07, 4.4786e-07, 1.1471e-05, 2.4972e-06, 1.7159e-03, 3.1686e-04, 3.8349e-06, 2.3981e-04, 2.3053e-06, 3.4829e-07, 1.0206e-04, 2.3513e-05, 8.6500e-08, 4.5018e-06, 9.9747e-01,
        1.9257e-06, 1.3839e-06, 1.1302e-05, 2.8269e-06, 4.5107e-07])
        
The predictions are 37 probabilities that go from 0 to 1, and total 1.
```python
len(preds[0]),preds[0].sum()
```
(37, TensorImage(1.))

To transform the activations of our model into predictions like this, we used a new fucntion called the ***softmax*** **activation function**.

### Softmax

We used the *softmax* function in the last layer of the model to ensure that the activations were all between 0 and 1, and that all they sum to 1.     
softmax is the multi-category equivalent of sigmoid — we have to **use it anytime we have more than two categories** and the probabilities of the categories must add to 1, and we often use it even when there are just two categories, just to make things a bit more consistent.     
```python
def softmax(x): 
     return exp(x) / exp(x).sum(dim=1, keepdim=True)
```

> **Jargon:** Exponential Function (exp) Defined as e ** x, where e is a special number approximately equal to 2.718. 
It is the inverse of the natural logarithm function. Note that **exp is always positive and increases very rapidly!**

Taking the exponential ensures all our numbers are positive, and then dividing by the sum ensures we are going to have a bunch of numbers that add up to 1.     
The exponential also has a nice property: if one of the numbers in our activations x is slightly bigger than the others, the exponential will amplify this (since it grows exponentially), which means that in the *softmax*, that number will be closer to 1. So it’s ideal for training a classifier when we know each picture has a definite label.     
*(Note that it may be less ideal during inference, as you might want your model to sometimes tell you it doesn’t recognize any of the classes that it has seen during training, and not pick a class because it has a slightly bigger activation score.)*

*Softmax* is the first part of the cross-entropy loss — the second part is *log likelihood*.

### Log Likehood


