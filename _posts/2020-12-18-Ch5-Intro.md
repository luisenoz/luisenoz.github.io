# CHAPTER 5.1: IMAGE CLASSIFICATION

To make your model really work, and work reliably, there are a lot of details you have to get right, 
and a lot of details that you have to check.     
This process requires being able to look inside your neural network as it trains and as it makes predictions, 
find possible problems, and know how to fix them.

We'll need to do a deep dive into the mechanics of deep learning. 
- What is the architecture of a computer vision model, 
 - an NLP model, 
 - a tabular model, and others.
 - How do we create an architecture that matches the needs of our particular domain
 - How do we get the best possible results from the training process 
 - How do we make things faster
 - What do we have to change as our datasets change.
 
 We will start by repeating the same basic applications that we looked at in the first chapter, but we are going to do two things: 
 - Make them better.  
 - Apply them to a wider variety of types of data.

## FROM DOGS AND CATS TO PET BREEDES

The same dataset we used to identify cats from dogs will also allow us to work on a much more challenging problem:     
**figuring out what breed of pet is shown in each image.**

In real life, we'll normally start with a dataset that we don't know anything about. We then have to figure out:
= how it is put together, 
- how to extract the data we need from it, 
- and what that data looks like.

We already downloaded the Pets dataset, and we can get a path to this dataset using the same code as in Chapter 1: 
```python
from fastai.vision.all import *
path = untar_data(URLs.PETS)
```

Now, to be able to extract the breed of each pet, we'll need to understand how the data is laid out.     
Data is usually provided in one of these two ways: 
- Individual files representing items of data, possibly organized into folders or with filenames representing information about those items  
- A table of data (e.g., in CSV format) in which each row is an item and may include filenames providing connections between the data in the table 
and data in other formats, such as text documents and images.

To see what is in our dataset, we can use the *ls* method: 
```python
path.ls() 
```
(#3) [Path('annotations'),Path('images'),Path('models')]

The *annotations* directory contains information about where the pets are. 
Since we'll be doing classification, not localization, we will ignore the *annotations* directory.     
We can use the same *ls* method to se what's inside the *images* folder:
```python
(path/'images').ls()
```

(#7394) [Path('images/great_pyrenees_173.jpg'),Path('images/wheaten_terrier_46.jpg'),
Path('images/Ragdoll_262.jpg'),Path('images/german_shorthaired_3.jpg'),
Path('images/american_bulldog_196.jpg'),Path('images/boxer_188.jpg'),
Path('images/staffordshire_bull_terrier_173.jpg'),Path('images/basset_hound_71.jpg'),
Path('images/staffordshire_bull_terrier_37.jpg'),Path('images/yorkshire_terrier_18.jpg')...].    

The first thing that is shown is the number of items in the collection, prefixed with a *#*, and only the first few items are displayed.

We can see that each file name is structured as follows:
1. the pet breed,
2. then an underscore (_),
3. a number,
4. the file extension (.jpg)

We need to create a piece of code that extracts the breed from a single Path, and the use it for the whole dataset.     
To allow us to test our code, let’s pick out one of these filenames: 
```python
fname = (path/"images").ls()[0]
```
The authors suggest that the most powerful and flexible way to extract information from strings like this is to use a **regular expressions**, 
also known as a ***regex***. A regular expression is a special string, written in the regular expression language, 
which specifies a general rule for deciding whether another string passes a test (i.e., “matches” the regular expression), 
and also possibly for plucking a particular part or parts out of that other string.

> Since Jeremy talks so highly about *regex* and considers them as one of the most useful tools in our programming toolkit, and also many of his students found regex as the most exiting tool to learn, I dived into several tutorials in the web and summarised the main concpets in a notebook. I hadn't worked with regex before and found them interesting but challenging and not easy to digest without a lot of prectice. But you can form your opinion looking at the notebook here, if you're not used to them. [regex notebook](https://github.com/luisenoz/luisenoz.github.io/blob/master/images/RegEx.ipynb)

Jeremy uses *findall*, one of the *regex* methods, to try a regular expression against the filename of the *fname* object:
```python
re.findall(r'(.+)_\d+.jpg$', fname.name)
```
*This regular expression plucks out all the characters leading up to the last underscore character, 
as long as the subsequent characters are numerical digits and then the JPEG file extension.*

['great_pyrenees']

Now that we know it worked for one example, we ca use it to label the whole dataset.     
As expected, *fastai* comes with several classes to help with labelling. For example, if we want to label with regular expressions, we can use the *RegexLabeller* class. And in this case we'll use the Datablock API we already used in Chapter 2.
```python
pets = DataBlock(blocks = (ImageBlock, CategoryBlock),
                 get_items=get_image_files,
splitter=RandomSplitter(seed=42),
                 get_y=using_attr(RegexLabeller(r'(.+)_\d+.jpg$'), 'name'),
                 item_tfms=Resize(460),
                 batch_tfms=aug_transforms(size=224, min_scale=0.75))
dls = pets.dataloaders(path/"images")
```
One important piece of this DataBlock call that we haven’t seen before is in these two lines: 
```python
item_tfms=Resize(460),
batch_tfms=aug_transforms(size=224, min_scale=0.75)
```

Those lines implement a *fastai* data augmentation strategy called *presizing*, that is designaed to reduce data destruction while maintaining a good performance.

## Presizing

We need the images to have all the same dimensions.     
We also search to minimise the claculations needed for augmentation.     
There is also the challenge that, if performed after resizing down, various common data augmentation transforms might introduce 
spurious empty zones, degrade data, or both.
*Presizing* adopts two startegies to work around those challenges:
1. Resizes images to relatively *large* dimensions - significantly larger than the target training dimensiosn.
2. Compose all common augmentation operations *(including a resize to the final target size)* into one, 
and perform the combined operation on the GPU only once at the end of processing, 
rather than performing the operations individually and interpolating multiple times.

The first step, ***crop full width or height*** is in *item_tfms* so it's applied to each individual image before it's copied to the GPU. It ensures all images are of the same size.     
The resize, creates images large enough that they have spare margin to allow further augmentation transforms on their inner regions without creating empty zones. This transformation works by resizing to a square, using a large crop size. On the training set, the crop area is chosen randomly, and the size of the crop is selected to cover the entire width (for portrait images) or height (for landscape images) of the image, whichever is smaller. In the validation set the centre square of the image is always chosen.

In the second step, ***random crop and augment*** is in *batch_tfms* and it's applied to a batch all at once on the GPU, which means is faster and all of the potentially destructive operations are done together, with a single interpolation at the end. On the validation set, only the resize to the final size needed for the model is done here. On the training set, the random crop and any other augmentations are done first.

To implement this process in fastai, you use *Resize* as an item transform with a large size, and *RandomResizedCrop* as a batch transform with a smaller size. *RandomResizedCrop* will be added if you include the *min_scale* parameter in your *aug_transforms* function, as was done in the DataBlock. Alternatively, you can use *pad* or *squish* instead of *crop* (the default) for the initial *Resize*.

If you looked at an image that has been zoomed, interpolated, rotated, and then interpolated again *(which is the approach used by all other deep learning libraries)*, and an image that has been zoomed and rotated as one operation and then interpolated once *(the fastai approach)*, you would see that the first image is less well defined and could have reflection padding artifacts; as well as some parts disappeared entirely. Jeremy found that, in practice, using *presizing* significantly improves the accuracy of models and often results in speedups too.









