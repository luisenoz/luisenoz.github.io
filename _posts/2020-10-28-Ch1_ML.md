# WHAT IS MACHINE LEARNING

Deep learning is a modern area in the more general discipline of Machine Learning.  
And ML, like any regular programming, is a way to get computers to complete a specific task.  
The difference is that instead of telling the computer the exact steps required to solve a problem, ML shows the computer examples of the problem, 
and let it figure out how to solve it itself.  

The key notions about this concept are:
- The idea of assigning weights that, in conjuction with the inputs, will define how the model will operate.
- The fact that different weights assignments will make the model produce different performances.
- The requirement of an automatic mechanism or formula to meassure the model's performance.
- A mechanism to improve the performance by changing the weights assigments.

## NEURAL NETWORKS

You could define a NN as a kind of function that is so flexible that it could be used to solve any given problem, just by varying its weights.  
A mathematical proof called *the universal approximation theorem* shows that, in theory, this function can solve any problem to any level of accuracy.  
And for the other part of the model - a completely general way to update the weights of a neural network, to make it improve at any given task,
we already have the *stochastic gradient descent* (SGD)!!!  
And for an automatic means of testing the actual performance of any current weight assignment; we can simply define our model’s performance 
as its accuracy at predicting the correct answers.

### DL Jargon

- **architecture** is the functional part of the model (the NN).
- **parameters** are the weights. They are calculated by the NN looking to improve the performance. They are not to be confused with *hyperparameters* 
that are values defined by the user looking to run the model more efficiently.
- **predictions** are calculated from the **independent variables**, which is *the data* without including the dependent variables called **labels**.
- The **results** of the model are called **predictions**.
- The *meassure of performance* is called the **loss**.
- The *loss* depends on the *predictions* and the *labels*

### ML Limitations:

- A model cannot be created without data.  
- A model can learn to operate on only the patterns seen in the input data used to train it.  
- This learning approach creates only predictions, not recommended actions.  
- It’s not enough to just have examples of input data; we need labels for that data too.

Another critical insight comes from considering how a model interacts with its environment. This can create what's called **feedback loops:** 
If we train a model with a given set of features, and we exhibit action based on those features, 
then those features are now correlated with the outcome and all subsequent models will continue to use them.  
In a *positive feedback loop*, the more the model is used, the more biased the data becomes, making the model even more biased, and so forth.  




