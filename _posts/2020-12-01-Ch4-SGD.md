# Stochastic Gradient Descent

Instead of trying to find the similarity between an image and an “ideal image,” we could instead look at each individual pixel 
and come up with a set of weights for each, such that the highest weights are associated with those pixels most likely to be black 
for a particular category.     
For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, 
but they are likely to be activated for an 8, so they should have a high weight for an 8.     
This can be represented as a function and set of weight values for each possible category — for instance, the probability of being the number 8:     
```python
def pr_eight(x,w) = (x*w).sum()
```

Here we are assuming that X is the image, represented as a vector — in other words, with all of the rows stacked up end to end into a single long line. 
And we are assuming that the weights are a vector W.     
We just need some way to update the weights to make them a little bit better. With such an approach, we can repeat that step a number of times, 
making the weights better and better, until they are as good as we can make them.     
Searching for the best vector W is a way to search for the best function for recognizing 8s.

These are the steps required to turn this function into a machine learning classifier:
1. *Initialise* the weights.
2. For each image, use the weights to *predict* if it's a 3 or a 7.
3. Based on those predictions, calculate how good the model is (its *loss*).
4. Calculate the *gradient*, which meassures for each weight how changing that weight would change the loss.
5. *Step* (change) all the weights accrding wiith the calculation in point 4.
6. Back to step 2 and *repeat* the process.
7. Iterate until you decide to *stop* the process, (becasue the model is good enough or you don't want to spend more time training it)

There are many ways to do each of these seven steps. And these are the details that make a big difference for deep learning practitioners, 
but it turns out that the general approach to each one follows some basic principles.      
___
Here are a few guidelines:

### Initialise
We initialize the parameters to random values. There are certainly other choices we could make, 
but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well.

### Loss
We need a function that will return a number that is small if the performance of the model is good 
*(the standard approach is to treat a small loss as good and a large loss as bad, although this is just a convention)*.

### Step
Calculus allows us to directly figure out in which direction, and by roughly how much, to change each weight, 
without having to try manually by small changes. The way to do this is by calculating gradients. Using gradients is just a performance optimization; 
we would get exactly the same results by using the slower manual process as well.

### Stop
Once we’ve decided how many epochs to train the model, we apply that decision. 
For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time.
___

To illustrate what these steps wpuld look like with a simple example, we will define a very simple function, the quadratic, 
and pretend that this is our loss function, and x is a weight parameter of the function:
```python
def f(x): return x**2
```
We can use fastai to get a graph of that funtion:
```python
plot_function(f, 'x', 'x**2')
```

And follow the sequance by picking a random value for a parameter, and calculating the value of the loss:
```python
plot_function(f, 'x', 'x**2')
plt.scatter(-1.5, f(-1.5), color='red');
```

Now we can look what would happen if we increased or decreased our parameter by a little bit — the adjustment. 
This is simply the slope at a particular point.     
We can change our weight by a little in the direction of the slope, calculate our loss and adjustment again, and repeat this a few times. 
Eventually, we will get to the lowest point on our curve.

Regardless of how complicated our functions become, this basic approach of gradient descent will not significantly change.

## Calculating Gradients

Fom calculus we know that **the *derivative* of a function tells us how much a change in its parameters will change its result**.     
The key point about derivatives is that **for any fucntion we can calculate its derivative**.    
**The derivative is another function, that calculates the change instead of the value**.    

More specifically, gradient is defined as the change in the value of the function, divided by the change in the value of the parameter.    
When we know how our function will change, we know what we need to do to make it smaller. This is thekey to machine learning: 
having a way to change the parameters of a function to make it smaller.     
The derivative is a computational shortcut that allow us to directly calculate the gradient of a function.

Our function will normally have lots of weights that we need to adjust, so when we calculate the derivative, we won’t get back one number, but lots of them 
— a gradient for every weight. But there is nothing mathematically tricky here; we can calculate the derivative with respect to one weight 
and treat all the other ones as constant, and then repeat that for each other weight. This is how all of the gradients are calculated, for every weight.

PyTorch is able to automatically compute the derivative of nearly any function, and very fast.     
___
1. Pick a tensor value at which we want gradients:
```python
xt = tensor(3.).requires_grad_()
```
The special method **requires_grad_** tells PyTorch that we want to calculate gradients with respect to that variable at that value, 
so PyTorch will remember to keep track of how to compute gradients of the other direct calculations on it that we will ask for.

2. Now we calculate our function with that value. 
PyTorch prints not just the value calculated *(3**2 = 9)*, but also a note that it has a gradient function it’ll be using to calculate our gradients when needed:
```python
yt = f(xt)
yt
```
tensor(9., grad_fn=<PowBackward0>)

3. We use PyTorch to calculate the gradients:
```python
yt.backward()
```
The *“backward”* here refers to **backpropagation**, which is the name given to the process of calculating the derivative of each layer.     
In a neural network this would be called the *backward pass*, as opposed to the *forward pass*, which is where the activations are calculated.

4. We can view the gradients by checking the *grad* attribute of our tensor:
```python
xt.grad
```
tensor(6.).     
The derivative of x**2 is 2*x, and we have x=3, so the gradients should be 2 * 3 = 6, which is what PyTorch calculated for us!
___
Now we can repeat the steps but with a vector instead of a single number as argument of our function:     
```python
xt = tensor([3.,4.,10.]).requires_grad_()
xt
```
tensor([ 3.,  4., 10.], requires_grad=True)     
And we’ll add *sum* to our function so it can take a vector (i.e., a rank-1 tensor) and return a scalar (i.e., a rank-0 tensor):
```python
def f(x): return (x**2).sum()

yt = f(xt)
yt
```
tensor(125., grad_fn=<SumBackward0>)     
*(3 ** 2 + 4 ** 2 + 10 ** 2) = (9 + 16 + 100) = 125*

```python
yt.backward()
xt.grad
```
tensor([ 6.,  8., 20.]).    
The derivative of x**2 is 2*x, and we have x=(3, 4, 10), so the gradients should be 2 * 3 = 6, 2 * 4 = 8 and 2 * 10 = 20, which is what PyTorch calculated.
___

The gradients tell us only the slope of our function; they don’t tell us exactly how far to adjust the parameters. 
But they do give us some idea of how far: if the slope is very large, that may suggest that we have more adjustments to do, 
whereas if the slope is very small, that may suggest that we are close to the optimal value.

## Stepping with a Learning Rate

Deciding how to change our parameters based on the values of the gradients is an important part of the deep learning process.     
Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called ***the learning rate (LR)***.     
The learning rate is often a number between 0.001 and 0.1.     
Once you’ve picked a learning rate, you can adjust your parameters using this simple function:
```python
w -= w.grad * lr
```
This is known as stepping your parameters, using an optimization step. 

Getting the correct LR is very imprtant because:
- If you pick a learning rate that’s too low, it can mean having to do a lot of steps.
- But picking a learning rate that’s too high is even worse — it can result in the loss getting worse.
- If the learning rate is too high, it may also “bounce” around, rather than diverging.
