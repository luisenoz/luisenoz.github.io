# CHAPTER 5: IMAGE CLASSIFICATION

To make your model really work, and work reliably, there are a lot of details you have to get right, 
and a lot of details that you have to check.     
This process requires being able to look inside your neural network as it trains and as it makes predictions, 
find possible problems, and know how to fix them.

We'll need to do a deep dive into the mechanics of deep learning. 
- What is the architecture of a computer vision model, 
 - an NLP model, 
 - a tabular model, and others.
 - How do we create an architecture that matches the needs of our particular domain
 - How do we get the best possible results from the training process 
 - How do we make things faster
 - What do we have to change as our datasets change.
 
 We will start by repeating the same basic applications that we looked at in the first chapter, but we are going to do two things: 
 - Make them better.  
 - Apply them to a wider variety of types of data.

## FROM DOGS AND CATS TO PET BREEDES

The same dataset we used to identify cats from dogs will also allow us to work on a much more challenging problem:     
**figuring out what breed of pet is shown in each image.**

In real life, we'll normally start with a dataset that we don't know anything about. We then have to figure out:
= how it is put together, 
- how to extract the data we need from it, 
- and what that data looks like.

We already downloaded the Pets dataset, and we can get a path to this dataset using the same code as in Chapter 1: 
```python
from fastai.vision.all import *
path = untar_data(URLs.PETS)
```

Now, to be able to extract the breed of each pet, we'll need to understand how the data is laid out.     
Data is usually provided in one of these two ways: 
- Individual files representing items of data, possibly organized into folders or with filenames representing information about those items  
- A table of data (e.g., in CSV format) in which each row is an item and may include filenames providing connections between the data in the table 
and data in other formats, such as text documents and images.

To see what is in our dataset, we can use the *ls* method: 
```python
path.ls() 
```
(#3) [Path('annotations'),Path('images'),Path('models')]

The *annotations* directory contains information about where the pets are. 
Since we'll be doing classification, not localization, we will ignore the *annotations* directory.     
We can use the same *ls* method to se what's inside the *images* folder:
```python
(path/'images').ls()
```

(#7394) [Path('images/great_pyrenees_173.jpg'),Path('images/wheaten_terrier_46.jpg'),
Path('images/Ragdoll_262.jpg'),Path('images/german_shorthaired_3.jpg'),
Path('images/american_bulldog_196.jpg'),Path('images/boxer_188.jpg'),
Path('images/staffordshire_bull_terrier_173.jpg'),Path('images/basset_hound_71.jpg'),
Path('images/staffordshire_bull_terrier_37.jpg'),Path('images/yorkshire_terrier_18.jpg')...].    

The first thing that is shown is the number of items in the collection, prefixed with a *#*, and only the first few items are displayed.

We can see that each file name is structured as follows:
1. the pet breed,
2. then an underscore (_),
3. a number,
4. the file extension (.jpg)

We need to create a piece of code that extracts the breed from a single Path, and the use it for the whole dataset.     
To allow us to test our code, let’s pick out one of these filenames: 
```python
fname = (path/"images").ls()[0]
```
The authors suggest that the most powerful and flexible way to extract information from strings like this is to use a **regular expressions**, 
also known as a ***regex***. A regular expression is a special string, written in the regular expression language, 
which specifies a general rule for deciding whether another string passes a test (i.e., “matches” the regular expression), 
and also possibly for plucking a particular part or parts out of that other string.

> Since Jeremy talks so highly about *regex* and considers them as one of the most useful tools in our programming toolkit, and also many of his students found regex as the most exiting tool to learn, I dived into several tutorials in the web and summarised the main concpets in a notebook. I hadn't worked with regex before and found them interesting but challenging and not easy to digest without a lot of prectice. But you can form your opinion looking at the notebook here, if you're not used to them. [regex notebook](https://github.com/luisenoz/luisenoz.github.io/blob/master/images/RegEx.ipynb)

Jeremy uses *findall*, one of the *regex* methods, to try a regular expression against the filename of the *fname* object:
```python
re.findall(r'(.+)_\d+.jpg$', fname.name)
```
*This regular expression plucks out all the characters leading up to the last underscore character, 
as long as the subsequent characters are numerical digits and then the JPEG file extension.*

['great_pyrenees']

Now that we know it worked for one example, we ca use it to label the whole dataset.     
As expected, *fastai* comes with several classes to help with labelling. For example, if we want to label with regular expressions, we can use the *RegexLabeller* class. And in this case we'll use the Datablock API we already used in Chapter 2.
```python
pets = DataBlock(blocks = (ImageBlock, CategoryBlock),
                 get_items=get_image_files,
splitter=RandomSplitter(seed=42),
                 get_y=using_attr(RegexLabeller(r'(.+)_\d+.jpg$'), 'name'),
                 item_tfms=Resize(460),
                 batch_tfms=aug_transforms(size=224, min_scale=0.75))
dls = pets.dataloaders(path/"images")
```
One important piece of this DataBlock call that we haven’t seen before is in these two lines: 
```python
item_tfms=Resize(460),
batch_tfms=aug_transforms(size=224, min_scale=0.75)
```

Those lines implement a *fastai* data augmentation strategy called *presizing*, that is designaed to reduce data destruction while maintaining a good performance.

## Presizing

We need the images to have all the same dimensions.     
We also search to minimise the claculations needed for augmentation.     
There is also the challenge that, if performed after resizing down, various common data augmentation transforms might introduce 
spurious empty zones, degrade data, or both.
*Presizing* adopts two startegies to work around those challenges:
1. Resizes images to relatively *large* dimensions - significantly larger than the target training dimensiosn.
2. Compose all common augmentation operations *(including a resize to the final target size)* into one, 
and perform the combined operation on the GPU only once at the end of processing, 
rather than performing the operations individually and interpolating multiple times.

The first step, ***crop full width or height*** is in *item_tfms* so it's applied to each individual image before it's copied to the GPU. It ensures all images are of the same size.     
The resize, creates images large enough that they have spare margin to allow further augmentation transforms on their inner regions without creating empty zones. This transformation works by resizing to a square, using a large crop size. On the training set, the crop area is chosen randomly, and the size of the crop is selected to cover the entire width (for portrait images) or height (for landscape images) of the image, whichever is smaller. In the validation set the centre square of the image is always chosen.

In the second step, ***random crop and augment*** is in *batch_tfms* and it's applied to a batch all at once on the GPU, which means is faster and all of the potentially destructive operations are done together, with a single interpolation at the end. On the validation set, only the resize to the final size needed for the model is done here. On the training set, the random crop and any other augmentations are done first.

To implement this process in fastai, you use *Resize* as an item transform with a large size, and *RandomResizedCrop* as a batch transform with a smaller size. *RandomResizedCrop* will be added if you include the *min_scale* parameter in your *aug_transforms* function, as was done in the DataBlock. Alternatively, you can use *pad* or *squish* instead of *crop* (the default) for the initial *Resize*.

If you looked at an image that has been zoomed, interpolated, rotated, and then interpolated again *(which is the approach used by all other deep learning libraries)*, and an image that has been zoomed and rotated as one operation and then interpolated once *(the fastai approach)*, you would see that the first image is less well defined and could have reflection padding artifacts; as well as some parts disappeared entirely. Jeremy found that, in practice, using *presizing* significantly improves the accuracy of models and often results in speedups too.

### Checking and debugging a DataBlock

Writing a DataBlock we will get an error message if we have a syntax error somewhere in our code, but we have no guarantee that our template is going to work on our data source as we intend. So, **before training a model, we should always check our data**.

```python
dls.show_batch(nrows=1, ncols=3)
```
The *show_batch* method will show the images and we can then check that each photo corresponds to the right label. If we're not familiar with the dataset, we can always search a sample in Google and verify that our images and labels are right.

If we made a mistake while building our DataBlock, we likely won’t see it before this step.     
To debug this, the authors encourage us to use the *summary* method. It will attempt to create a batch from the source we give it, with a lot of details.     
Also, if it fails, we will see exactly at which point the error happens, and the library will try to give some help.

For instance, one common mistake is to forget to use a *Resize* transform, so we end up with pictures of different sizes and are not able to batch them.

```python
pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),
                 get_items=get_image_files, 
                 splitter=RandomSplitter(seed=42),
                 get_y=using_attr(RegexLabeller(r'(.+)_\d+.jpg$'), 'name'))
pets1.summary(path/"images")
```
The summary will provide the following result:

---
Setting-up type transforms pipelines.    
Collecting items from /storage/data/oxford-iiit-pet/images.    
Found 7390 items.    
2 datasets of sizes 5912,1478.    
Setting up Pipeline: PILBase.create.    
Setting up Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}.    

Building one sample.    
  Pipeline: PILBase.create.    
      starting from.    
            /storage/data/oxford-iiit-pet/images/shiba_inu_98.jpg.    
      applying PILBase.create gives.    
      PILImage mode=RGB size=500x374
      Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}.    
      starting from.    
           /storage/data/oxford-iiit-pet/images/shiba_inu_98.jpg.    
      applying partial gives. 
      shiba_inu.    
      applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives.    
      TensorCategory(33)

Final sample: (PILImage mode=RGB size=500x374, TensorCategory(33))

Setting up after_item: Pipeline: ToTensor.    
Setting up before_batch: Pipeline:      
Setting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}

Building one batch.    
Applying item_tfms to the first sample:     
  Pipeline: ToTensor.    
      starting from.    
       (PILImage mode=RGB size=500x374, TensorCategory(33))
    applying ToTensor gives.    
     (TensorImage of size 3x374x500, TensorCategory(33))

Adding the next 3 samples

No before_batch transform to apply

Collating items in a batch.     
**Error! It's not possible to collate your items in a batch. 
Collating items in a batch.     
Could not collate the 0-th members of your tuples because got the following shapes.     
torch.Size([3, 374, 500]),torch.Size([3, 375, 500]),torch.Size([3, 500, 424]),torch.Size([3, 351, 500])**

---

We can see exactly how we gathered the data and split it, how we went from a filename to a sample (the tuple (image, category)), 
then what item transforms were applied and how it failed to collate those samples in a batch (because of the different shapes).

Once we think the data looks right, the authors generally recommend the next step should be using it to train a simple model.     
For this initial test, we can use the same simple model that we used in Chapter 1:
```python
learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(2)
```
|epoch. |train_loss. |valid_loss  |error_rate  |time|
|-----|-----|-----|-----|-----|
|0	|1.483786	|0.346272	|0.100812	|00:30|

|epoch. |train_loss. |valid_loss  |error_rate  |time|
|-----|-----|-----|-----|-----|
|0	|0.495099	|0.311673	|0.089986	|00:35|
|1	|0.318182	|0.216932	|0.071719	|00:30|

We haven’t actually told fastai what loss function we want to use. But fastai will generally try to select an appropriate loss function 
based on the kind of data and model we are using.     
In this case, we have image data and a categorical outcome, so fastai will default to using ***cross-entropy loss***.


## Cross-Entropy Loss

Cross-entropy loss is a function similar to the one we've been using but with 2 advantages:
1. It works, even when the dependent variable has more than 2 categories *(several breeds instead of dog or cat)*
2. It results in faster and more reliable training.

But, before trying to understand how cross-entropy loss works, we first have to understand how the actual data 
and activations that are seen by the loss function look like.

### Viewing Activation and Labels

We can start by using the *one_batch* method to get a btach of real data from the *DataLoaders*
```python
x,y = dls.one_batch()
y
```
TensorCategory([31,  4,  1, 30,  6,  6, 11, 15, 24, 14, 13, 17,  2, 12,  5, 34,  6, 19, 15,  3,  7,  8, 33, 12,  8,     
1, 13, 14, 30,  8, 25, 32, 25,  5, 17, 31, 29,  9, 35, 24,  2, 30, 15, 19,  6, 35, 10,  5,     
        14,  6, 10, 29, 25, 22,  2, 33,  6,  7, 25, 29, 27, 15, 32, 13], device='cuda:0')

Our batch size is 64, so we have 64 rows in this tensor.     
Each row is a single integer between 0 and 36, representing our 37 possible pet breeds.

We can view the predictions *(the activations of the final layer of our neural network)* by using *Learner.get_preds*.     
This function takes either a dataset index (0 for train and 1 for valid) or an iterator of batches. Thus, we can pass it a simple list with our batch to get our predictions.     
It returns predictions and targets by default, but since we already have the targets, we can effectively ignore them by assigning to the special variable *_*:

```python
preds,_ = learn.get_preds(dl=[(x,y)])
preds[0]
```
TensorImage([2.9641e-08, 5.0075e-07, 8.4463e-07, 9.4931e-07, 4.5364e-06, 1.2933e-05, 1.2391e-06, 3.8727e-05, 1.4260e-05, 1.2109e-06, 4.3693e-08, 2.6370e-07, 3.9565e-06, 6.5704e-06, 1.8649e-06, 1.2336e-06,
        1.7263e-07, 1.7804e-07, 4.4786e-07, 1.1471e-05, 2.4972e-06, 1.7159e-03, 3.1686e-04, 3.8349e-06, 2.3981e-04, 2.3053e-06, 3.4829e-07, 1.0206e-04, 2.3513e-05, 8.6500e-08, 4.5018e-06, 9.9747e-01,
        1.9257e-06, 1.3839e-06, 1.1302e-05, 2.8269e-06, 4.5107e-07])
        
The predictions are 37 probabilities that go from 0 to 1, and total 1.
```python
len(preds[0]),preds[0].sum()
```
(37, TensorImage(1.))

To transform the activations of our model into predictions like this, we used a new fucntion called the ***softmax*** **activation function**.

### Softmax

We used the *softmax* function in the last layer of the model to ensure that the activations were all between 0 and 1, and that all they sum to 1.     
softmax is the multi-category equivalent of sigmoid — we have to **use it anytime we have more than two categories** and the probabilities of the categories must add to 1, and we often use it even when there are just two categories, just to make things a bit more consistent.     
```python
def softmax(x): 
     return exp(x) / exp(x).sum(dim=1, keepdim=True)
```

```python
torch.random.manual_seed(42);
acts = torch.randn((6,2))*2
acts
```
tensor([[ 0.6734,  0.2576],     
        [ 0.4689,  0.4607],     
        [-2.2457, -0.3727],     
        [ 4.4164, -1.2760],     
        [ 0.9233,  0.5347],     
        [ 1.0698,  1.6187]])

```python
sm_acts = torch.softmax(acts, dim=1)
sm_acts
```
tensor([[0.6025, 0.3975],     
        [0.5021, 0.4979],     
        [0.1332, 0.8668],     
        [0.9966, 0.0034],     
        [0.5959, 0.4041],     
        [0.3661, 0.6339]])
        

> **Jargon:** Exponential Function (exp) Defined as e ** x, where e is a special number approximately equal to 2.718. 
It is the inverse of the natural logarithm function. Note that **exp is always positive and increases very rapidly!**

Taking the exponential ensures all our numbers are positive, and then dividing by the sum ensures we are going to have a bunch of numbers that add up to 1.     
The exponential also has a nice property: if one of the numbers in our activations x is slightly bigger than the others, the exponential will amplify this (since it grows exponentially), which means that in the *softmax*, that number will be closer to 1. So it’s ideal for training a classifier when we know each picture has a definite label.     
*(Note that it may be less ideal during inference, as you might want your model to sometimes tell you it doesn’t recognize any of the classes that it has seen during training, and not pick a class because it has a slightly bigger activation score.)*

*Softmax* is the first part of the cross-entropy loss — the second part is *log likelihood*.

### Log Likehood

As we moved from *sigmoid* to *softmax*, we need to extend the loss function to work with any number of categories *(in this case, we have 37 categories)*.     
Our activations, after softmax now, are between 0 and 1, and sum to 1 for each row in the batch of predictions.     
Our targets are integers between 0 and 36.

In the binary case, we used *torch.where* to select between *inputs* and *1-inputs*.
```python
def mnist_loss(inputs, targets):
    inputs = inputs.sigmoid()
    return torch.where(targets==1, 1-inputs, inputs).mean()
```
But when we take a binary classification just as another type of a general classifiaction but with only 2 categories, it becomes easier, 
because we now have 2 columns containing the equivalent of *inputs* and *1-inputs*, and so, all we need to do is select the value from the right column.     
Let's look at how it would work in Pytorch, with an example based on 3s and 7s:
1. Let's say these are our labels:
```python
targ = tensor([0,1,0,1,1,0])
```
2. And these the *softmax* activation *(from the previus run)*:
```python
sm_acts
```
tensor([[0.6025, 0.3975],     
        [0.5021, 0.4979],     
        [0.1332, 0.8668],     
        [0.9966, 0.0034],     
        [0.5959, 0.4041],     
        [0.3661, 0.6339]])
        
Then for each item of *targ*, we can use that to select the appropriate column of *sm_acts* using **tensor indexing**:
```python
idx = range(6)
sm_acts[idx, targ]
```
tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])

To see what’s happening here, let’s put all the columns together in a table, where the first two columns are our activations, then we have the targets, the row index, and finally the result shown in the preceding code:
```python
from IPython.display import HTML
df = pd.DataFrame(sm_acts, columns=["3","7"])
df['targ'] = targ
df['idx'] = idx
df['loss'] = sm_acts[range(6), targ]
t = df.style.hide_index()
#To have html code compatible with our script
html = t._repr_html_().split('</style>')[1]
html = re.sub(r'<table id="([^"]+)"\s*>', r'<table >', html)
display(HTML(html))
```
|3	|7	|targ	|idx	|loss  |
|:---|:---|:---|:---|:---|
|0.602469	|0.397531	|0	|0	|0.602469|
|0.502065	|0.497935	|1	|1	|0.497935|
|0.133188	|0.866811	|0	|2	|0.133188|
|0.996640	|0.003360	|1	|3	|0.003360|
|0.595949	|0.404051	|1	|4	|0.404051|
|0.366118	|0.633882	|0	|5	|0.366118|

From the table, we can see that the final column ("loss") can be calculated by taking the *targ* and *idx* columns as indices into the two-column matrix containing the 3 and 7 columns.     
That’s what ```sm_acts[idx, targ]``` is doing:

- sm_acts[0][0] = 0.602469
- sm_acts[1][1] = 0.497935
- ...
- sm_acts[5][0] = 0.366118

This works just as well with more than two columns. If we added an activation column for every digit (0 through 9), and then *targ* contained a number from 0 to 9, as long as the activation columns sum to 1 (as they will, if we use softmax), we’ll have a loss function that shows how well we’re predicting each digit.

We’re picking the loss only from the column containing the correct label. We don’t need to consider the other columns, because by the definition of softmax, they add up to 1 minus the activation corresponding to the correct label. Therefore, making the activation for the correct label as high as possible we're also decreasing the activation of the remaining columns.

As usual, PyTorch provides a function that does exactly the same thing as *sm_acts[range(n), targ]* *(except it takes the negative, because when applying the log afterward, we will have negative numbers)*, called **nll_loss** *(NLL stands for negative log likelihood)*:

```python
sm_acts[idx, targ]
```
tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])

```python
F.nll_loss(sm_acts, targ, reduction='none')
```
tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])

However, despite its name, this *nll_loss* function doesn't take the log.

### Taking the log

The function we saw in the previous section works quite well as a loss function, but the problem is that we are using probabilities, 
and probabilities cannot be smaller than 0 or greater than 1. That means our model will not care whether it predicts 0.99 or 0.999. 
While those numbers are very close together — in another sense, 0.999 is 10 times more confident than 0.99.

So, to solve that limiation we want to transform our numbers between 0 and 1 to instead be between negative infinity and infinity.   
And there is a mathematical function that does exactly this: **the logarithm** (available as ***torch.log***).   
The logarithm function has this identity: 
y = b ** a.  
a = log(y,b).     
A quantity representing the power *(a)* to which a fixed number (the base) *(b)* must be raised to produce a given number *(y)*.
In this case, we’re assuming that log(y,b) returns log y base b. However, *log* in Python uses the special number *e* (2.718…) as the base.   
n = log (y, e) = The power *n* to which *e* must be raised to produce *y*.   

The key thing to know about logarithms for DL is this relationship:  

**log(a*b) = log(a)+log(b)**     

It means that logarithms increase linearly when the underlying signal increases exponentially or multiplicatively.     
The clear advantage is that multiplication, which can create really, really large and really, really small numbers, 
can be replaced by addition, which is much less likely to result in scales that are difficult for our computers to handle.

Taking the mean of the positive or negative log of our probabilities (depending on whether it’s the correct or incorrect class) 
gives us the negative log likelihood loss. In PyTorch, *nll_loss* assumes that you already took the log of the softmax, 
so it doesn’t do the logarithm for you.

> The “nll” in *nll_loss* stands for “negative log likelihood,” but it doesn’t actually take the log at all! 
It assumes you have already taken the log.    
PyTorch has a function called *log_softmax* that combines log and softmax in a fast and accurate way, 
but *nll_loss* is designed to be used after log_softmax.

When we first take the *softmax*, and then the *log likelihood* of that, that combination is called ***cross-entropy loss***. 
In PyTorch, this is available as ***nn.CrossEntropyLoss*** (which, in practice, does *log_softmax* and then *nll_loss*):
```python
loss_func = nn.CrossEntropyLoss()
'''
This is a class. Instantiating it gives an object that behaves like a function:
```python
loss_func(acts, targ)
```
tensor(1.8045)

All PyTorch loss functions are provided in two forms, the class form just shown as well as a plain functional form, available in the F namespace: 
```python
F.cross_entropy(acts, targ)
```
tensor(1.8045)

By default, PyTorch loss functions take the mean of the loss of all items, but we can use *reduction='none'* to disable that:
```python
nn.CrossEntropyLoss(reduction='none')(acts, targ)
```
tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])

## Model Interpretation

It’s very hard to interpret loss functions directly, because they are designed to be things computers can differentiate and optimize, 
not things that people can understand.     
That’s why we have metrics. These are not used in the optimization process, but just to help us understand what’s going on.     
We saw in Chapter 1 that we can use a *confusion matrix* to see where our model is doing well and where it’s doing badly:
```python
interp = ClassificationInterpretation.from_learner(learn)
interp.plot_confusion_matrix(figsize=(12,12), dpi=60)
```
This confusion matrix is very hard to read. We have 37 pet breeds, which means we have 37×37 entries in this giant matrix!     
Instead, we can use the *most_confused method*, which just shows us the cells of the confusion matrix with the most incorrect predictions (here, with at least 5 or more):
```python
interp.most_confused(min_val=5)
```
[('Ragdoll', 'Birman', 8),     
 ('miniature_pinscher', 'chihuahua', 8),     
  ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 5)]
  
A little bit of Googling tells us that the most common category errors shown here are breed differences that even expert breeders sometimes disagree about.     
So, we seem to have a good baseline. What can we do now to make it even better?

## Improving the model

The first thing we need to set when training a model is the learning rate. If our learning rate is too low, it can take many, many epochs to train our model. Not only does this waste time, but it also means that we may have problems with overfitting, because every time we do a complete pass through the data, we give our model a chance to memorize it.

In 2015, researcher Leslie Smith came up with a brilliant idea, called the **learning rate finder**. 
His idea was to 
- start with a very, very small learning rate, something so small that we would never expect it to be too big to handle. 
- we use that for one mini-batch, 
- find what the losses are afterward, 
- and then increase the learning rate by a certain percentage, (e.g., doubling it each time).
- Then we do another mini-batch, track the loss, and double the learning rate again. 
- We keep doing this until the loss gets worse, instead of better. This is the point where we know we have gone too far. 
- We then select a learning rate a bit lower than this point.

The authors' advice is to pick either of these: 
- One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)  
- The last point where the loss was clearly decreasing.

The learning rate finder *lr_find* in fastai computes those points on the curve to help us.
```python
learn = cnn_learner(dls, resnet34, metrics=error_rate)
lr_min,lr_steep = learn.lr_find()
```
![lr_find](https://github.com/luisenoz/luisenoz.github.io/blob/master/images/LearnRateFinder.png)

```python
print(f"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}")
```
Minimum/10: 1.00e-02, steepest point: 5.25e-03

We can see on the plot that in the range 1e-6 to 1e-3, nothing really happens and the model doesn’t train.   
Then the loss starts to decrease until it reaches a minimum, and then increases again.   
We don’t want a learning rate greater than 1e-1, as it will cause training to diverge, 
but 1e-1 is already too high: at this stage, we’ve left the period where the loss was decreasing steadily.
It appears that a learning rate around 3e-3 would be appropriate:
```python
learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(2, base_lr=3e-3)
```
|epoch	|train_loss	|valid_loss	|error_rate	|time|
|:---|:---|:---|:---|---:|
|0	|1.268113	|0.314848	|0.106901	|00:34|

|epoch	|train_loss	|valid_loss	|error_rate	|time|
|:---|:---|:---|:---|---:|
|0	|0.519503	|0.397454	|0.122463	|00:43|
|1	|0.341325	|0.253027	|0.081867	|00:43|

> The learning rate finder plot has a ***logarithmic scale***, which is why the middle point between 1e-3 and 1e-2 is between 3e-3 and 4e-3.   
This is because we care mostly about the order of magnitude of the learning rate.

Now that we know how to find a good learning rate to train our model, let’s look at how we can fine-tune the weights of a pretrained model.

### Unfreezing & Transfer Learning

As we saw in Chapter 1, the basic idea behind *transfer learning* is that a pre-trained model, potentially trained on millins of data points *(such as ImageNet)*, is fine-tuned for another task.     
That pre-trained convolutional neural network would consists of many linear layers with a nonlinear activation function between each pair, followed by one or more final linear layers with an activation function such as softmax at the very end.     
The final linear layer of the pre-trained CNN would use a matrix with enough columns such that the output size is the same as the number of classes in that  model (assuming that we are doing classification).     
But that final linear layer is unlikely to be of any use for us when we are fine-tuning in a transfer learning setting, because it is specifically designed to classify the categories in the original pretraining dataset.     
So when we do transfer learning, we remove it, and replace it with a new linear layer with the correct number of outputs for our desired task (in this case, there would be 37 activations).     
This newly added linear layer will have entirely random weights and therefore random outputs, but all of the layers prior to the last one have been carefully trained to be good at image classification tasks in general. We want to train a model in such a way that we allow it to remember all of these generally useful ideas from the pretrained model, use them to solve our particular task, and adjust them only as required for the specifics of our particular task.

***The challenge when fine-tuning is to replace the random weights in the added linear layers with weights that correctly achieve our desired task without breaking the carefully pretrained weights and the other layers.***

A trick to make this happening is to tell the optimizer to update the weights in only those randomly added final layers and do not change the weights in the rest of the neural network at all. This is called **freezing** those pretrained layers.

In fastai, When we call the *fine_tune* method, fastai does two things: 
- Trains the randomly added layers for one epoch, with all other layers frozen.
- Unfreezes all the layers, and trains them for the number of epochs requested.

---
It's possible our particular dataset may get better results by doing things slightly differently.     
If that's the case, the *fine_tune* method has parameters we can use to change its behavior. Howver, it might be easiest for us to just call the underlying methods directly to customise the method's behavior. We can see the source code for the method by using the following syntax: 
```python
learn.fine_tune??
```
```python
Signature:
learn.fine_tune(
    epochs,
    base_lr=0.002,
    freeze_epochs=1,
    lr_mult=100,
    pct_start=0.3,
    div=5.0,
    lr_max=None,
    div_final=100000.0,
    wd=None,
    moms=None,
    cbs=None,
    reset_opt=False,
)
Source:   
@patch
@log_args(but_as=Learner.fit)
@delegates(Learner.fit_one_cycle)
def fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,
              pct_start=0.3, div=5.0, **kwargs):
    "Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR"
    self.freeze()
    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)
    base_lr /= 2
    self.unfreeze()
    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)
File:      /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/callback/schedule.py
Type:      method
```
---

Let's try to gp trough the *freezing* method manually:
1. train the randomly added layers for three epochs, using *fit_one_cycle*. *fit_one_cycle* is the suggested way to train models without using *fine_tune*. What *fit_one_cycle* does is to start training at a low learning rate, gradually increase it for the first section of training, and then gradually decrease it again for the last section of training:
```python
learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fit_one_cycle(3, 3e-3)
```
|epoch	|train_loss	|valid_loss	|error_rate	|time|
|:---|:---|:---|:---|:---|
|0	|1.127467	|0.361891	|0.110961	|00:35|
|1	|0.509897	|0.272498	|0.086604	|00:35|
|2	|0.331713	|0.248952	|0.087957	|00:35|

2. Unfreeze the model.
```python
learn.unfreeze()
```

3. run *lr_find* again, because now we have more layers to train, and weights that have already been trained for three epochs, therefore, our previously found learning rate isn’t appropriate anymore:
```python
learn.lr_find()
```
SuggestedLRs(lr_min=6.918309736647643e-07, lr_steep=6.309573450380412e-07)

The graph should be different from when we had random weights: We shouldn't have that sharp descent that indicates the model is training.   
That’s because our model has been trained already.So, hHere we have a sharp increase, and we should take a point well before that sharp increase — for instance, 1e-5.     
*(The point with the maximum gradient isn’t what we look for here and should be ignored.)*

4. Train at a suitable learning rate:
```python
learn.fit_one_cycle(6, lr_max=1e-5)
```
|epoch	|train_loss	|valid_loss	|error_rate	|time|
|:---|:---|:---|:---|:---|
|0	|0.249777	|0.242456	|0.083897	|00:43|
|1	|0.241040	|0.232725	|0.079161	|00:43|
|2	|0.211402	|0.223761	|0.075778	|00:44|
|3	|0.204896	|0.214796	|0.068336	|00:43|
|4	|0.188423	|0.214600	|0.069689	|00:43|
|5	|0.178320	|0.216158	|0.068336	|00:44|

This has improved our model a bit, but there’s more we can do.   
The deepest layers of our pretrained model might not need as high a learning rate as the last ones, 
so we should probably use different learning rates for those — this is known as using **discriminative learning rates**.

### Discriminative Learning Rates

Even after we unfreeze, we still care a lot about the quality of those pretrained weights.  
Given that the pretrained weights have been trained for hundreds of epochs, on millions of images, 
we would not expect that the best learning rate for those pretrained parameters would be as high as for the randomly added parameters.  
In addition, remember that the first layer learns very simple foundations, like edge and gradient detectors; and these are likely to be just as useful for nearly any task. On the other hand, the later layers learn much more complex concepts, like “eye” and “sunset,” which might not be useful in our task at all (maybe we’re classifying car models, for instance). So it makes sense to let the later layers fine-tune more quickly than earlier layers.

Therefore, fastai’s default approach is to use *discriminative learning rates*:   
1. use a lower learning rate for the early layers of the neural network,
2. and a higher learning rate for the later layers (and especially the randomly added layers).


![Jasn_Yosinki](https://github.com/luisenoz/luisenoz.github.io/blob/master/images/1*495mUnP8maaDeVluUTdiTw.png)

fastai lets you pass a Python *slice* object anywhere that a learning rate is expected.   
The first value passed will be the learning rate in the earliest layer of the neural network, 
and the second value will be the learning rate in the final layer.     
The layers in between will have learning rates that are multiplicatively equidistant throughout that range.

Let’s replicate the previous training, but this time we’ll set only the lowest layer of our net to a learning rate of 1e-6; 
the other layers will scale up to 1e-4.
```python
learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fit_one_cycle(3, 3e-3)
learn.unfreeze()
learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))
```
|epoch	|train_loss	|valid_loss	|error_rate	|time|
|:---|:---|:---|:---|:---|
|0	|1.116598	|0.331760	|0.100812	|00:36|
|1	|0.520378	|0.277590	|0.080514	|00:36|
|2	|0.310345	|0.241896	|0.078484	|00:36|

|epoch	|train_loss	|valid_loss	|error_rate	|time|
|:---|:---|:---|:---|:---|
|0	|0.255238	|0.237682	|0.071719	|00:44|
|1	|0.253180	|0.231166	|0.067659	|00:45|
|2	|0.231527	|0.226924	|0.066982	|00:44|
|3	|0.208412	|0.227629	|0.071719	|00:45|
|4	|0.193091	|0.223916	|0.071042	|00:45|
|5	|0.175225	|0.212828	|0.066306	|00:45|
|6	|0.155275	|0.211512	|0.069689	|00:45|
|7	|0.144928	|0.214255	|0.067659	|00:44|
|8	|0.137607	|0.207077	|0.071042	|00:45|
|9	|0.137892	|0.206854	|0.070365	|00:45|
|10|0.121813	|0.208009	|0.068336	|00:44|
|11|0.119612	|0.206126	|0.069012	|00:45|

We can see the improvement in fine-tinug using fastai graph of the training and validation loss: 
```python
learn.recorder.plot_loss()
```
![tr&vl_loss](https://github.com/luisenoz/luisenoz.github.io/blob/master/images/Unknown.png)

As we can see, the training loss keeps getting better and better. But the validation loss improvement slows and sometimes even gets worse!     
This is the point at which the model is starting to overfit. The model is becoming overconfident of its predictions.   
But this does not mean that it is getting less accurate, necessarily. If we take a look at the table of training results per epoch, we will often see that the accuracy continues improving, even as the validation loss gets worse.   
In the end, **what matters is our accuracy, or more generally our chosen metrics, not the loss**. The loss is just the function we’ve given the computer to help us to optimize.

### Selecting the Number of Epochs

The first approach to training should be to simply pick a number of epochs that will train in the amount of time that we are happy to wait for.   
Then look at the training and validation loss plots, as shown previously, and in particular our metrics. If we see that they are still getting better even in the final epochs, we'll know that we have not trained for too long.

On the other hand, we may well see that the metrics we have chosen are really getting worse at the end of training.   
The validation loss will first get worse during training because the model gets overconfident, and only later will get worse because it is incorrectly memorizing the data. In practice, we will about only the latter issue. *(our loss function is something that we use to allow our optimizer to have something it can differentiate and optimize; it’s not the thing we care about in practice)*.    
If we find that we have overfit, we should retrain our model from scratch, and this time select a total number of epochs based on where our previous best results were found.

### Deeper Architectures

In general, a model with more parameters can model your data more accurately. For most architectures we can create larger versions of them by simply adding more layers. However, since we want to use pretrained models, we need to make sure that we choose a number of layers that have already been pretrained for us. *(For instance, the ResNet architecture that we are using in this chapter comes in variants with 18, 34, 50, 101, and 152 layers, pretrained on ImageNet)*

In general, a bigger model has the ability to better capture the real underlying relationships in our data, as well as to capture and memorize the specific details of our individual images. However, using a deeper model is going to require more GPU RAM, so we may need to lower the size of your batches to avoid an out-of-memory error. This happens when we try to fit too much inside your GPU and looks like this:
     Cuda runtime error: out of memory

We may have to restart your notebook when this happens. The way to solve it is to use a smaller batch size.     
We can pass the batch size we want to the call by creating our DataLoaders with *bs=*

The other downside of deeper architectures is that they take quite a bit longer to train, 
but one technique that can speed things up a lot is ***mixed-precision training***. This refers to using less-precise numbers *(half-precision floating point, also called fp16)* where possible during training.     
Nearly all current NVIDIA GPUs support a special feature called *tensor cores* that can dramatically speed up neural network training, by 2–3×. 
And they also require a lot less GPU memory.     
To make that work in fastai, we just need to import the *callback.fp16* module and add *to_fp16()* after the learner creation

So let’s try a ResNet-50 now with mixed precision

```python
from fastai.callback.fp16 import *
learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16()
learn.fine_tune(6, freeze_epochs=3)
```
|epoch	|train_loss	|valid_loss	|error_rate	|time|
|:---|:---|:---|:---|:---|
|0	|1.432270	|0.293598	|0.097429	|00:59|
|1	|0.610418	|0.265547	|0.087957	|01:00|
|2	|0.410859	|0.253046	|0.080514	|01:00|

|epoch	|train_loss	|valid_loss	|error_rate	|time|
|:---|:---|:---|:---|:---|
|0	|0.242966	|0.222662	|0.075778	|01:13|
|1	|0.309474	|0.335628	|0.089986	|01:13|
|2	|0.276870	|0.274505	|0.078484	|01:13|
|3	|0.169375	|0.184272	|0.058187	|01:14|
|4	|0.086844	|0.189915	|0.062246	|01:12|
|5	|0.058870	|0.180698	|0.054804	|01:13|


We’re not seeing a clear win from the deeper model. This is useful to remember — **bigger models aren’t necessarily better models** for all particular cases!     
Try small models before starting scaling up.

