# CHAPTER 1.1 - The Introduction

## Deep Learning is for everyone!!!

You won't need:
- A lot of data
- A master in mathematics
- Huge super fast and expensive computers

Deep learning is a computer technique to extract and transform data by using multiple layers of neural networks. 
Each of these layers takes its inputs from previous layers and progressively refine them.
The layers are trained by algorithms that minimize their errors and improve their accuracy. 
In this way, the network learns by itself to perform a specified task without need of the user's instructions as in normal programs.

Here’s a list of some of the many different areas for which deep learning, or methods heavily using deep learning, is now the best in the world:
- *Natural Language Processing (NLP)*
- *Computer Vision*
- *Medicine*
- *Biology*
- *Image Generation*
- *Recomendation Systems*
- *Playing Games*
- *Robotics*
- *Financial and Logistical forecasting*
- *Text-to-Speech and more*

**A bit of history**

In 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, teamed up to develop a mathematical model of an artificial neuron.

A psychologist, Frank Rosenblatt further developed the artificial neuron to give it the ability to learn. 
Even more importantly, he built the first device that used these principles, the Mark I Perceptron.

An MIT professor, Marvin Minsky wrote a book about Rosenblatt’s invention where they showed that a single layer of these devices was unable to learn
some simple but critical mathematical functions (such as XOR). 
In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed. 
Unfortunately, only the first of these insights was widely recognized and as a result, the global academic community nearly entirely gave up on neural networks 
for the next two decades.

The most pivotal work in neural networks in the last 50 years was the multi-volume Parallel Distributed Processing (PDP) by David Rumelhart, 
James McClelland, and the PDP Research Group, released in 1986.
The approach laid out in PDP is very similar to the approach used in today’s neural networks.
PDP required the following: 
- A set of processing units.
- A state of activation.
- An output function for each unit.
- A pattern of connectivity among units.
- A propagation rule for propagating activities through the network.
- An activation rule for combining the inputs to a unit with the current state of that unit to produce the output from the unit.
- A learning rule whereby patterns of connectivity are modified by experience.
- An environment within which the system must operate.

Neural networks are now finally living up to their potential, thanks to the use of more layers, because of improvements in computer hardware, 
increases in data availability, and algorithmic that allow neural networks to be trained faster and more easily. 

We now have what Rosenblatt promised: 
*“A machine capable of perceiving, recognizing, and identifying its surroundings without any human training or control.”*

## The Software, and why in the end it does not matter! ##

**PyTorch** has become the world’s fastest-growing deep learning library and is already used for most research papers at top conferences.
It works best as a low-level foundation library, providing the basic operations for higher-level functionality.

**The fastai library** is the most popular library for adding this higher-level functionality on top of PyTorch.
*(the book covers the version 2 of the fastai library)* 

However, it doesn’t really matter what software we learn, because it takes only a few days to switch from one library to another.

**What really matters is learning the deep learning foundations and techniques properly.**

**We can assume that whatever specific libraries and software we learn today will be obsolete in a year or two!!!**

We'll be using one of the most popular programming experimentation platform: **Jupyter Notebooks**.

Fortunately for me, I already have had some experience with Jupyter Notebooks before, so at least working with them will not be a completely unknown subject.

The chapter 1 then delves into building our first model. And that's so amazing that I'll create a new post to relate my experience and learnings from that.


